{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Donkey\u00ae Donkey is a high level self driving library written in Python. It was developed with a focus on enabling fast experimentation and easy contribution. Build your own Donkey Donkey is the standard car that most people build first. The parts cost about $250 to $300 and take 2 hours to assemble. Here are the main steps to build your own car: Assemble hardware. Install software. Create Donkey App. Calibrate your car. Start driving. Train an autopilot. Experiment with simulator. Hello World. Donkeycar is designed to make adding new parts to your car easy. Here's an example car application that captures images from the camera and saves them. import donkey as dk #initialize the vehicle V = dk.Vehicle() #add a camera part cam = dk.parts.PiCamera() V.add(cam, outputs=['image'], threaded=True) #add tub part to record images tub = dk.parts.Tub(path='~/mycar/data', inputs=['image'], types=['image_array']) V.add(tub, inputs=inputs) #start the vehicle's drive loop V.start(max_loop_count=100) Installation How to install Why the name Donkey? The ultimate goal of this project is to build something useful. Donkey's were one of the first domesticated pack animals, they're notoriously stubborn, and they are kid safe. Until the car can navigate from one side of a city to the other, we'll hold off naming it after some celestial being.","title":"Home"},{"location":"#about-donkey","text":"Donkey is a high level self driving library written in Python. It was developed with a focus on enabling fast experimentation and easy contribution.","title":"About Donkey&reg;"},{"location":"#build-your-own-donkey","text":"Donkey is the standard car that most people build first. The parts cost about $250 to $300 and take 2 hours to assemble. Here are the main steps to build your own car: Assemble hardware. Install software. Create Donkey App. Calibrate your car. Start driving. Train an autopilot. Experiment with simulator.","title":"Build your own Donkey"},{"location":"#hello-world","text":"Donkeycar is designed to make adding new parts to your car easy. Here's an example car application that captures images from the camera and saves them. import donkey as dk #initialize the vehicle V = dk.Vehicle() #add a camera part cam = dk.parts.PiCamera() V.add(cam, outputs=['image'], threaded=True) #add tub part to record images tub = dk.parts.Tub(path='~/mycar/data', inputs=['image'], types=['image_array']) V.add(tub, inputs=inputs) #start the vehicle's drive loop V.start(max_loop_count=100)","title":"Hello World."},{"location":"#installation","text":"How to install","title":"Installation"},{"location":"#why-the-name-donkey","text":"The ultimate goal of this project is to build something useful. Donkey's were one of the first domesticated pack animals, they're notoriously stubborn, and they are kid safe. Until the car can navigate from one side of a city to the other, we'll hold off naming it after some celestial being.","title":"Why the name Donkey?"},{"location":"contribute/","text":"Contribute to Donkey Donkey is an open source project to help accelerate the development of self driving autos. Guiding Development Principles Modularity : A self driving system is composed of standalone, independently configurable components that can be combined to make a car. Minimalism : Each component should be kept short (<100 lines of code). Each piece of code should be transparent upon first reading. No black magic, it slows the speed of innovation. Extensibility : New components should be simple to create by following a template. Python : Keep it simple. These guidelines are nearly copied from Keras , because they are so good Add a part Are you a hardware specialist that can write a donkey part wrapper for a GPS unit or a data scientist that can write an recursive neural net autopilot? If so please write a part so other people driving donkeys can use the part. How do parts work? Check out this overview Fix or report a bug If you find a problem with the code and you know how to fix it then please clone the repo, make your fix, and submit your pull request. Reply to issues Helping close or triage the issues is a good way to help. If You Need An Inspiration Search the code or docs for TODO to find places where you might be able to find a better solution. Improve the documentation You can fix grammar or provide clarity by clicking the the Edit on GitHub link in the top right corner. Here's a guide to how to create and edit docs.","title":"Contribute"},{"location":"contribute/#contribute-to-donkey","text":"Donkey is an open source project to help accelerate the development of self driving autos.","title":"Contribute to Donkey"},{"location":"contribute/#guiding-development-principles","text":"Modularity : A self driving system is composed of standalone, independently configurable components that can be combined to make a car. Minimalism : Each component should be kept short (<100 lines of code). Each piece of code should be transparent upon first reading. No black magic, it slows the speed of innovation. Extensibility : New components should be simple to create by following a template. Python : Keep it simple. These guidelines are nearly copied from Keras , because they are so good","title":"Guiding Development Principles"},{"location":"contribute/#add-a-part","text":"Are you a hardware specialist that can write a donkey part wrapper for a GPS unit or a data scientist that can write an recursive neural net autopilot? If so please write a part so other people driving donkeys can use the part. How do parts work? Check out this overview","title":"Add a part"},{"location":"contribute/#fix-or-report-a-bug","text":"If you find a problem with the code and you know how to fix it then please clone the repo, make your fix, and submit your pull request.","title":"Fix or report a bug"},{"location":"contribute/#reply-to-issues","text":"Helping close or triage the issues is a good way to help.","title":"Reply to issues"},{"location":"contribute/#if-you-need-an-inspiration","text":"Search the code or docs for TODO to find places where you might be able to find a better solution.","title":"If You Need An Inspiration"},{"location":"contribute/#improve-the-documentation","text":"You can fix grammar or provide clarity by clicking the the Edit on GitHub link in the top right corner. Here's a guide to how to create and edit docs.","title":"Improve the documentation"},{"location":"faq/","text":"FAQ What types of RC cars work with the donkey platform? Most hobby grade RC cars will work fine with the electronics, but you'll need to make your own base-plate and camera holder. To make sure the car will work with Donkey check these things. it has a separate ESC and receiver. Some cheaper cars have these combined so it would require soldering to connect the Donkey motor controller to the ESC. The ESC uses three-wire connectors. This will make it easy to just plug into the Donkey hardware. Brushed motors are easier because they can go slower but sensored brushless motors(w/sensored ESC) can work as well. For more information, see Roll Your Own . What car can I use if I'm not in the USA? The easiest thing to do would be to take your parts down to your local RC / hobby shop and check that the car you want works with the parts. Here are some parts people have said work in other countries. Australia: KAOS (functionally equivalent to the Exceed Magnet) China: HSP 94186 (functionally equivalent to the Exceed Magnet) Add your country to this list (click edit this in top left corner) How can I make my own track? You can use tape, ribbon or even rope. The most popular tracks are 4ft wide and have 2in white borders with a dashed yellow center line. The Oakland track is about 70 feet around the center line. Key race characteristics include: straightaways. left and right turns hairpin turn start/finish line. Will Donkey Work on different hardware? Yes. It's all python so you can run it on any system. Usually the hard part of porting Donkey will be getting the hardware working. Here are a couple systems that people have tried or talked about. NVIDA TX2 - This was implemented with a webcam and used a teensy to control the motor/servos. I2c control of PCA9685 works as well. Pi-Zero - Yes, try following the steps for the PiB/B+. They should work for the PiZero. After a reboot, I don't see the (donkey) in front of the prompt, and I get python errors when I run If you used this disc setup guide above, you used conda to manage your virtual environment. You need to activate the donkey conda environment with: conda activate donkey optionally you can add that line to the last line of your ~/.bashrc to have it active each time you login. How to get latest Donkey source When donkey has changed you can get the latest source. You've installed it directly from the github repo, so getting latest is easy: cd donkeycar git pull origin master donkey createcar --path ~/mycar --overwrite","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#what-types-of-rc-cars-work-with-the-donkey-platform","text":"Most hobby grade RC cars will work fine with the electronics, but you'll need to make your own base-plate and camera holder. To make sure the car will work with Donkey check these things. it has a separate ESC and receiver. Some cheaper cars have these combined so it would require soldering to connect the Donkey motor controller to the ESC. The ESC uses three-wire connectors. This will make it easy to just plug into the Donkey hardware. Brushed motors are easier because they can go slower but sensored brushless motors(w/sensored ESC) can work as well. For more information, see Roll Your Own .","title":"What types of RC cars work with the donkey platform?"},{"location":"faq/#what-car-can-i-use-if-im-not-in-the-usa","text":"The easiest thing to do would be to take your parts down to your local RC / hobby shop and check that the car you want works with the parts. Here are some parts people have said work in other countries. Australia: KAOS (functionally equivalent to the Exceed Magnet) China: HSP 94186 (functionally equivalent to the Exceed Magnet) Add your country to this list (click edit this in top left corner)","title":"What car can I use if I'm not in the USA?"},{"location":"faq/#how-can-i-make-my-own-track","text":"You can use tape, ribbon or even rope. The most popular tracks are 4ft wide and have 2in white borders with a dashed yellow center line. The Oakland track is about 70 feet around the center line. Key race characteristics include: straightaways. left and right turns hairpin turn start/finish line.","title":"How can I make my own track?"},{"location":"faq/#will-donkey-work-on-different-hardware","text":"Yes. It's all python so you can run it on any system. Usually the hard part of porting Donkey will be getting the hardware working. Here are a couple systems that people have tried or talked about. NVIDA TX2 - This was implemented with a webcam and used a teensy to control the motor/servos. I2c control of PCA9685 works as well. Pi-Zero - Yes, try following the steps for the PiB/B+. They should work for the PiZero.","title":"Will Donkey Work on different hardware?"},{"location":"faq/#after-a-reboot-i-dont-see-the-donkey-in-front-of-the-prompt-and-i-get-python-errors-when-i-run","text":"If you used this disc setup guide above, you used conda to manage your virtual environment. You need to activate the donkey conda environment with: conda activate donkey optionally you can add that line to the last line of your ~/.bashrc to have it active each time you login.","title":"After a reboot, I don't see the (donkey) in front of the prompt, and I get python errors when I run"},{"location":"faq/#how-to-get-latest-donkey-source","text":"When donkey has changed you can get the latest source. You've installed it directly from the github repo, so getting latest is easy: cd donkeycar git pull origin master donkey createcar --path ~/mycar --overwrite","title":"How to get latest Donkey source"},{"location":"legacy/","text":"Legacy This part of documentation it was left as reference to old original classic design, because it may still bring value for some Do-It-Yourself users. In the future this should be moved to another sections in the docs. Hardware If you purchased parts from the Donkey Car Store, skip to step 3. Step 1: Print Parts thingiverse I printed parts in black PLA, with .3mm layer height with a .5mm nozzle and no supports. The top roll bar is designed to be printed upside down. Step 2: Clean up parts Almost all 3D Printed parts will need clean up. Re-drill holes, and clean up excess plastic. In particular, clean up the slots in the side of the roll bar, as shown in the picture below: Step 3: Assemble Top plate and Roll Cage If you have an Exceed Short Course Truck, Blaze or Desert Monster watch this video Slide the nut into the slot in the side of the roll cage. This is not particularly easy. You may need to clean out the hole again and use a small screwdriver to push the screw in such that it lines up with the hole in the bottom of the roll cage. Once you have slid the nut in, you can attach the bottom plate. Once again, this may be tricky. I use the small screwdriver to push against the nut to keep it from spinning in the slot. Good news: you should never have to do this again. Step 4: Connect Servo Shield to Raspberry Pi You could do this after attaching the Raspberry Pi to the bottom plate, I just think it is easier to see the parts when they are laying on the workbench. Connect the parts as you see below: For reference, below is the Raspberry Pi Pinout for reference. You will notice we connect to 3.3v, the two I2C pins (SDA and SCL) and ground: Step 5: Attach Raspberry Pi to 3D Printed bottom plate Before you start, now is a good time to insert the already flashed SD card and bench test the electronics. Once that is done, attaching the Raspberry Pi and Servo is as simple as running screws through the board into the screw bosses on the top plate. The M2.5x12mm screws should be the perfect length to go through the board, the plastic and still have room for a washer. The \u201ccap\u201d part of the screw should be facing up and the nut should be on the bottom of the top plate. The ethernet and USB ports should face forward. This is important as it gives you access to the SD card and makes the camera ribbon cable line up properly. Attach the USB battery to the underside of the printed bottom plate using cable ties or velcro. Step 6: Attach Camera There are two versions of the donkey chassis, the newer one does not have screws, the older one does. This includes instructions for both: Screwless Design The newer design is pretty simple, just slip the camera into the slot, cable end first. However, be careful not to push on the camera lens and instead press the board. If you need to remove the camera the temptation is to push on the lens, instead push on the connector as is shown in these pictures. Design with Screws Attaching the camera is a little tricky, the M2 screws can be screwed into the plastic but it is a little hard. I recommend drilling the holes out with a 1.5mm bit (1/16th bit in Imperial land) then pre threading them with the screws before putting the camera on. It is only necessary to put two screws in. Sometimes using the two top screw holes can result in a short. Put screws in the bottom two holes. Before using the car, remove the plastic film from the camera lens. It is easy to put the camera cable in the wrong way so look at these photos and make sure the cable is put in properly. There are loads of tutorials on youtube if you are not used to this. Step 7: Put it all together Note if you have a Desert Monster Chassis see 7B section below The final steps are straightforward. First attach the roll bar assembly to the car. This is done using the same pins that came with the vehicle. Second run the servo cables up to the car. The throttle cable runs to channel 0 on the servo controller and steering is channel 1. Now you are done with the hardware!! Step 7b: Attach Adapters (Desert Monster only) The Desert monster does not have the same set up for holding the body on the car and needs two adapters mentioned above. To attach the adapters you must first remove the existing adapter from the chassis and screw on the custom adapter with the same screws as is shown in this photo: Once this is done, go back to step 7 Software Congrats! Now to get your get your car moving, see the software instructions section.","title":"Legacy"},{"location":"legacy/#legacy","text":"This part of documentation it was left as reference to old original classic design, because it may still bring value for some Do-It-Yourself users. In the future this should be moved to another sections in the docs.","title":"Legacy"},{"location":"legacy/#hardware","text":"If you purchased parts from the Donkey Car Store, skip to step 3.","title":"Hardware"},{"location":"legacy/#step-1-print-parts","text":"thingiverse I printed parts in black PLA, with .3mm layer height with a .5mm nozzle and no supports. The top roll bar is designed to be printed upside down.","title":"Step 1: Print Parts"},{"location":"legacy/#step-2-clean-up-parts","text":"Almost all 3D Printed parts will need clean up. Re-drill holes, and clean up excess plastic. In particular, clean up the slots in the side of the roll bar, as shown in the picture below:","title":"Step 2: Clean up parts"},{"location":"legacy/#step-3-assemble-top-plate-and-roll-cage","text":"If you have an Exceed Short Course Truck, Blaze or Desert Monster watch this video Slide the nut into the slot in the side of the roll cage. This is not particularly easy. You may need to clean out the hole again and use a small screwdriver to push the screw in such that it lines up with the hole in the bottom of the roll cage. Once you have slid the nut in, you can attach the bottom plate. Once again, this may be tricky. I use the small screwdriver to push against the nut to keep it from spinning in the slot. Good news: you should never have to do this again.","title":"Step 3: Assemble Top plate and Roll Cage"},{"location":"legacy/#step-4-connect-servo-shield-to-raspberry-pi","text":"You could do this after attaching the Raspberry Pi to the bottom plate, I just think it is easier to see the parts when they are laying on the workbench. Connect the parts as you see below: For reference, below is the Raspberry Pi Pinout for reference. You will notice we connect to 3.3v, the two I2C pins (SDA and SCL) and ground:","title":"Step 4: Connect Servo Shield to Raspberry Pi"},{"location":"legacy/#step-5-attach-raspberry-pi-to-3d-printed-bottom-plate","text":"Before you start, now is a good time to insert the already flashed SD card and bench test the electronics. Once that is done, attaching the Raspberry Pi and Servo is as simple as running screws through the board into the screw bosses on the top plate. The M2.5x12mm screws should be the perfect length to go through the board, the plastic and still have room for a washer. The \u201ccap\u201d part of the screw should be facing up and the nut should be on the bottom of the top plate. The ethernet and USB ports should face forward. This is important as it gives you access to the SD card and makes the camera ribbon cable line up properly. Attach the USB battery to the underside of the printed bottom plate using cable ties or velcro.","title":"Step 5: Attach Raspberry Pi to 3D Printed bottom plate"},{"location":"legacy/#step-6-attach-camera","text":"There are two versions of the donkey chassis, the newer one does not have screws, the older one does. This includes instructions for both: Screwless Design The newer design is pretty simple, just slip the camera into the slot, cable end first. However, be careful not to push on the camera lens and instead press the board. If you need to remove the camera the temptation is to push on the lens, instead push on the connector as is shown in these pictures. Design with Screws Attaching the camera is a little tricky, the M2 screws can be screwed into the plastic but it is a little hard. I recommend drilling the holes out with a 1.5mm bit (1/16th bit in Imperial land) then pre threading them with the screws before putting the camera on. It is only necessary to put two screws in. Sometimes using the two top screw holes can result in a short. Put screws in the bottom two holes. Before using the car, remove the plastic film from the camera lens. It is easy to put the camera cable in the wrong way so look at these photos and make sure the cable is put in properly. There are loads of tutorials on youtube if you are not used to this.","title":"Step 6: Attach Camera"},{"location":"legacy/#step-7-put-it-all-together","text":"Note if you have a Desert Monster Chassis see 7B section below The final steps are straightforward. First attach the roll bar assembly to the car. This is done using the same pins that came with the vehicle. Second run the servo cables up to the car. The throttle cable runs to channel 0 on the servo controller and steering is channel 1. Now you are done with the hardware!!","title":"Step 7: Put it all together"},{"location":"legacy/#step-7b-attach-adapters-desert-monster-only","text":"The Desert monster does not have the same set up for holding the body on the car and needs two adapters mentioned above. To attach the adapters you must first remove the existing adapter from the chassis and screw on the custom adapter with the same screws as is shown in this photo: Once this is done, go back to step 7","title":"Step 7b: Attach Adapters (Desert Monster only)"},{"location":"legacy/#software","text":"Congrats! Now to get your get your car moving, see the software instructions section.","title":"Software"},{"location":"release/","text":"Release notes Notes on how to release donkey. Create a startup disk. Download the previous disk image and create the startup disk. Move disk to you pi. Pull the lastest donkeycar code. Make your changes. Move the disk back to your computer. Clean up : Remove your wi-fi password Change the host name to donkeypi Delete .gitconfig . Create the disk image from the SD card Run sudo gparted to see the size of the disk partitions. Resize the partitions to be as small as possible. Right click the partition to see the last sector of the partition. Run: bash sudo dd if=/dev/mmcblk0 of=~/donkey_2.5.0_pi3.img bs=512 count=<last sector> status=progress Zip the .img file and upload to Dropbox. Update the link in the instructions. Create a release Run the tests on computer and pi. pytest Update versions in __init__ and setup.py","title":"Releases"},{"location":"release/#release-notes","text":"Notes on how to release donkey.","title":"Release notes"},{"location":"release/#create-a-startup-disk","text":"Download the previous disk image and create the startup disk. Move disk to you pi. Pull the lastest donkeycar code. Make your changes. Move the disk back to your computer. Clean up : Remove your wi-fi password Change the host name to donkeypi Delete .gitconfig . Create the disk image from the SD card Run sudo gparted to see the size of the disk partitions. Resize the partitions to be as small as possible. Right click the partition to see the last sector of the partition. Run: bash sudo dd if=/dev/mmcblk0 of=~/donkey_2.5.0_pi3.img bs=512 count=<last sector> status=progress Zip the .img file and upload to Dropbox. Update the link in the instructions.","title":"Create a startup disk."},{"location":"release/#create-a-release","text":"Run the tests on computer and pi. pytest Update versions in __init__ and setup.py","title":"Create a release"},{"location":"roll_your_own/","text":"Roll Your Own Car The Quick and Dirty Your car needs to be easy to control from a Raspberry Pi Your car needs to be not too large, because it will be too heavy and dangerous (and expensive) Your car needs to be not too small, because it needs to carry a certain minimum amount of equipment Your car needs to meet minimum performance standards in power and control for the model to make sense for it Your car needs to be smooth to control even at low speeds This generally means: Your car needs to have a speed controller for the motor (ESC) that takes a standard RC 3-pin control signal (RC PWM style) Your car needs to have a steering servo that takes a standard RC 3-pin control signal (RC PWM style) Your car needs to have a radio receiver that contains standard 100-mil (2.54 mm) pin headers for each of the ESC and the steering servo. Your car needs to be between 1/18th scale (smallest end) and 1/8th scale (largest end) if you want to race in the DIYRobocars race. Your car needs to either use a brushed motor, or a sensored brushless motor. Sensorless brushless motors are too rough at low speeds. If you buy a car with a brushless motor included it is invariably a sensorless brushless motor and will need to be replaced along with the ESC. Other options are perhaps possible, see the end of this document. Many car builders end up looking at \"integrated\" RC hobby cars, because they are typically cheaper. However, the reason these are cheaper, is that they will integrate many parts of electronics and mechanics into a single package, which means that we can't intersect the appropriate signals to control the car with a Raspberry Pi. In fact, the expected signals may not even exist at all in an integrated car. Here is an example of an integrated RX and ESC - typically these should be avoided: You also need to know some things about electronics, such as the difference between power rails and control signals, what the duration of a microsecond is, and how Volts, Amperes, Watts, Hours, Ohms, and other measurement units relate. Chassis build While there are lots of designs out there besides the Donkeycar, but two stand out and are worth mentioning specifically. Chilicorn rail This is a flexible mounting system developed by Markku.ai. markku.ai markku-ai github sCAD Files Doug LaRue, a long time community member has extensive designs for making your own chassis in sCAD. If you want to roll your own but are not comfortable with CAD this is a good place to start. Basic Donkeycar 1/28 scale car inverted donkey car Servo Specifics An RC servo is used for controlling the steering wheels of the car. This servo typically expects around 4.8V to 6V input on the power wire (varies by car) and a PWM control signal on the signal wire. Typically, the three wires are colored black-red-white, or brown-red-yellow, where the dark wire (black/brown) is ground, and the center wire (red) is power, and the light wire (white/yellow) is control. The control signal is RC-style PWM, where one pulse is sent 60 times a second, and the width of this pulse controls how left/right the servo turns. When this pulse is 1500 microseconds, the servo is centered; when the pulse is 1000 microseconds, the servo is turned all the way left (or right) and when the pulse is 2000 microseconds, the servo is turned all the way in the other direction. This is NOT the same kind of PWM that you would use to control the duty cycle of a motor, or the brightness of a LED. The power for the servo typically comes from the motor ESC, which has a BEC (Battery Eliminator Circuit) built in. ESC Specifics The role of the ESC is to take a RC PWM control signal (pulse between 1000 and 2000 microseconds) in, and use that to control the power to the motor so the motor spins with different amounts of power in forward or reverse. Again, 1500 microseconds typically means \"center\" which for the motor means \"dead stop.\" The battery typically connects straight to the ESC using thicker wiring than the simple control signals, because the motor draws many more amps than the control. The ESC then connects on to the motor with equally thick power wiring. The standard Donkey motor and ESC probably have a peak current of about 12A; a 1/8th scale RC car with powerful brushless motor can have a peak draw up to 200A! Additionally, the ESC typically contains a linear or switching voltage converter that outputs the power needed to control the steering servo; this is typically somewhere in the 4.8V to 6V range. Most BECs built into ESCs will not deliver more than about 1A of current, so it is not typically possible to power both the steering servo and the Raspberry Pi from the BEC. Receiver Specifics If you buy a \"kit car\" that is listed as \"needs a receiver,\" then you don't need to buy a receiver. The Raspberry Pi plus the PCA9685 board take the role of the receiver, outputting control signals to the car. Buying a \"kit car\" that comes with steering servo, motor, and ESC, but not with radio, is actually a great way to make sure that the car you build will have the right signalling, because any RC car with separate receiver will be designed for the appropriate PWM signals. If your car comes with a receiver, make sure it has the appropriate three-pin headers next to each other for steering servo and for ESC control. Some receivers may have additional three-pin headers for additional channels, which may be empty or may control fancy attachments like horns, lights, and so forth. There is a modification to the Donkey car which uses the RC radio to drive the car when collecting training data; this will give better control of the car than you typically get with a PlayStation controller, or cell phone. However, it also requires replacing the PCA9685 board with an external micro-controller, and changing the software of the Donkey to use it. Finally, some receivers can output, in addition to the PWM control signals, a serial data packet that contains the control signals. An example of such a receiver is the FS-i6B, which has 6 output channels for PWM signals, but can output 10 channels of data at 115,200 bps as serial data, which you can read with an external micro-controller, or perhaps even with the Raspberry Pi (requires re-configuration of the Pi boot loader, and custom modifications to the donkey software.) Batteries The Donkey comes with a Nickel Metal Hydride battery (NiMH) which is just enough to make its motor go, for a little bit of time (5-10 minutes) before needing a recharge. The specifications on this battery are 6 cells, 1100 mAh. Because NiHM batteries range from 0.9V to 1.35V with a \"nominal\" voltage of 1.2V, you can expect to see voltages in the 5.4V to 8.1V range. NiHM batteries have medium energy capacity per weight and volume. Thus, you can improve the runtime and performance of the Magnet car by upgrading to a Lithium Polymer battery (LiPo.) Typically, you will get a 2 cell battery (2S) and Lithium batteries have 3.2V to 4.2V per cell, so you will see voltages in the 6.4V to 8.4V range. Additionally, Lithium Polymer batteries generally have higher current capacity (amount of Amps the battery can deliver at one point while driving) as well as energy storage (number of Amp Hours the battery stores when fully charged) so it may also last longer. Note that the amount of charge a battery can hold (how long it runs) is measured in Ampere-hours (Ah), or milli-Ampere-hours (mAh), whereas the amount of current a battery can instantaneously deliver while driving is measured simply in Amperes. But to make things more confusing, Amperes are often re-calculated in terms of multiples of the energy content, divided by one hour; this ratio is often called \"C.\" Thus, a LiPo rated for 10C and 2000 mAh, can deliver 20 Amperes of current while driving. A NiHM rated for 5C and 1100 mAh can deliver 5.5 Amperes of current while driving. Batteries typically will deliver more than the C rating for very short amounts of time, but will heat up or build up internal resistance such that that is not something you can rely on for normal operation. For your custom car, be aware of the voltages needed for the ESC and motor of the car, and make sure to get a battery that matches in voltage. Smaller RC cars will come with NiMH for affordability, or 2S LiPo for power. Larger RC cars will use 3S (11.1V) or 4S (14.8V) or even 6S (22.2V) Lithium batteries, and thus need to have ESC and motor combinations to match. Finally, be sure to get a charger that matches your battery. If you have a LiPo battery, get a good Lithium battery charger, with a balancing plug that matches your battery. Never discharge a Lithium battery below 3.2V per cell; if you let it run dead, it will not want to be charged up to normal voltage again, and trying to do so may very well overheat the batter and light it on fire! See YouTube pictures of burning Teslas for what that can look like. Seriously, houses have burned down because people have tried to save $10 by re-charging a Lithium battery that they forgot to disconnect and it ran down too much. It's not worth it. Instead, get a battery alarm, that you plug into the battery balance connector, and it beeps when the battery has discharged so much that you should disconnect and recharge it. Physical Constraints Adding the additional battery and electronics for self-driving to a toy car will add more load than the car was initially designed for. For a large, 1/8th scale car, this may not be much of a problem. For a small car, 1/18th scale or below, the additional weight and top-heaviness will cause the car to not react well to the steering output, which may cause the self-driving model to be less able to control the car. If you use a car that's not the standard Magnet, at a minimum, you will have to figure out how to mount all the hardware securely. Just piling things on and hoping wiring will keep it in place will not work for things that actually drive and turn. Finding good mounting points, and making your own \"base plate\" with measurements from the car you have, is likely to be necessary. You can build this base plate using 3D printing, laser cutting, CNC milling, or even just drilling into a thin piece of plywood, but getting a good fit to your chassis is important, so don't rush it or try to cut corners. Doug LaRue also built a configurator in Thingiverse that enables people to easily make custom 3D printed plates. Other Options Yes, you can make a self-driving car out of your 1/5th scale Nitro Dragster. You will just have to learn even more about the different bits and pieces of the solution, and figure out all the necessary integration yourself. The control signals for a Nitro car are the same, so this might not even be hard. However, the indoors arenas used for Donkey Racing Meetups do not allow fuel-burning cars, only electric. Yes, you can make a self-driving car out of a cheap two-wheel chassis that uses a LM298 H-bridge with direct PWM control to \"tank steer\" two wheels. However, you will have to adapt the Donkey software to output the right steering controls, and you will additionally have to figure out how to wire up the H-bridge to the Pi in a way that makes sense to you; the PWM signals output by the PCA9685 board are the RC control kind, NOT the motor control kind! Also, most affordable two-wheel-drive robot chassis are not actually big enough, strong enough, and mechanically consistent enough to make for good Donkey Car candidates.","title":"Roll Your Own"},{"location":"roll_your_own/#roll-your-own-car","text":"","title":"Roll Your Own Car"},{"location":"roll_your_own/#the-quick-and-dirty","text":"Your car needs to be easy to control from a Raspberry Pi Your car needs to be not too large, because it will be too heavy and dangerous (and expensive) Your car needs to be not too small, because it needs to carry a certain minimum amount of equipment Your car needs to meet minimum performance standards in power and control for the model to make sense for it Your car needs to be smooth to control even at low speeds This generally means: Your car needs to have a speed controller for the motor (ESC) that takes a standard RC 3-pin control signal (RC PWM style) Your car needs to have a steering servo that takes a standard RC 3-pin control signal (RC PWM style) Your car needs to have a radio receiver that contains standard 100-mil (2.54 mm) pin headers for each of the ESC and the steering servo. Your car needs to be between 1/18th scale (smallest end) and 1/8th scale (largest end) if you want to race in the DIYRobocars race. Your car needs to either use a brushed motor, or a sensored brushless motor. Sensorless brushless motors are too rough at low speeds. If you buy a car with a brushless motor included it is invariably a sensorless brushless motor and will need to be replaced along with the ESC. Other options are perhaps possible, see the end of this document. Many car builders end up looking at \"integrated\" RC hobby cars, because they are typically cheaper. However, the reason these are cheaper, is that they will integrate many parts of electronics and mechanics into a single package, which means that we can't intersect the appropriate signals to control the car with a Raspberry Pi. In fact, the expected signals may not even exist at all in an integrated car. Here is an example of an integrated RX and ESC - typically these should be avoided: You also need to know some things about electronics, such as the difference between power rails and control signals, what the duration of a microsecond is, and how Volts, Amperes, Watts, Hours, Ohms, and other measurement units relate.","title":"The Quick and Dirty"},{"location":"roll_your_own/#chassis-build","text":"While there are lots of designs out there besides the Donkeycar, but two stand out and are worth mentioning specifically.","title":"Chassis build"},{"location":"roll_your_own/#chilicorn-rail","text":"This is a flexible mounting system developed by Markku.ai. markku.ai markku-ai github","title":"Chilicorn rail"},{"location":"roll_your_own/#scad-files","text":"Doug LaRue, a long time community member has extensive designs for making your own chassis in sCAD. If you want to roll your own but are not comfortable with CAD this is a good place to start. Basic Donkeycar 1/28 scale car inverted donkey car","title":"sCAD Files"},{"location":"roll_your_own/#servo-specifics","text":"An RC servo is used for controlling the steering wheels of the car. This servo typically expects around 4.8V to 6V input on the power wire (varies by car) and a PWM control signal on the signal wire. Typically, the three wires are colored black-red-white, or brown-red-yellow, where the dark wire (black/brown) is ground, and the center wire (red) is power, and the light wire (white/yellow) is control. The control signal is RC-style PWM, where one pulse is sent 60 times a second, and the width of this pulse controls how left/right the servo turns. When this pulse is 1500 microseconds, the servo is centered; when the pulse is 1000 microseconds, the servo is turned all the way left (or right) and when the pulse is 2000 microseconds, the servo is turned all the way in the other direction. This is NOT the same kind of PWM that you would use to control the duty cycle of a motor, or the brightness of a LED. The power for the servo typically comes from the motor ESC, which has a BEC (Battery Eliminator Circuit) built in.","title":"Servo Specifics"},{"location":"roll_your_own/#esc-specifics","text":"The role of the ESC is to take a RC PWM control signal (pulse between 1000 and 2000 microseconds) in, and use that to control the power to the motor so the motor spins with different amounts of power in forward or reverse. Again, 1500 microseconds typically means \"center\" which for the motor means \"dead stop.\" The battery typically connects straight to the ESC using thicker wiring than the simple control signals, because the motor draws many more amps than the control. The ESC then connects on to the motor with equally thick power wiring. The standard Donkey motor and ESC probably have a peak current of about 12A; a 1/8th scale RC car with powerful brushless motor can have a peak draw up to 200A! Additionally, the ESC typically contains a linear or switching voltage converter that outputs the power needed to control the steering servo; this is typically somewhere in the 4.8V to 6V range. Most BECs built into ESCs will not deliver more than about 1A of current, so it is not typically possible to power both the steering servo and the Raspberry Pi from the BEC.","title":"ESC Specifics"},{"location":"roll_your_own/#receiver-specifics","text":"If you buy a \"kit car\" that is listed as \"needs a receiver,\" then you don't need to buy a receiver. The Raspberry Pi plus the PCA9685 board take the role of the receiver, outputting control signals to the car. Buying a \"kit car\" that comes with steering servo, motor, and ESC, but not with radio, is actually a great way to make sure that the car you build will have the right signalling, because any RC car with separate receiver will be designed for the appropriate PWM signals. If your car comes with a receiver, make sure it has the appropriate three-pin headers next to each other for steering servo and for ESC control. Some receivers may have additional three-pin headers for additional channels, which may be empty or may control fancy attachments like horns, lights, and so forth. There is a modification to the Donkey car which uses the RC radio to drive the car when collecting training data; this will give better control of the car than you typically get with a PlayStation controller, or cell phone. However, it also requires replacing the PCA9685 board with an external micro-controller, and changing the software of the Donkey to use it. Finally, some receivers can output, in addition to the PWM control signals, a serial data packet that contains the control signals. An example of such a receiver is the FS-i6B, which has 6 output channels for PWM signals, but can output 10 channels of data at 115,200 bps as serial data, which you can read with an external micro-controller, or perhaps even with the Raspberry Pi (requires re-configuration of the Pi boot loader, and custom modifications to the donkey software.)","title":"Receiver Specifics"},{"location":"roll_your_own/#batteries","text":"The Donkey comes with a Nickel Metal Hydride battery (NiMH) which is just enough to make its motor go, for a little bit of time (5-10 minutes) before needing a recharge. The specifications on this battery are 6 cells, 1100 mAh. Because NiHM batteries range from 0.9V to 1.35V with a \"nominal\" voltage of 1.2V, you can expect to see voltages in the 5.4V to 8.1V range. NiHM batteries have medium energy capacity per weight and volume. Thus, you can improve the runtime and performance of the Magnet car by upgrading to a Lithium Polymer battery (LiPo.) Typically, you will get a 2 cell battery (2S) and Lithium batteries have 3.2V to 4.2V per cell, so you will see voltages in the 6.4V to 8.4V range. Additionally, Lithium Polymer batteries generally have higher current capacity (amount of Amps the battery can deliver at one point while driving) as well as energy storage (number of Amp Hours the battery stores when fully charged) so it may also last longer. Note that the amount of charge a battery can hold (how long it runs) is measured in Ampere-hours (Ah), or milli-Ampere-hours (mAh), whereas the amount of current a battery can instantaneously deliver while driving is measured simply in Amperes. But to make things more confusing, Amperes are often re-calculated in terms of multiples of the energy content, divided by one hour; this ratio is often called \"C.\" Thus, a LiPo rated for 10C and 2000 mAh, can deliver 20 Amperes of current while driving. A NiHM rated for 5C and 1100 mAh can deliver 5.5 Amperes of current while driving. Batteries typically will deliver more than the C rating for very short amounts of time, but will heat up or build up internal resistance such that that is not something you can rely on for normal operation. For your custom car, be aware of the voltages needed for the ESC and motor of the car, and make sure to get a battery that matches in voltage. Smaller RC cars will come with NiMH for affordability, or 2S LiPo for power. Larger RC cars will use 3S (11.1V) or 4S (14.8V) or even 6S (22.2V) Lithium batteries, and thus need to have ESC and motor combinations to match. Finally, be sure to get a charger that matches your battery. If you have a LiPo battery, get a good Lithium battery charger, with a balancing plug that matches your battery. Never discharge a Lithium battery below 3.2V per cell; if you let it run dead, it will not want to be charged up to normal voltage again, and trying to do so may very well overheat the batter and light it on fire! See YouTube pictures of burning Teslas for what that can look like. Seriously, houses have burned down because people have tried to save $10 by re-charging a Lithium battery that they forgot to disconnect and it ran down too much. It's not worth it. Instead, get a battery alarm, that you plug into the battery balance connector, and it beeps when the battery has discharged so much that you should disconnect and recharge it.","title":"Batteries"},{"location":"roll_your_own/#physical-constraints","text":"Adding the additional battery and electronics for self-driving to a toy car will add more load than the car was initially designed for. For a large, 1/8th scale car, this may not be much of a problem. For a small car, 1/18th scale or below, the additional weight and top-heaviness will cause the car to not react well to the steering output, which may cause the self-driving model to be less able to control the car. If you use a car that's not the standard Magnet, at a minimum, you will have to figure out how to mount all the hardware securely. Just piling things on and hoping wiring will keep it in place will not work for things that actually drive and turn. Finding good mounting points, and making your own \"base plate\" with measurements from the car you have, is likely to be necessary. You can build this base plate using 3D printing, laser cutting, CNC milling, or even just drilling into a thin piece of plywood, but getting a good fit to your chassis is important, so don't rush it or try to cut corners. Doug LaRue also built a configurator in Thingiverse that enables people to easily make custom 3D printed plates.","title":"Physical Constraints"},{"location":"roll_your_own/#other-options","text":"Yes, you can make a self-driving car out of your 1/5th scale Nitro Dragster. You will just have to learn even more about the different bits and pieces of the solution, and figure out all the necessary integration yourself. The control signals for a Nitro car are the same, so this might not even be hard. However, the indoors arenas used for Donkey Racing Meetups do not allow fuel-burning cars, only electric. Yes, you can make a self-driving car out of a cheap two-wheel chassis that uses a LM298 H-bridge with direct PWM control to \"tank steer\" two wheels. However, you will have to adapt the Donkey software to output the right steering controls, and you will additionally have to figure out how to wire up the H-bridge to the Pi in a way that makes sense to you; the PWM signals output by the PCA9685 board are the RC control kind, NOT the motor control kind! Also, most affordable two-wheel-drive robot chassis are not actually big enough, strong enough, and mechanically consistent enough to make for good Donkey Car candidates.","title":"Other Options"},{"location":"supported_cars/","text":"Supported cars Magnet and HSP 94186 The magnet chassis was the first standard Donkey build. However in many cases it may not be available. Try searching for both the Magnet and HSP 94186 on ebay, banggood, ali express etc. The HSP 94186 is the same as the Magnet and will work. If you speak mandarin it is always available on Taobao. Taobao item offer Exceed Desert Monster, Short Course Truck, and Blaze The Desert Monster, SCT and Blaze are made by the same manufacturer as the Magnet and has the same motor and ESC. The chassis is slightly different so it requires an adapter and some extra hardware to work with the standard donkey platform. With the adapters the camera placement will be identical to the Magnet and should be able to share models. It is worth noting that the Desert Monster and SCT also has some nice characteristics including narrower, more road friendly tires and the Blaze has a slightly narrower stance which makes it less likely to hit things. To purchase one of these cars follow the following links: Exceed Magnet Blue Exceed Desert Monster Green Exceed Short Course Truck Green , Red Exceed Blaze Blue , Yellow Wild Blue Max Red Exceed Blaze Blue , Yellow , Wild Blue , Max Red To assemble one of these you will need some additional parts than the standard build, these can be purchased as a kit on the donkey store at: Purchase: Donkey Store Part Description Link Approximate Cost 3D printed Adapters Files: thingiverse.com/thing:2260575 $10 Chassis Clips Amazon $5 To assemble first remove the plastic cover and roll cage then unscrew the posts that hold up the cover and replace with the adapters. Visual instructions to follow. LaTrax Prerunner The LaTrax prerunner is a supported car and follows the same build instructions as the Desert Monster. However the adapters get screwed in as is shown in the photo below. Donkey Pro To build a donkey pro the following parts are Needed Part Description Link Approximate Cost Donkey Pro Plastics and base Thingiverse or Donkeystore $50 (8) M2.5 standoff (8) M2.5 Nylock nuts (8) M2.5x6mm socket head cap screws (4) M3x10 plastic self threading screw To assemble the Raspberry pi to the chassis this assembly picture should clarify how it fits together. Tamaya TT-01 (Advanced Build) The TT-01 is a new build that is a higher end version of the Donkey. This is an advanced build and requires existing RC skills or the desire to learn them - along with some willingness to trial and error. For first time builders we recommend the Magnet. That said, it has some pros and cons that people should be aware of, presented below. Pros: Better kinematics and traction on smooth surfaces - basically this means it will corner better Larger build area for adding other sensors. Globally available with several clones. Cons: Assembly required! - you will need to supply your own ESC, battery, servo, pinion gear and motor. Needs to run on a smooth surface like a driveway or parking lot. Larger size requires a larger 3D printer to print chassis, otherwise purchase at the Donkeystore. More expensive In addition to the standard donkey parts, Raspberry Pi etc, you will need to buy the following components. Part Description Link Approximate Cost TT-01 Clone Chassis amazon other TT01s may be used $130 ESC Hobbyking 10.60 Brushed Motor Hobbyking $5 Steering Servo Hobbyking $5 Battery Hobbyking or similar 2S 5000 mAh battery $21 Pinion Gear Amazon $7 TT01 Plastics Thingiverse or Donkeystore $50 Note: purchasing from Hobbyking is tricky. They can ship from multiple warehouses and it can be expensive and time consuming if shipping from one overseas. You may need to buy an alternate component if one of the items above are not available in your local warehouse. If You Want to Roll Your Own It's totally possible to diverge from the main Donkey build, and still have a car that drives well and is fun to work with. We've seen a large variety of cars in the various Donkey competitions around the world. However, when you want to diverge, there are several things you need to know, or you will not be successful. There are many cost and quality trade-offs where the lower cost options simply won't work. We've already worked hard to find the cheapest available options that will work, so you should not expect to choose other options to save money. Rolling your own is more about learning, experimentation, and going to new and uncharged places. To find out more about what you need, see Roll Your Own .","title":"Supported Cars"},{"location":"supported_cars/#supported-cars","text":"","title":"Supported cars"},{"location":"supported_cars/#magnet-and-hsp-94186","text":"The magnet chassis was the first standard Donkey build. However in many cases it may not be available. Try searching for both the Magnet and HSP 94186 on ebay, banggood, ali express etc. The HSP 94186 is the same as the Magnet and will work. If you speak mandarin it is always available on Taobao. Taobao item offer","title":"Magnet and HSP 94186"},{"location":"supported_cars/#exceed-desert-monster-short-course-truck-and-blaze","text":"The Desert Monster, SCT and Blaze are made by the same manufacturer as the Magnet and has the same motor and ESC. The chassis is slightly different so it requires an adapter and some extra hardware to work with the standard donkey platform. With the adapters the camera placement will be identical to the Magnet and should be able to share models. It is worth noting that the Desert Monster and SCT also has some nice characteristics including narrower, more road friendly tires and the Blaze has a slightly narrower stance which makes it less likely to hit things. To purchase one of these cars follow the following links: Exceed Magnet Blue Exceed Desert Monster Green Exceed Short Course Truck Green , Red Exceed Blaze Blue , Yellow Wild Blue Max Red Exceed Blaze Blue , Yellow , Wild Blue , Max Red To assemble one of these you will need some additional parts than the standard build, these can be purchased as a kit on the donkey store at: Purchase: Donkey Store Part Description Link Approximate Cost 3D printed Adapters Files: thingiverse.com/thing:2260575 $10 Chassis Clips Amazon $5 To assemble first remove the plastic cover and roll cage then unscrew the posts that hold up the cover and replace with the adapters. Visual instructions to follow.","title":"Exceed Desert Monster, Short Course Truck, and Blaze"},{"location":"supported_cars/#latrax-prerunner","text":"The LaTrax prerunner is a supported car and follows the same build instructions as the Desert Monster. However the adapters get screwed in as is shown in the photo below.","title":"LaTrax Prerunner"},{"location":"supported_cars/#donkey-pro","text":"To build a donkey pro the following parts are Needed Part Description Link Approximate Cost Donkey Pro Plastics and base Thingiverse or Donkeystore $50 (8) M2.5 standoff (8) M2.5 Nylock nuts (8) M2.5x6mm socket head cap screws (4) M3x10 plastic self threading screw To assemble the Raspberry pi to the chassis this assembly picture should clarify how it fits together.","title":"Donkey Pro"},{"location":"supported_cars/#tamaya-tt-01-advanced-build","text":"The TT-01 is a new build that is a higher end version of the Donkey. This is an advanced build and requires existing RC skills or the desire to learn them - along with some willingness to trial and error. For first time builders we recommend the Magnet. That said, it has some pros and cons that people should be aware of, presented below. Pros: Better kinematics and traction on smooth surfaces - basically this means it will corner better Larger build area for adding other sensors. Globally available with several clones. Cons: Assembly required! - you will need to supply your own ESC, battery, servo, pinion gear and motor. Needs to run on a smooth surface like a driveway or parking lot. Larger size requires a larger 3D printer to print chassis, otherwise purchase at the Donkeystore. More expensive In addition to the standard donkey parts, Raspberry Pi etc, you will need to buy the following components. Part Description Link Approximate Cost TT-01 Clone Chassis amazon other TT01s may be used $130 ESC Hobbyking 10.60 Brushed Motor Hobbyking $5 Steering Servo Hobbyking $5 Battery Hobbyking or similar 2S 5000 mAh battery $21 Pinion Gear Amazon $7 TT01 Plastics Thingiverse or Donkeystore $50 Note: purchasing from Hobbyking is tricky. They can ship from multiple warehouses and it can be expensive and time consuming if shipping from one overseas. You may need to buy an alternate component if one of the items above are not available in your local warehouse.","title":"Tamaya TT-01 (Advanced Build)"},{"location":"supported_cars/#if-you-want-to-roll-your-own","text":"It's totally possible to diverge from the main Donkey build, and still have a car that drives well and is fun to work with. We've seen a large variety of cars in the various Donkey competitions around the world. However, when you want to diverge, there are several things you need to know, or you will not be successful. There are many cost and quality trade-offs where the lower cost options simply won't work. We've already worked hard to find the cheapest available options that will work, so you should not expect to choose other options to save money. Rolling your own is more about learning, experimentation, and going to new and uncharged places. To find out more about what you need, see Roll Your Own .","title":"If You Want to Roll Your Own"},{"location":"tests/","text":"Tests There is a limited test suite to ensure that the your changes to the code don't break something unintended. Run all the tests Look into the .travis.yml for a more detailed commands used for tests: install section is used to install required packages and setting up environment. script section is actually used to run test suite jobs section may contain another tasks related to other things like tests or deployments. Notice: in .travis.yml env var named TRAVIS_PYTHON_VERSION is populated from python section, such as TRAVIS_PYTHON_VERSION=3.6 . Please refer to the travis documentation for more details. Code Organization The test code is in tests foders in the same folder as the code. This is to help keep the test code linked to the code its self. If you change the code, change the tests. :) TODO: Skip tests that require specific hardware.","title":"Tests"},{"location":"tests/#tests","text":"There is a limited test suite to ensure that the your changes to the code don't break something unintended.","title":"Tests"},{"location":"tests/#run-all-the-tests","text":"Look into the .travis.yml for a more detailed commands used for tests: install section is used to install required packages and setting up environment. script section is actually used to run test suite jobs section may contain another tasks related to other things like tests or deployments. Notice: in .travis.yml env var named TRAVIS_PYTHON_VERSION is populated from python section, such as TRAVIS_PYTHON_VERSION=3.6 . Please refer to the travis documentation for more details.","title":"Run all the tests"},{"location":"tests/#code-organization","text":"The test code is in tests foders in the same folder as the code. This is to help keep the test code linked to the code its self. If you change the code, change the tests. :) TODO: Skip tests that require specific hardware.","title":"Code Organization"},{"location":"dev_guide/docs/","text":"How to create and edit docs This guide will show you how to create and edit docs in markdown format, and submit them for integration into the Donkeycar official docs. Create Donkeycar from Template Create a set of files to control your Donkey with this command: donkey createcar --path ~/mycar See also more information on createcar. Configure Options Look at myconfig.py in your newly created directory, ~/mycar cd ~/mycar nano myconfig.py Each line has a comment mark. The commented text shows the default value. When you want to make an edit to over-write the default, uncomment the line by removing the # and any spaces before the first character of the option. example: # STEERING_LEFT_PWM = 460 becomes: STEERING_LEFT_PWM = 500 when edited. You will adjust these later in the calibrate section. Configure I2C PCA9685 If you are using a PCA9685 card, make sure you can see it on I2C. Jetson Nano : sudo usermod -aG i2c $USER sudo reboot After a reboot, then try: sudo i2cdetect -r -y 1 Raspberry Pi : sudo apt-get install -y i2c-tools sudo i2cdetect -y 1 This should show you a grid of addresses like: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- In this case, the 40 shows up as the address of our PCA9685 board. If this does not show up, then check your wiring to the board. On a pi, ensure I2C is enabled in menu of sudo raspi-config (notice, it suggest reboot). If you have assigned a non-standard address to your board, then adjust the address in the myconfig.py under variable PCA9685_I2C_ADDR . If your board is on another bus, then you can specify that with the PCA9685_I2C_BUSNUM . Jetson Nano : set PCA9685_I2C_BUSNUM = 1 in your myconfig.py . For the pi, this will be auto detected by the Adafruit library. But not on the Jetson Nano. Sombrero Setup Set HAVE_SOMBRERO = True in your myconfig.py if you have a sombrero board. Robo HAT MM1 Setup Set HAVE_ROBOHAT = True in your myconfig.py if you have a Robo HAT MM1 board. Also set the following variables according to your setup. Most people will be using the below values, however, if you are using a Jetson Nano, please set MM1_SERIAL_PORT = '/dev/ttyTHS1' #ROBOHAT MM1 HAVE_ROBOHAT = False # set to true when using the Robo HAT MM1 from Robotics Masters. This will change to RC Control. MM1_STEERING_MID = 1500 # Adjust this value if your car cannot run in a straight line MM1_MAX_FORWARD = 2000 # Max throttle to go fowrward. The bigger the faster MM1_STOPPED_PWM = 1500 MM1_MAX_REVERSE = 1000 # Max throttle to go reverse. The smaller the faster MM1_SHOW_STEERING_VALUE = False # Serial port # -- Default Pi: '/dev/ttyS0' # -- Jetson Nano: '/dev/ttyTHS1' # -- Google coral: '/dev/ttymxc0' # -- Windows: 'COM3', Arduino: '/dev/ttyACM0' # -- MacOS/Linux:please use 'ls /dev/tty.*' to find the correct serial port for mm1 # eg.'/dev/tty.usbmodemXXXXXX' and replace the port accordingly MM1_SERIAL_PORT = '/dev/ttyS0' # Serial Port for reading and sending MM1 data (raspberry pi default) # adjust controller type as Robohat MM1 CONTROLLER_TYPE='MM1' # adjust drive train for web interface DRIVE_TRAIN_TYPE = 'MM1' The Robo HAT MM1 uses a RC Controller and CircuitPython script to drive the car during training. You must put the CircuitPython script onto the Robo HAT MM1 with your computer before you can continue. Download the CircuitPython Donkey Car Driver for Robo HAT MM1 to your computer from here Connect the MicroUSB connector on the Robo HAT MM1 to your computer's USB port. A CIRCUITPY device should appear on the computer as a USB Storage Device Copy the file downloaded in Step 1 to the CIRCUITPY USB Storage Device. Rename the file code.py . Unplug USB Cable from the Robo HAT MM1 and place on top of the Raspberry Pi, as you would any HAT. You may need to enable the hardware serial port on your Raspberry Pi. On your Raspberry Pi... Run the command sudo raspi-config Navigate to the 5 - Interfaceing options section. Navigate to the P6 - Serial section. When asked: Would you like a login shell to be accessible over serial? NO When asked: Would you like the serial port hardware to be enabled? YES Close raspi-config Restart If you would like additional hardware or software support with Robo HAT MM1, there are a few guides published on Hackster.io. They are listed below. Raspberry Pi + Robo HAT MM1 Jetson Nano + Robo HAT MM1 Simulator + Robo HAT MM1 Joystick setup If you plan to use a joystick, take a side track over to here . Camera Setup Raspberry Pi : If you are on a raspberry pi and using the recommended pi camera, then no changes are needed to your myconfg.py . Jetson Nano : When using a Sony IMX219 based camera, and you are using the default car template, then you will want edit your myconfg.py to have: CAMERA_TYPE = \"CSIC\" . For flipping the image vertically set CSIC_CAM_GSTREAMER_FLIP_PARM = 3 - this is helpful if you have to mount the camera in a rotated position. Set IMAGE_W = 224 and also IMAGE_H = 224 . CVCAM is a camera type that has worked for USB cameras when OpenCV is setup. This requires additional setup for OpenCV for Nano or OpenCV for Raspberry Pi . WEBCAM is a camera type that uses the pygame library, also typically for USB cameras. That requires additional setup for pygame . Troubleshooting If you are having troubles with your camera, check out our Discourse FAQ for hardware troubleshooting . Check this forum for more help. Keeping Things Up To Date Make all config changes to myconfig.py and they will be preserved through an update. If you are a long time user, you might be used to editing config.py. You should switch to editing myconfig.py instead. Later on, when changes occur that you would like to get, you can pull latest code, then issue a: cd projects/donkeycar git pull donkey createcar --path ~/mycar --overwrite Your ~/mycar/manage.py, ~/mycar/config.py and other files will change with this operation, but myconfig.py will not be touched. Your data and models dirs will not be touched. Note: If you are updating from Donkey<3.0 to 3.0+ it is very likely you will need to start over with a new virtual environment. We've had a few users hit this snag. Next calibrate your car .","title":"How to create and edit docs"},{"location":"dev_guide/docs/#how-to-create-and-edit-docs","text":"This guide will show you how to create and edit docs in markdown format, and submit them for integration into the Donkeycar official docs.","title":"How to create and edit docs"},{"location":"dev_guide/docs/#create-donkeycar-from-template","text":"Create a set of files to control your Donkey with this command: donkey createcar --path ~/mycar See also more information on createcar.","title":"Create Donkeycar from Template"},{"location":"dev_guide/docs/#configure-options","text":"Look at myconfig.py in your newly created directory, ~/mycar cd ~/mycar nano myconfig.py Each line has a comment mark. The commented text shows the default value. When you want to make an edit to over-write the default, uncomment the line by removing the # and any spaces before the first character of the option. example: # STEERING_LEFT_PWM = 460 becomes: STEERING_LEFT_PWM = 500 when edited. You will adjust these later in the calibrate section.","title":"Configure Options"},{"location":"dev_guide/docs/#configure-i2c-pca9685","text":"If you are using a PCA9685 card, make sure you can see it on I2C. Jetson Nano : sudo usermod -aG i2c $USER sudo reboot After a reboot, then try: sudo i2cdetect -r -y 1 Raspberry Pi : sudo apt-get install -y i2c-tools sudo i2cdetect -y 1 This should show you a grid of addresses like: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- In this case, the 40 shows up as the address of our PCA9685 board. If this does not show up, then check your wiring to the board. On a pi, ensure I2C is enabled in menu of sudo raspi-config (notice, it suggest reboot). If you have assigned a non-standard address to your board, then adjust the address in the myconfig.py under variable PCA9685_I2C_ADDR . If your board is on another bus, then you can specify that with the PCA9685_I2C_BUSNUM . Jetson Nano : set PCA9685_I2C_BUSNUM = 1 in your myconfig.py . For the pi, this will be auto detected by the Adafruit library. But not on the Jetson Nano.","title":"Configure I2C PCA9685"},{"location":"dev_guide/docs/#sombrero-setup","text":"Set HAVE_SOMBRERO = True in your myconfig.py if you have a sombrero board.","title":"Sombrero Setup"},{"location":"dev_guide/docs/#robo-hat-mm1-setup","text":"Set HAVE_ROBOHAT = True in your myconfig.py if you have a Robo HAT MM1 board. Also set the following variables according to your setup. Most people will be using the below values, however, if you are using a Jetson Nano, please set MM1_SERIAL_PORT = '/dev/ttyTHS1' #ROBOHAT MM1 HAVE_ROBOHAT = False # set to true when using the Robo HAT MM1 from Robotics Masters. This will change to RC Control. MM1_STEERING_MID = 1500 # Adjust this value if your car cannot run in a straight line MM1_MAX_FORWARD = 2000 # Max throttle to go fowrward. The bigger the faster MM1_STOPPED_PWM = 1500 MM1_MAX_REVERSE = 1000 # Max throttle to go reverse. The smaller the faster MM1_SHOW_STEERING_VALUE = False # Serial port # -- Default Pi: '/dev/ttyS0' # -- Jetson Nano: '/dev/ttyTHS1' # -- Google coral: '/dev/ttymxc0' # -- Windows: 'COM3', Arduino: '/dev/ttyACM0' # -- MacOS/Linux:please use 'ls /dev/tty.*' to find the correct serial port for mm1 # eg.'/dev/tty.usbmodemXXXXXX' and replace the port accordingly MM1_SERIAL_PORT = '/dev/ttyS0' # Serial Port for reading and sending MM1 data (raspberry pi default) # adjust controller type as Robohat MM1 CONTROLLER_TYPE='MM1' # adjust drive train for web interface DRIVE_TRAIN_TYPE = 'MM1' The Robo HAT MM1 uses a RC Controller and CircuitPython script to drive the car during training. You must put the CircuitPython script onto the Robo HAT MM1 with your computer before you can continue. Download the CircuitPython Donkey Car Driver for Robo HAT MM1 to your computer from here Connect the MicroUSB connector on the Robo HAT MM1 to your computer's USB port. A CIRCUITPY device should appear on the computer as a USB Storage Device Copy the file downloaded in Step 1 to the CIRCUITPY USB Storage Device. Rename the file code.py . Unplug USB Cable from the Robo HAT MM1 and place on top of the Raspberry Pi, as you would any HAT. You may need to enable the hardware serial port on your Raspberry Pi. On your Raspberry Pi... Run the command sudo raspi-config Navigate to the 5 - Interfaceing options section. Navigate to the P6 - Serial section. When asked: Would you like a login shell to be accessible over serial? NO When asked: Would you like the serial port hardware to be enabled? YES Close raspi-config Restart If you would like additional hardware or software support with Robo HAT MM1, there are a few guides published on Hackster.io. They are listed below. Raspberry Pi + Robo HAT MM1 Jetson Nano + Robo HAT MM1 Simulator + Robo HAT MM1","title":"Robo HAT MM1 Setup"},{"location":"dev_guide/docs/#joystick-setup","text":"If you plan to use a joystick, take a side track over to here .","title":"Joystick setup"},{"location":"dev_guide/docs/#camera-setup","text":"Raspberry Pi : If you are on a raspberry pi and using the recommended pi camera, then no changes are needed to your myconfg.py . Jetson Nano : When using a Sony IMX219 based camera, and you are using the default car template, then you will want edit your myconfg.py to have: CAMERA_TYPE = \"CSIC\" . For flipping the image vertically set CSIC_CAM_GSTREAMER_FLIP_PARM = 3 - this is helpful if you have to mount the camera in a rotated position. Set IMAGE_W = 224 and also IMAGE_H = 224 . CVCAM is a camera type that has worked for USB cameras when OpenCV is setup. This requires additional setup for OpenCV for Nano or OpenCV for Raspberry Pi . WEBCAM is a camera type that uses the pygame library, also typically for USB cameras. That requires additional setup for pygame .","title":"Camera Setup"},{"location":"dev_guide/docs/#troubleshooting","text":"If you are having troubles with your camera, check out our Discourse FAQ for hardware troubleshooting . Check this forum for more help.","title":"Troubleshooting"},{"location":"dev_guide/docs/#keeping-things-up-to-date","text":"Make all config changes to myconfig.py and they will be preserved through an update. If you are a long time user, you might be used to editing config.py. You should switch to editing myconfig.py instead. Later on, when changes occur that you would like to get, you can pull latest code, then issue a: cd projects/donkeycar git pull donkey createcar --path ~/mycar --overwrite Your ~/mycar/manage.py, ~/mycar/config.py and other files will change with this operation, but myconfig.py will not be touched. Your data and models dirs will not be touched. Note: If you are updating from Donkey<3.0 to 3.0+ it is very likely you will need to start over with a new virtual environment. We've had a few users hit this snag.","title":"Keeping Things Up To Date"},{"location":"dev_guide/docs/#next-calibrate-your-car","text":"","title":"Next calibrate your car."},{"location":"dev_guide/model/","text":"How to build your own model Note: This requires version >= 4.1.X Overview Constructor Training Interface Parts Interface Example Overview You might want to write your own model: If you find the models that ship with donkey not sufficient, and you want to experiment with your own model infrastructure If you want to add more input data to the model because your car has more sensors Constructor Models are located in donkeycar/parts/keras.py . Your own model needs to inherit from KerasPilot and initialize your model: class KerasSensors(KerasPilot): def __init__(self, input_shape=(120, 160, 3), num_sensors=2): super().__init__() self.num_sensors = num_sensors self.model = self.create_model(input_shape) Here, you implement the keras model in the member function create_model() . The model needs to have labelled input and output tensors. These are required for the training to work. Training interface What is required for your model to work, are the following functions: def compile(self): self.model.compile(optimizer=self.optimizer, metrics=['accuracy'], loss={'angle_out': 'categorical_crossentropy', 'throttle_out': 'categorical_crossentropy'}, loss_weights={'angle_out': 0.5, 'throttle_out': 0.5}) The compile function tells keras how to define the loss function for training. We are using the KerasCategorical model as an example. The loss function here makes explicit usage of the output tensors of the model ( angle_out, throttle_out ). def x_transform(self, record: TubRecord): img_arr = record.image(cached=True) return img_arr In this function you define how to extract the input data from your recorded data. This data is usually called X in the ML frame work . We have shown the implementation in the base class which works for all models that have only the image as input. The function returns a single data item if the model has only one input. You need to return a tuple if your model uses more input data. Note: If your model has more inputs, the tuple needs to have the image in the first place. def y_transform(self, record: TubRecord): angle: float = record.underlying['user/angle'] throttle: float = record.underlying['user/throttle'] return angle, throttle In this function you specify how to extract the y values (i.e. target values) from your recorded data. def x_translate(self, x: XY) -> Dict[str, Union[float, np.ndarray]]: return {'img_in': x} Here we require a translation of how the X value that you extracted above will be fed into tf.data . Note, tf.data expects a dictionary if the model has more than one input variable, so we have chosen to use dictionaries also in the one-argument case for consistency. Above we have shown the implementation in the base class which works for all models that have only the image as input. You don't have to overwrite neither x_transform nor x_translate if your model only uses the image as input data. Note: the keys of the dictionary must match the name of the input layers in the model. def y_translate(self, y: XY) -> Dict[str, Union[float, np.ndarray]]: if isinstance(y, tuple): angle, throttle = y return {'angle_out': angle, 'throttle_out': throttle} else: raise TypeError('Expected tuple') Similar to the above, this provides the translation of the y data into the dictionary required for tf.data . This example shows the implementation of KerasLinear . Note: the keys of the dictionary must match the name of the output layers in the model. def output_shapes(self): # need to cut off None from [None, 120, 160, 3] tensor shape img_shape = self.get_input_shape()[1:] shapes = ({'img_in': tf.TensorShape(img_shape)}, {'angle_out': tf.TensorShape([15]), 'throttle_out': tf.TensorShape([20])}) return shapes This function returns a tuple of two dictionaries that tells tensorflow which shapes are used in the model. We have shown the example of the KerasCategorical model here. Note 1: As above, the keys of the two dictionaries must match the name of the input and output layers in the model. Note 2: Where the model returns scalar numbers, the corresponding type has to be tf.TensorShape([]) . Parts interface In the car application the model is called through the run() function. That function is already provided in the base class where the normalisation of the input image is happening centrally. Instead, the derived classes have to implement inference() which works on the normalised data. If you have additional data that needs to be normalised, too, you might want to override run() as well. def inference(self, img_arr, other_arr): img_arr = img_arr.reshape((1,) + img_arr.shape) outputs = self.model.predict(img_arr) steering = outputs[0] throttle = outputs[1] return steering[0][0], throttle[0][0] Here we are showing the implementation of the linear model. Please note that the input tensor shape always contains the batch dimension in the first place, hence the shape of the input image is adjusted from (120, 160, 3) -> (1, 120, 160, 3) . Note: _If you are passing another array in the other_arr variable, you will have to do a similar re-shaping. Example Let's build a new donkey model which is based on the standard linear model but has following changes w.r.t. input data and network design: The model takes an additional vector of input data that represents a set of values from distance sensors which are attached to the front of the car. The model adds a couple of more feed-forward layers to combine the CNN layers of the vision system with the distance sensor data. Building the model using keras So here is the example model: class KerasSensors(KerasPilot): def __init__(self, input_shape=(120, 160, 3), num_sensors=2): super().__init__() self.num_sensors = num_sensors self.model = self.create_model(input_shape) def create_model(self, input_shape): drop = 0.2 img_in = Input(shape=input_shape, name='img_in') x = core_cnn_layers(img_in, drop) x = Dense(100, activation='relu', name='dense_1')(x) x = Dropout(drop)(x) x = Dense(50, activation='relu', name='dense_2')(x) x = Dropout(drop)(x) # up to here, this is the standard linear model, now we add the # sensor data to it sensor_in = Input(shape=(self.num_sensors, ), name='sensor_in') y = sensor_in z = concatenate([x, y]) # here we add two more dense layers z = Dense(50, activation='relu', name='dense_3')(z) z = Dropout(drop)(z) z = Dense(50, activation='relu', name='dense_4')(z) z = Dropout(drop)(z) # two outputs for angle and throttle outputs = [ Dense(1, activation='linear', name='n_outputs' + str(i))(z) for i in range(2)] # the model needs to specify the additional input here model = Model(inputs=[img_in, sensor_in], outputs=outputs) return model def compile(self): self.model.compile(optimizer=self.optimizer, loss='mse') def inference(self, img_arr, other_arr): img_arr = img_arr.reshape((1,) + img_arr.shape) sens_arr = other_arr.reshape((1,) + other_arr.shape) outputs = self.model.predict([img_arr, sens_arr]) steering = outputs[0] throttle = outputs[1] return steering[0][0], throttle[0][0] def x_transform(self, record: TubRecord) -> XY: img_arr = super().x_transform(record) # for simplicity we assume the sensor data here is normalised sensor_arr = np.array(record.underlying['sensor']) # we need to return the image data first return img_arr, sensor_arr def x_translate(self, x: XY) -> Dict[str, Union[float, np.ndarray]]: assert isinstance(x, tuple), 'Requires tuple as input' # the keys are the names of the input layers of the model return {'img_in': x[0], 'sensor_in': x[1]} def y_transform(self, record: TubRecord): angle: float = record.underlying['user/angle'] throttle: float = record.underlying['user/throttle'] return angle, throttle def y_translate(self, y: XY) -> Dict[str, Union[float, np.ndarray]]: if isinstance(y, tuple): angle, throttle = y # the keys are the names of the output layers of the model return {'n_outputs0': angle, 'n_outputs1': throttle} else: raise TypeError('Expected tuple') def output_shapes(self): # need to cut off None from [None, 120, 160, 3] tensor shape img_shape = self.get_input_shape()[1:] # the keys need to match the models input/output layers shapes = ({'img_in': tf.TensorShape(img_shape), 'sensor_in': tf.TensorShape([self.num_sensors])}, {'n_outputs0': tf.TensorShape([]), 'n_outputs1': tf.TensorShape([])}) return shapes We could have inherited from KerasLinear which already provides the implementation of y_transform(), y_translate(), compile() . However, to make it explicit for the general case we have implemented all functions here. The model requires the sensor data to be an array in the TubRecord with key \"sensor\" . Creating a tub Because we don't have a tub with sensor data, let's create one with fake sensor entries: import os import tarfile import numpy as np from donkeycar.parts.tub_v2 import Tub from donkeycar.pipeline.types import TubRecord from donkeycar.config import load_config if __name__ == '__main__': # put your path to your car app my_car = os.path.expanduser('~/mycar') cfg = load_config(os.path.join(my_car, 'config.py')) # put your path to donkey project tar = tarfile.open(os.path.expanduser( '~/Python/donkeycar/donkeycar/tests/tub/tub.tar.gz')) tub_parent = os.path.join(my_car, 'data2/') tar.extractall(tub_parent) tub_path = os.path.join(tub_parent, 'tub') tub1 = Tub(tub_path) tub2 = Tub(os.path.join(my_car, 'data2/tub_sensor'), inputs=['cam/image_array', 'user/angle', 'user/throttle', 'sensor'], types=['image_array', 'float', 'float', 'list']) for record in tub1: t_record = TubRecord(config=cfg, base_path=tub1.base_path, underlying=record) img_arr = t_record.image(cached=False) record['sensor'] = list(np.random.uniform(size=2)) record['cam/image_array'] = img_arr tub2.write_record(record) Making the model available We don't have a dynamic factory yet, so we need to add the new model into the function get_model_by_type() in the module donkeycar/utils.py : ... elif model_type == 'sensor': kl = KerasSensors(input_shape=input_shape) ... Go train In your car app folder now the following should work: donkey train --tub data2/tub_sensor --model models/pilot.h5 --type sensor Because of the random values in the data the model will not converge quickly, the goal here is to get it working in the framework. Support and discussions Please join the Discord Donkey Car group for support and discussions.","title":"Create your own model"},{"location":"dev_guide/model/#how-to-build-your-own-model","text":"Note: This requires version >= 4.1.X Overview Constructor Training Interface Parts Interface Example","title":"How to build your own model"},{"location":"dev_guide/model/#overview","text":"You might want to write your own model: If you find the models that ship with donkey not sufficient, and you want to experiment with your own model infrastructure If you want to add more input data to the model because your car has more sensors","title":"Overview"},{"location":"dev_guide/model/#constructor","text":"Models are located in donkeycar/parts/keras.py . Your own model needs to inherit from KerasPilot and initialize your model: class KerasSensors(KerasPilot): def __init__(self, input_shape=(120, 160, 3), num_sensors=2): super().__init__() self.num_sensors = num_sensors self.model = self.create_model(input_shape) Here, you implement the keras model in the member function create_model() . The model needs to have labelled input and output tensors. These are required for the training to work.","title":"Constructor"},{"location":"dev_guide/model/#training-interface","text":"What is required for your model to work, are the following functions: def compile(self): self.model.compile(optimizer=self.optimizer, metrics=['accuracy'], loss={'angle_out': 'categorical_crossentropy', 'throttle_out': 'categorical_crossentropy'}, loss_weights={'angle_out': 0.5, 'throttle_out': 0.5}) The compile function tells keras how to define the loss function for training. We are using the KerasCategorical model as an example. The loss function here makes explicit usage of the output tensors of the model ( angle_out, throttle_out ). def x_transform(self, record: TubRecord): img_arr = record.image(cached=True) return img_arr In this function you define how to extract the input data from your recorded data. This data is usually called X in the ML frame work . We have shown the implementation in the base class which works for all models that have only the image as input. The function returns a single data item if the model has only one input. You need to return a tuple if your model uses more input data. Note: If your model has more inputs, the tuple needs to have the image in the first place. def y_transform(self, record: TubRecord): angle: float = record.underlying['user/angle'] throttle: float = record.underlying['user/throttle'] return angle, throttle In this function you specify how to extract the y values (i.e. target values) from your recorded data. def x_translate(self, x: XY) -> Dict[str, Union[float, np.ndarray]]: return {'img_in': x} Here we require a translation of how the X value that you extracted above will be fed into tf.data . Note, tf.data expects a dictionary if the model has more than one input variable, so we have chosen to use dictionaries also in the one-argument case for consistency. Above we have shown the implementation in the base class which works for all models that have only the image as input. You don't have to overwrite neither x_transform nor x_translate if your model only uses the image as input data. Note: the keys of the dictionary must match the name of the input layers in the model. def y_translate(self, y: XY) -> Dict[str, Union[float, np.ndarray]]: if isinstance(y, tuple): angle, throttle = y return {'angle_out': angle, 'throttle_out': throttle} else: raise TypeError('Expected tuple') Similar to the above, this provides the translation of the y data into the dictionary required for tf.data . This example shows the implementation of KerasLinear . Note: the keys of the dictionary must match the name of the output layers in the model. def output_shapes(self): # need to cut off None from [None, 120, 160, 3] tensor shape img_shape = self.get_input_shape()[1:] shapes = ({'img_in': tf.TensorShape(img_shape)}, {'angle_out': tf.TensorShape([15]), 'throttle_out': tf.TensorShape([20])}) return shapes This function returns a tuple of two dictionaries that tells tensorflow which shapes are used in the model. We have shown the example of the KerasCategorical model here. Note 1: As above, the keys of the two dictionaries must match the name of the input and output layers in the model. Note 2: Where the model returns scalar numbers, the corresponding type has to be tf.TensorShape([]) .","title":"Training interface"},{"location":"dev_guide/model/#parts-interface","text":"In the car application the model is called through the run() function. That function is already provided in the base class where the normalisation of the input image is happening centrally. Instead, the derived classes have to implement inference() which works on the normalised data. If you have additional data that needs to be normalised, too, you might want to override run() as well. def inference(self, img_arr, other_arr): img_arr = img_arr.reshape((1,) + img_arr.shape) outputs = self.model.predict(img_arr) steering = outputs[0] throttle = outputs[1] return steering[0][0], throttle[0][0] Here we are showing the implementation of the linear model. Please note that the input tensor shape always contains the batch dimension in the first place, hence the shape of the input image is adjusted from (120, 160, 3) -> (1, 120, 160, 3) . Note: _If you are passing another array in the other_arr variable, you will have to do a similar re-shaping.","title":"Parts interface"},{"location":"dev_guide/model/#example","text":"Let's build a new donkey model which is based on the standard linear model but has following changes w.r.t. input data and network design: The model takes an additional vector of input data that represents a set of values from distance sensors which are attached to the front of the car. The model adds a couple of more feed-forward layers to combine the CNN layers of the vision system with the distance sensor data.","title":"Example"},{"location":"dev_guide/model/#building-the-model-using-keras","text":"So here is the example model: class KerasSensors(KerasPilot): def __init__(self, input_shape=(120, 160, 3), num_sensors=2): super().__init__() self.num_sensors = num_sensors self.model = self.create_model(input_shape) def create_model(self, input_shape): drop = 0.2 img_in = Input(shape=input_shape, name='img_in') x = core_cnn_layers(img_in, drop) x = Dense(100, activation='relu', name='dense_1')(x) x = Dropout(drop)(x) x = Dense(50, activation='relu', name='dense_2')(x) x = Dropout(drop)(x) # up to here, this is the standard linear model, now we add the # sensor data to it sensor_in = Input(shape=(self.num_sensors, ), name='sensor_in') y = sensor_in z = concatenate([x, y]) # here we add two more dense layers z = Dense(50, activation='relu', name='dense_3')(z) z = Dropout(drop)(z) z = Dense(50, activation='relu', name='dense_4')(z) z = Dropout(drop)(z) # two outputs for angle and throttle outputs = [ Dense(1, activation='linear', name='n_outputs' + str(i))(z) for i in range(2)] # the model needs to specify the additional input here model = Model(inputs=[img_in, sensor_in], outputs=outputs) return model def compile(self): self.model.compile(optimizer=self.optimizer, loss='mse') def inference(self, img_arr, other_arr): img_arr = img_arr.reshape((1,) + img_arr.shape) sens_arr = other_arr.reshape((1,) + other_arr.shape) outputs = self.model.predict([img_arr, sens_arr]) steering = outputs[0] throttle = outputs[1] return steering[0][0], throttle[0][0] def x_transform(self, record: TubRecord) -> XY: img_arr = super().x_transform(record) # for simplicity we assume the sensor data here is normalised sensor_arr = np.array(record.underlying['sensor']) # we need to return the image data first return img_arr, sensor_arr def x_translate(self, x: XY) -> Dict[str, Union[float, np.ndarray]]: assert isinstance(x, tuple), 'Requires tuple as input' # the keys are the names of the input layers of the model return {'img_in': x[0], 'sensor_in': x[1]} def y_transform(self, record: TubRecord): angle: float = record.underlying['user/angle'] throttle: float = record.underlying['user/throttle'] return angle, throttle def y_translate(self, y: XY) -> Dict[str, Union[float, np.ndarray]]: if isinstance(y, tuple): angle, throttle = y # the keys are the names of the output layers of the model return {'n_outputs0': angle, 'n_outputs1': throttle} else: raise TypeError('Expected tuple') def output_shapes(self): # need to cut off None from [None, 120, 160, 3] tensor shape img_shape = self.get_input_shape()[1:] # the keys need to match the models input/output layers shapes = ({'img_in': tf.TensorShape(img_shape), 'sensor_in': tf.TensorShape([self.num_sensors])}, {'n_outputs0': tf.TensorShape([]), 'n_outputs1': tf.TensorShape([])}) return shapes We could have inherited from KerasLinear which already provides the implementation of y_transform(), y_translate(), compile() . However, to make it explicit for the general case we have implemented all functions here. The model requires the sensor data to be an array in the TubRecord with key \"sensor\" .","title":"Building the model using keras"},{"location":"dev_guide/model/#creating-a-tub","text":"Because we don't have a tub with sensor data, let's create one with fake sensor entries: import os import tarfile import numpy as np from donkeycar.parts.tub_v2 import Tub from donkeycar.pipeline.types import TubRecord from donkeycar.config import load_config if __name__ == '__main__': # put your path to your car app my_car = os.path.expanduser('~/mycar') cfg = load_config(os.path.join(my_car, 'config.py')) # put your path to donkey project tar = tarfile.open(os.path.expanduser( '~/Python/donkeycar/donkeycar/tests/tub/tub.tar.gz')) tub_parent = os.path.join(my_car, 'data2/') tar.extractall(tub_parent) tub_path = os.path.join(tub_parent, 'tub') tub1 = Tub(tub_path) tub2 = Tub(os.path.join(my_car, 'data2/tub_sensor'), inputs=['cam/image_array', 'user/angle', 'user/throttle', 'sensor'], types=['image_array', 'float', 'float', 'list']) for record in tub1: t_record = TubRecord(config=cfg, base_path=tub1.base_path, underlying=record) img_arr = t_record.image(cached=False) record['sensor'] = list(np.random.uniform(size=2)) record['cam/image_array'] = img_arr tub2.write_record(record)","title":"Creating a tub"},{"location":"dev_guide/model/#making-the-model-available","text":"We don't have a dynamic factory yet, so we need to add the new model into the function get_model_by_type() in the module donkeycar/utils.py : ... elif model_type == 'sensor': kl = KerasSensors(input_shape=input_shape) ...","title":"Making the model available"},{"location":"dev_guide/model/#go-train","text":"In your car app folder now the following should work: donkey train --tub data2/tub_sensor --model models/pilot.h5 --type sensor Because of the random values in the data the model will not converge quickly, the goal here is to get it working in the framework.","title":"Go train"},{"location":"dev_guide/model/#support-and-discussions","text":"Please join the Discord Donkey Car group for support and discussions.","title":"Support and discussions"},{"location":"guide/build_hardware/","text":"How to Build a Donkey\u00ae Overview Parts Needed Hardware: Step 1: Print Parts Step 2: Clean up parts Step 3: Assemble Top plate and Roll Cage Step 4: Connect Servo Shield to Raspberry Pi Step 5: Attach Raspberry Pi to 3D Printed bottom plate Step 6: Attach Camera Step 7: Put it all together Software Overview The latest version of the software installation instructions are maintained in the software instructions section. Be sure to follow those instructions after you've built your car. Choosing a Car There are 4 fully supported chassis all made under the \"Exceed\" Brand: Exceed Magnet Blue Exceed Desert Monster Green Exceed Short Course Truck Green , Red Exceed Blaze Blue , Yellow , Wild Blue , Max Red Note: If they are out of stock on Amazon, you can find the cars at the Exceed Website These cars are electrically identical but have different tires, mounting and other details. It is worth noting that the Desert Monster, Short Course Truck and Blaze all require adapters which can be easily printed or purchased from the donkey store. These are the standard build cars because they are mostly plug and play, both have a brushed motor which makes training easier, they handle rough driving surfaces well and are inexpensive. Here is a video overview of the different cars and how to assemble them. In addition there are 3 more cars supported under the \"Donkey Pro\" name. These are 1/10 scale cars which means that they are bigger, perform a little better and are slightly more expensive. They can be found here: HobbyKing Trooper (not pro version) found here HobbyKing Mission-D found here Tamaya TT01 or Clone commonly used knockoff found here - found worldwide but usually has to be built as a kits. The other two cars are ready to be donkified, this one, however is harder to assemble. Here is a video that goes over the different models. The Donkey Pro models are not yet very well documented, just a word of warning. For more detail and other options, follow the link to: supported cars Roll Your Own Car Alternatively If you know RC or need something the standard Donkey does not support, you can roll your own. Here is a quick reference to help you along the way. Roll Your Own Video Overview of Hardware Assembly This video covers how to assemble a standard Donkey Car, it also covers the Sombrero, the Raspberry Pi and the nVidia Jetson Nano. Parts Needed The following instructions are for the Raspberry Pi, below in Optional Upgrades section, you can find the NVIDIA Jetson Nano instructions. Option 1: Buying through an official Donkey Store There are two official stores: If you are in the US, you can use the Donkey store . The intention of the Donkey Store is to make it easier and less expensive to build the Donkey Car. The Donkey Store is run by the original founders of donkey car and profits are used to fund development of the donkey cars. Also it is worth noting the design of the parts out of the Donkey store is slightly improved over the standard build as it uses better parts that are only available in large quantities or are harder to get. The Donkey Store builds are open source like all others. If you are in Asia, the DIYRobocars community in Hong Kong also sells car kits at Robocar Store . They are long term Donkey community members and use proceeds to support the R&D efforts of this project. It is worth noting they can also sell to Europe and the US but it is likely less cost effective. Part Description Link Approximate Cost Exceed Magnet, Desert Monster, Blaze, or Short Course Truck See links above ~$90 USB Battery with microUSB cable (any battery capable of 2A 5V output is sufficient) Anker 6700 mAh or Anker Powercore II $22 Raspberry Pi 3b+ Pi 3b+ $42 MicroSD Card (many will work, we strongly recommend this one) 64GB https://amzn.to/2XP7UAa $11.99 Donkey Partial Kit KIT $82 to $125 Option 2: Bottoms Up Build If you want to buy the parts yourself, want to customize your donkey or live outside of the US, you may want to choose the bottoms up build. Keep in mind you will have to print the donkey car parts which can be found here Part Description Link Approximate Cost Magnet Car or alternative see cars above under 'choosing a car' $92 M2x6 screws (8) Amazon or Donkey Store $4.89 * M3x10 screws (3) Amazon or Donkey Store $7.89 * USB Battery with microUSB cable (any battery capable of 2A 5V output is sufficient) Anker 6700 mAh $22 Raspberry Pi 3b+ Pi 3B+ $38 MicroSD Card (many will work, I like this one because it boots quickly) 64GB $18.99 Wide Angle Raspberry Pi Camera Amazon or Donkey Store $25 Female to Female Jumper Wire Amazon or Donkey Car Store $7 * Servo Driver PCA 9685 Amazon or Donkey Car Store $12 ** 3D Printed roll cage and top plate. Purchase: Donkey Store Files: thingiverse.com/thing:2260575 $50 * If it is hard to find these components, there is some wiggle room. Instead of an M2 you can use an M2.2, m2.3 or #4 SAE screw. Instead of an M3 a #6 SAE screw can be used. Machine screws can be used in a pinch. ** This component can be purchased from Ali Express for ~$2-4 if you can wait the 30-60 days for shipping. Optional Upgrades NVIDIA JetsonNano Hardware Options The NVIDIA Jetson Nano is fully supported by the donkey Car. To assemble the Donkey Car you will need a few parts including the Wifi card, Antennas and camera. In addition you will need this Adapter . If you want to print it yourself, it is on the Thingiverse page for the project. Due to the higher power usage and consumption you should consider the 10Ahr 3A USB battery pack listed below and a good cable rated for 3A. Plug in the Servo driver the same as the Raspberry Pi, just keep in mind that the Jetson pinout is reversed and that the Sombrero is not supported. Finally this is the Donkey Assembled. Part Description Link Approximate Cost Nvidia Jetson Nano Amazon $99 Jetson Nano Adapter Donkey Store $7 Camera Module Donkey Store $27 WiFi Card Amazon $18 Antennas Donkey Store $7 For other options for part, feel free to look at the jetbot documentation here . Sombrero Hat NOTE: the Sombrero is out of stock at any stores - we are looking at other options or will place another order. The sombrero hat replaces the Servo driver and the USB battery and can be purchased at the Donkeycar store here and video instructions can be found here . Implementing the Sombrero hat requires a LiPo battery (see below). Documentation is in Github . LiPo Battery and Accessories: LiPo batteries have significantly better energy density and have a better dropoff curve. See below (courtesy of Traxxas). Part Description Link Approximate Cost LiPo Battery hobbyking.com/en_us/turnigy-1800mah-2s-20c-lipo-pack.html or amazon.com/gp/product/B0072AERBE/ $8.94 to $~17 Lipo Charger (takes 1hr to charge the above battery) charger $13 Lipo Battery Case (to prevent damage if they explode) lipo safe $8 Hardware If you purchased parts from the Donkey Car Store, skip to step 3. Step 1: Print Parts If you do not have a 3D Printer, you can order parts from Donkey Store , Shapeways or 3dHubs . I printed parts in black PLA, with 2mm layer height and no supports. The top roll bar is designed to be printed upside down. Remember that you need to print the adapters unless you have a \"Magnet\" I printed parts in black PLA, with .3mm layer height with a .5mm nozzle and no supports. The top roll bar is designed to be printed upside down. Step 2: Clean up parts Almost all 3D Printed parts will need clean up. Re-drill holes, and clean up excess plastic. In particular, clean up the slots in the side of the roll bar, as shown in the picture below: Step 3: Assemble Top plate and Roll Cage If you have an Exceed Short Course Truck, Blaze or Desert Monster watch this video This is a relatively simple assembly step. Just use the 3mm self tapping screws to scew the plate to the roll cage. When attaching the roll cage to the top plate, ensure that the nubs on the top plate face the roll-cage. This will ensure the equipment you mount to the top plate fits easily. Step 4: Connect Servo Shield to Raspberry Pi note: this is not necessary if you have a Sombrero, the Sombrero just plugs into the Pi You could do this after attaching the Raspberry Pi to the bottom plate, I just think it is easier to see the parts when they are laying on the workbench. Connect the parts as you see below: For reference, below is the Raspberry Pi Pinout for reference. You will notice we connect to 3.3v, the two I2C pins (SDA and SCL) and ground: Step 5: Attach Raspberry Pi to 3D Printed bottom plate Before you start, now is a good time to insert the already flashed SD card and bench test the electronics. Once that is done, attaching the Raspberry Pi and Servo is as simple as running screws through the board into the screw bosses on the top plate. The M2.5x12mm screws should be the perfect length to go through the board, the plastic and still have room for a washer. The \u201ccap\u201d part of the screw should be facing up and the nut should be on the bottom of the top plate. The ethernet and USB ports should face forward. This is important as it gives you access to the SD card and makes the camera ribbon cable line up properly. Attach the USB battery to the underside of the printed bottom plate using cable ties or velcro. Step 6: Attach Camera Slip the camera into the slot, cable end first. However, be careful not to push on the camera lens and instead press the board. If you need to remove the camera the temptation is to push on the lens, instead push on the connector as is shown in these pictures. Before using the car, remove the plastic film or lens cover from the camera lens. It is easy to put the camera cable in the wrong way so look at these photos and make sure the cable is put in properly. There are loads of tutorials on youtube if you are not used to this. Step 7: Put it all together Note if you have a Desert Monster Chassis see 7B section below The final steps are straightforward. First attach the roll bar assembly to the car. This is done using the same pins that came with the vehicle. Second run the servo cables up to the car. The throttle cable runs to channel 0 on the servo controller and steering is channel 1. Now you are done with the hardware!! Step 7b: Attach Adapters (Desert Monster only) The Desert monster does not have the same set up for holding the body on the car and needs two adapters mentioned above. To attach the adapters you must first remove the existing adapter from the chassis and screw on the custom adapter with the same screws as is shown in this photo: Once this is done, go back to step 7 Software Congrats! Now to get your get your car moving, see the software instructions section. We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.","title":"Build a car."},{"location":"guide/build_hardware/#how-to-build-a-donkey","text":"Overview Parts Needed Hardware: Step 1: Print Parts Step 2: Clean up parts Step 3: Assemble Top plate and Roll Cage Step 4: Connect Servo Shield to Raspberry Pi Step 5: Attach Raspberry Pi to 3D Printed bottom plate Step 6: Attach Camera Step 7: Put it all together Software","title":"How to Build a Donkey&reg;"},{"location":"guide/build_hardware/#overview","text":"The latest version of the software installation instructions are maintained in the software instructions section. Be sure to follow those instructions after you've built your car.","title":"Overview"},{"location":"guide/build_hardware/#choosing-a-car","text":"There are 4 fully supported chassis all made under the \"Exceed\" Brand: Exceed Magnet Blue Exceed Desert Monster Green Exceed Short Course Truck Green , Red Exceed Blaze Blue , Yellow , Wild Blue , Max Red Note: If they are out of stock on Amazon, you can find the cars at the Exceed Website These cars are electrically identical but have different tires, mounting and other details. It is worth noting that the Desert Monster, Short Course Truck and Blaze all require adapters which can be easily printed or purchased from the donkey store. These are the standard build cars because they are mostly plug and play, both have a brushed motor which makes training easier, they handle rough driving surfaces well and are inexpensive. Here is a video overview of the different cars and how to assemble them. In addition there are 3 more cars supported under the \"Donkey Pro\" name. These are 1/10 scale cars which means that they are bigger, perform a little better and are slightly more expensive. They can be found here: HobbyKing Trooper (not pro version) found here HobbyKing Mission-D found here Tamaya TT01 or Clone commonly used knockoff found here - found worldwide but usually has to be built as a kits. The other two cars are ready to be donkified, this one, however is harder to assemble. Here is a video that goes over the different models. The Donkey Pro models are not yet very well documented, just a word of warning. For more detail and other options, follow the link to: supported cars","title":"Choosing a Car"},{"location":"guide/build_hardware/#roll-your-own-car","text":"Alternatively If you know RC or need something the standard Donkey does not support, you can roll your own. Here is a quick reference to help you along the way. Roll Your Own","title":"Roll Your Own Car"},{"location":"guide/build_hardware/#video-overview-of-hardware-assembly","text":"This video covers how to assemble a standard Donkey Car, it also covers the Sombrero, the Raspberry Pi and the nVidia Jetson Nano.","title":"Video Overview of Hardware Assembly"},{"location":"guide/build_hardware/#parts-needed","text":"The following instructions are for the Raspberry Pi, below in Optional Upgrades section, you can find the NVIDIA Jetson Nano instructions.","title":"Parts Needed"},{"location":"guide/build_hardware/#option-1-buying-through-an-official-donkey-store","text":"There are two official stores: If you are in the US, you can use the Donkey store . The intention of the Donkey Store is to make it easier and less expensive to build the Donkey Car. The Donkey Store is run by the original founders of donkey car and profits are used to fund development of the donkey cars. Also it is worth noting the design of the parts out of the Donkey store is slightly improved over the standard build as it uses better parts that are only available in large quantities or are harder to get. The Donkey Store builds are open source like all others. If you are in Asia, the DIYRobocars community in Hong Kong also sells car kits at Robocar Store . They are long term Donkey community members and use proceeds to support the R&D efforts of this project. It is worth noting they can also sell to Europe and the US but it is likely less cost effective. Part Description Link Approximate Cost Exceed Magnet, Desert Monster, Blaze, or Short Course Truck See links above ~$90 USB Battery with microUSB cable (any battery capable of 2A 5V output is sufficient) Anker 6700 mAh or Anker Powercore II $22 Raspberry Pi 3b+ Pi 3b+ $42 MicroSD Card (many will work, we strongly recommend this one) 64GB https://amzn.to/2XP7UAa $11.99 Donkey Partial Kit KIT $82 to $125","title":"Option 1: Buying through an official Donkey Store"},{"location":"guide/build_hardware/#option-2-bottoms-up-build","text":"If you want to buy the parts yourself, want to customize your donkey or live outside of the US, you may want to choose the bottoms up build. Keep in mind you will have to print the donkey car parts which can be found here Part Description Link Approximate Cost Magnet Car or alternative see cars above under 'choosing a car' $92 M2x6 screws (8) Amazon or Donkey Store $4.89 * M3x10 screws (3) Amazon or Donkey Store $7.89 * USB Battery with microUSB cable (any battery capable of 2A 5V output is sufficient) Anker 6700 mAh $22 Raspberry Pi 3b+ Pi 3B+ $38 MicroSD Card (many will work, I like this one because it boots quickly) 64GB $18.99 Wide Angle Raspberry Pi Camera Amazon or Donkey Store $25 Female to Female Jumper Wire Amazon or Donkey Car Store $7 * Servo Driver PCA 9685 Amazon or Donkey Car Store $12 ** 3D Printed roll cage and top plate. Purchase: Donkey Store Files: thingiverse.com/thing:2260575 $50 * If it is hard to find these components, there is some wiggle room. Instead of an M2 you can use an M2.2, m2.3 or #4 SAE screw. Instead of an M3 a #6 SAE screw can be used. Machine screws can be used in a pinch. ** This component can be purchased from Ali Express for ~$2-4 if you can wait the 30-60 days for shipping.","title":"Option 2: Bottoms Up Build"},{"location":"guide/build_hardware/#optional-upgrades","text":"NVIDIA JetsonNano Hardware Options The NVIDIA Jetson Nano is fully supported by the donkey Car. To assemble the Donkey Car you will need a few parts including the Wifi card, Antennas and camera. In addition you will need this Adapter . If you want to print it yourself, it is on the Thingiverse page for the project. Due to the higher power usage and consumption you should consider the 10Ahr 3A USB battery pack listed below and a good cable rated for 3A. Plug in the Servo driver the same as the Raspberry Pi, just keep in mind that the Jetson pinout is reversed and that the Sombrero is not supported. Finally this is the Donkey Assembled. Part Description Link Approximate Cost Nvidia Jetson Nano Amazon $99 Jetson Nano Adapter Donkey Store $7 Camera Module Donkey Store $27 WiFi Card Amazon $18 Antennas Donkey Store $7 For other options for part, feel free to look at the jetbot documentation here . Sombrero Hat NOTE: the Sombrero is out of stock at any stores - we are looking at other options or will place another order. The sombrero hat replaces the Servo driver and the USB battery and can be purchased at the Donkeycar store here and video instructions can be found here . Implementing the Sombrero hat requires a LiPo battery (see below). Documentation is in Github . LiPo Battery and Accessories: LiPo batteries have significantly better energy density and have a better dropoff curve. See below (courtesy of Traxxas). Part Description Link Approximate Cost LiPo Battery hobbyking.com/en_us/turnigy-1800mah-2s-20c-lipo-pack.html or amazon.com/gp/product/B0072AERBE/ $8.94 to $~17 Lipo Charger (takes 1hr to charge the above battery) charger $13 Lipo Battery Case (to prevent damage if they explode) lipo safe $8","title":"Optional Upgrades"},{"location":"guide/build_hardware/#hardware","text":"If you purchased parts from the Donkey Car Store, skip to step 3.","title":"Hardware"},{"location":"guide/build_hardware/#step-1-print-parts","text":"If you do not have a 3D Printer, you can order parts from Donkey Store , Shapeways or 3dHubs . I printed parts in black PLA, with 2mm layer height and no supports. The top roll bar is designed to be printed upside down. Remember that you need to print the adapters unless you have a \"Magnet\" I printed parts in black PLA, with .3mm layer height with a .5mm nozzle and no supports. The top roll bar is designed to be printed upside down.","title":"Step 1: Print Parts"},{"location":"guide/build_hardware/#step-2-clean-up-parts","text":"Almost all 3D Printed parts will need clean up. Re-drill holes, and clean up excess plastic. In particular, clean up the slots in the side of the roll bar, as shown in the picture below:","title":"Step 2: Clean up parts"},{"location":"guide/build_hardware/#step-3-assemble-top-plate-and-roll-cage","text":"If you have an Exceed Short Course Truck, Blaze or Desert Monster watch this video This is a relatively simple assembly step. Just use the 3mm self tapping screws to scew the plate to the roll cage. When attaching the roll cage to the top plate, ensure that the nubs on the top plate face the roll-cage. This will ensure the equipment you mount to the top plate fits easily.","title":"Step 3: Assemble Top plate and Roll Cage"},{"location":"guide/build_hardware/#step-4-connect-servo-shield-to-raspberry-pi","text":"note: this is not necessary if you have a Sombrero, the Sombrero just plugs into the Pi You could do this after attaching the Raspberry Pi to the bottom plate, I just think it is easier to see the parts when they are laying on the workbench. Connect the parts as you see below: For reference, below is the Raspberry Pi Pinout for reference. You will notice we connect to 3.3v, the two I2C pins (SDA and SCL) and ground:","title":"Step 4: Connect Servo Shield to Raspberry Pi"},{"location":"guide/build_hardware/#step-5-attach-raspberry-pi-to-3d-printed-bottom-plate","text":"Before you start, now is a good time to insert the already flashed SD card and bench test the electronics. Once that is done, attaching the Raspberry Pi and Servo is as simple as running screws through the board into the screw bosses on the top plate. The M2.5x12mm screws should be the perfect length to go through the board, the plastic and still have room for a washer. The \u201ccap\u201d part of the screw should be facing up and the nut should be on the bottom of the top plate. The ethernet and USB ports should face forward. This is important as it gives you access to the SD card and makes the camera ribbon cable line up properly. Attach the USB battery to the underside of the printed bottom plate using cable ties or velcro.","title":"Step 5: Attach Raspberry Pi to 3D Printed bottom plate"},{"location":"guide/build_hardware/#step-6-attach-camera","text":"Slip the camera into the slot, cable end first. However, be careful not to push on the camera lens and instead press the board. If you need to remove the camera the temptation is to push on the lens, instead push on the connector as is shown in these pictures. Before using the car, remove the plastic film or lens cover from the camera lens. It is easy to put the camera cable in the wrong way so look at these photos and make sure the cable is put in properly. There are loads of tutorials on youtube if you are not used to this.","title":"Step 6: Attach Camera"},{"location":"guide/build_hardware/#step-7-put-it-all-together","text":"Note if you have a Desert Monster Chassis see 7B section below The final steps are straightforward. First attach the roll bar assembly to the car. This is done using the same pins that came with the vehicle. Second run the servo cables up to the car. The throttle cable runs to channel 0 on the servo controller and steering is channel 1. Now you are done with the hardware!!","title":"Step 7: Put it all together"},{"location":"guide/build_hardware/#step-7b-attach-adapters-desert-monster-only","text":"The Desert monster does not have the same set up for holding the body on the car and needs two adapters mentioned above. To attach the adapters you must first remove the existing adapter from the chassis and screw on the custom adapter with the same screws as is shown in this photo: Once this is done, go back to step 7","title":"Step 7b: Attach Adapters (Desert Monster only)"},{"location":"guide/build_hardware/#software","text":"Congrats! Now to get your get your car moving, see the software instructions section. We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.","title":"Software"},{"location":"guide/calibrate/","text":"Calibrate your Car The point of calibrating your car is to make it drive consistently. How to adjust your car's settings You will need to ssh into your Pi to do the calibration. All of the car's settings are in the config.py and myconfig.py scripts generated when you ran the donkey createcar --path ~/mycar command. You can edit this file on your car by running: nano ~/mycar/myconfig.py Steering Calibration Make sure your car is off the ground to prevent a runaway situation. Turn on your car. Find the servo cable on your car and see what channel it's plugged into the PCA board. It should be 1 or 0. Run donkey calibrate --channel <your_steering_channel> --bus=1 Enter 360 and you should see the wheels on your car move slightly. If not enter 400 or 300 . Next enter values +/- 10 from your starting value to find the PWM setting that makes your car turn all the way left and all the way right. Remember these values. Enter these values in myconfig.py script as STEERING_RIGHT_PWM and STEERING_LEFT_PWM . Throttle Calibration Find the cable coming from your ESC and see what channel it goes into the PCA board. This is your throttle channel. run donkey calibrate --channel <your_throttle_channel> --bus=1 Enter 370 when prompted for a PWM value. You should hear your ESC beep indicating that it's calibrated. Enter 400 and you should see your cars wheels start to go forward. If not, its likely that this is reverse, try entering 330 instead. Keep trying different values until you've found a reasonable max speed and remember this PWM value. Reverse on RC cars is a little tricky because the ESC must receive a reverse pulse, zero pulse, reverse pulse to start to go backwards. To calibrate a reverse PWM setting... Use the same technique as above set the PWM setting to your zero throttle. Enter the reverse value, then the zero throttle value, then the reverse value again. Enter values +/- 10 of the reverse value to find a reasonable reverse speed. Remember this reverse PWM value. Now open your myconfig.py script and enter the PWM values for your car into the throttle_controller part: THROTTLE_FORWARD_PWM = PWM value for full throttle forward THROTTLE_STOPPED_PWM = PWM value for zero throttle THROTTLE_REVERSE_PWM = PWM value at full reverse throttle Fine tuning your calibration Note : optional Now that you have your car roughly calibrated you can try driving it to verify that it drives as expected. Here's how to fine tune your car's calibration. Start your car by running python manage.py drive . Go to <your_cars_hostname.local>:8887 in a browser. Press j until the cars steering is all the way right. Press i a couple times to get the car to go forward. Measure the diameter of the turn and record it on a spreadsheet. Repeat this measurement for different steering values for turning each direction. Chart these so you can see if your car turns the same in each direction. Corrections: If your car turns the same amount at an 80% turn and a 100% turn, change the PWM setting for that turn direction to be the PWM value at 80%. If your car is biased to turn one direction, change the PWM values of your turns in the opposite direction of the bias. After you've fine tuned your car the steering chart should look something like this. Next let's get driving!","title":"Calibrate steering and throttle."},{"location":"guide/calibrate/#calibrate-your-car","text":"The point of calibrating your car is to make it drive consistently.","title":"Calibrate your Car"},{"location":"guide/calibrate/#how-to-adjust-your-cars-settings","text":"You will need to ssh into your Pi to do the calibration. All of the car's settings are in the config.py and myconfig.py scripts generated when you ran the donkey createcar --path ~/mycar command. You can edit this file on your car by running: nano ~/mycar/myconfig.py","title":"How to adjust your car's settings"},{"location":"guide/calibrate/#steering-calibration","text":"Make sure your car is off the ground to prevent a runaway situation. Turn on your car. Find the servo cable on your car and see what channel it's plugged into the PCA board. It should be 1 or 0. Run donkey calibrate --channel <your_steering_channel> --bus=1 Enter 360 and you should see the wheels on your car move slightly. If not enter 400 or 300 . Next enter values +/- 10 from your starting value to find the PWM setting that makes your car turn all the way left and all the way right. Remember these values. Enter these values in myconfig.py script as STEERING_RIGHT_PWM and STEERING_LEFT_PWM .","title":"Steering Calibration"},{"location":"guide/calibrate/#throttle-calibration","text":"Find the cable coming from your ESC and see what channel it goes into the PCA board. This is your throttle channel. run donkey calibrate --channel <your_throttle_channel> --bus=1 Enter 370 when prompted for a PWM value. You should hear your ESC beep indicating that it's calibrated. Enter 400 and you should see your cars wheels start to go forward. If not, its likely that this is reverse, try entering 330 instead. Keep trying different values until you've found a reasonable max speed and remember this PWM value. Reverse on RC cars is a little tricky because the ESC must receive a reverse pulse, zero pulse, reverse pulse to start to go backwards. To calibrate a reverse PWM setting... Use the same technique as above set the PWM setting to your zero throttle. Enter the reverse value, then the zero throttle value, then the reverse value again. Enter values +/- 10 of the reverse value to find a reasonable reverse speed. Remember this reverse PWM value. Now open your myconfig.py script and enter the PWM values for your car into the throttle_controller part: THROTTLE_FORWARD_PWM = PWM value for full throttle forward THROTTLE_STOPPED_PWM = PWM value for zero throttle THROTTLE_REVERSE_PWM = PWM value at full reverse throttle","title":"Throttle Calibration"},{"location":"guide/calibrate/#fine-tuning-your-calibration","text":"Note : optional Now that you have your car roughly calibrated you can try driving it to verify that it drives as expected. Here's how to fine tune your car's calibration. Start your car by running python manage.py drive . Go to <your_cars_hostname.local>:8887 in a browser. Press j until the cars steering is all the way right. Press i a couple times to get the car to go forward. Measure the diameter of the turn and record it on a spreadsheet. Repeat this measurement for different steering values for turning each direction. Chart these so you can see if your car turns the same in each direction. Corrections: If your car turns the same amount at an 80% turn and a 100% turn, change the PWM setting for that turn direction to be the PWM value at 80%. If your car is biased to turn one direction, change the PWM values of your turns in the opposite direction of the bias. After you've fine tuned your car the steering chart should look something like this.","title":"Fine tuning your calibration"},{"location":"guide/calibrate/#next-lets-get-driving","text":"","title":"Next let's get driving!"},{"location":"guide/create_application/","text":"Create your car application If you are not already, please ssh into your vehicle . Create Donkeycar from Template Create a set of files to control your Donkey with this command: donkey createcar --path ~/mycar See also more information on createcar. Configure Options Look at myconfig.py in your newly created directory, ~/mycar cd ~/mycar nano myconfig.py Each line has a comment mark. The commented text shows the default value. When you want to make an edit to over-write the default, uncomment the line by removing the # and any spaces before the first character of the option. example: # STEERING_LEFT_PWM = 460 becomes: STEERING_LEFT_PWM = 500 when edited. You will adjust these later in the calibrate section. Configure I2C PCA9685 If you are using a PCA9685 card, make sure you can see it on I2C. Jetson Nano : sudo usermod -aG i2c $USER sudo reboot After a reboot, then try: sudo i2cdetect -r -y 1 Raspberry Pi : sudo apt-get install -y i2c-tools sudo i2cdetect -y 1 This should show you a grid of addresses like: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- In this case, the 40 shows up as the address of our PCA9685 board. If this does not show up, then check your wiring to the board. On a pi, ensure I2C is enabled in menu of sudo raspi-config (notice, it suggest reboot). If you have assigned a non-standard address to your board, then adjust the address in the myconfig.py under variable PCA9685_I2C_ADDR . If your board is on another bus, then you can specify that with the PCA9685_I2C_BUSNUM . Jetson Nano : set PCA9685_I2C_BUSNUM = 1 in your myconfig.py . For the pi, this will be auto detected by the Adafruit library. But not on the Jetson Nano. Sombrero Setup Set HAVE_SOMBRERO = True in your myconfig.py if you have a sombrero board. Robo HAT MM1 Setup Set HAVE_ROBOHAT = True in your myconfig.py if you have a Robo HAT MM1 board. Also set the following variables according to your setup. Most people will be using the below values, however, if you are using a Jetson Nano, please set MM1_SERIAL_PORT = '/dev/ttyTHS1' #ROBOHAT MM1 HAVE_ROBOHAT = False # set to true when using the Robo HAT MM1 from Robotics Masters. This will change to RC Control. MM1_STEERING_MID = 1500 # Adjust this value if your car cannot run in a straight line MM1_MAX_FORWARD = 2000 # Max throttle to go fowrward. The bigger the faster MM1_STOPPED_PWM = 1500 MM1_MAX_REVERSE = 1000 # Max throttle to go reverse. The smaller the faster MM1_SHOW_STEERING_VALUE = False # Serial port # -- Default Pi: '/dev/ttyS0' # -- Jetson Nano: '/dev/ttyTHS1' # -- Google coral: '/dev/ttymxc0' # -- Windows: 'COM3', Arduino: '/dev/ttyACM0' # -- MacOS/Linux:please use 'ls /dev/tty.*' to find the correct serial port for mm1 # eg.'/dev/tty.usbmodemXXXXXX' and replace the port accordingly MM1_SERIAL_PORT = '/dev/ttyS0' # Serial Port for reading and sending MM1 data (raspberry pi default) # adjust controller type as Robohat MM1 CONTROLLER_TYPE='MM1' # adjust drive train for web interface DRIVE_TRAIN_TYPE = 'MM1' The Robo HAT MM1 uses a RC Controller and CircuitPython script to drive the car during training. You must put the CircuitPython script onto the Robo HAT MM1 with your computer before you can continue. Download the CircuitPython Donkey Car Driver for Robo HAT MM1 to your computer from here Connect the MicroUSB connector on the Robo HAT MM1 to your computer's USB port. A CIRCUITPY device should appear on the computer as a USB Storage Device Copy the file downloaded in Step 1 to the CIRCUITPY USB Storage Device. Rename the file code.py . Unplug USB Cable from the Robo HAT MM1 and place on top of the Raspberry Pi, as you would any HAT. You may need to enable the hardware serial port on your Raspberry Pi. On your Raspberry Pi... Run the command sudo raspi-config Navigate to the 5 - Interfaceing options section. Navigate to the P6 - Serial section. When asked: Would you like a login shell to be accessible over serial? NO When asked: Would you like the serial port hardware to be enabled? YES Close raspi-config Restart If you would like additional hardware or software support with Robo HAT MM1, there are a few guides published on Hackster.io. They are listed below. Raspberry Pi + Robo HAT MM1 Jetson Nano + Robo HAT MM1 Simulator + Robo HAT MM1 Joystick setup If you plan to use a joystick, take a side track over to here . Camera Setup Raspberry Pi : If you are on a raspberry pi and using the recommended pi camera, then no changes are needed to your myconfg.py . Jetson Nano : When using a Sony IMX219 based camera, and you are using the default car template, then you will want edit your myconfg.py to have: CAMERA_TYPE = \"CSIC\" . For flipping the image vertically set CSIC_CAM_GSTREAMER_FLIP_PARM = 3 - this is helpful if you have to mount the camera in a rotated position. Set IMAGE_W = 224 and also IMAGE_H = 224 . CVCAM is a camera type that has worked for USB cameras when OpenCV is setup. This requires additional setup for OpenCV for Nano or OpenCV for Raspberry Pi . WEBCAM is a camera type that uses the pygame library, also typically for USB cameras. That requires additional setup for pygame . Troubleshooting If you are having troubles with your camera, check out our Discourse FAQ for hardware troubleshooting . Check this forum for more help. Keeping Things Up To Date Make all config changes to myconfig.py and they will be preserved through an update. If you are a long time user, you might be used to editing config.py. You should switch to editing myconfig.py instead. Later on, when changes occur that you would like to get, you can pull latest code, then issue a: cd projects/donkeycar git pull donkey createcar --path ~/mycar --overwrite Your ~/mycar/manage.py, ~/mycar/config.py and other files will change with this operation, but myconfig.py will not be touched. Your data and models dirs will not be touched. Note: If you are updating from Donkey<3.0 to 3.0+ it is very likely you will need to start over with a new virtual environment. We've had a few users hit this snag. Next calibrate your car .","title":"Create Donkeycar App."},{"location":"guide/create_application/#create-your-car-application","text":"If you are not already, please ssh into your vehicle .","title":"Create your car application"},{"location":"guide/create_application/#create-donkeycar-from-template","text":"Create a set of files to control your Donkey with this command: donkey createcar --path ~/mycar See also more information on createcar.","title":"Create Donkeycar from Template"},{"location":"guide/create_application/#configure-options","text":"Look at myconfig.py in your newly created directory, ~/mycar cd ~/mycar nano myconfig.py Each line has a comment mark. The commented text shows the default value. When you want to make an edit to over-write the default, uncomment the line by removing the # and any spaces before the first character of the option. example: # STEERING_LEFT_PWM = 460 becomes: STEERING_LEFT_PWM = 500 when edited. You will adjust these later in the calibrate section.","title":"Configure Options"},{"location":"guide/create_application/#configure-i2c-pca9685","text":"If you are using a PCA9685 card, make sure you can see it on I2C. Jetson Nano : sudo usermod -aG i2c $USER sudo reboot After a reboot, then try: sudo i2cdetect -r -y 1 Raspberry Pi : sudo apt-get install -y i2c-tools sudo i2cdetect -y 1 This should show you a grid of addresses like: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- In this case, the 40 shows up as the address of our PCA9685 board. If this does not show up, then check your wiring to the board. On a pi, ensure I2C is enabled in menu of sudo raspi-config (notice, it suggest reboot). If you have assigned a non-standard address to your board, then adjust the address in the myconfig.py under variable PCA9685_I2C_ADDR . If your board is on another bus, then you can specify that with the PCA9685_I2C_BUSNUM . Jetson Nano : set PCA9685_I2C_BUSNUM = 1 in your myconfig.py . For the pi, this will be auto detected by the Adafruit library. But not on the Jetson Nano.","title":"Configure I2C PCA9685"},{"location":"guide/create_application/#sombrero-setup","text":"Set HAVE_SOMBRERO = True in your myconfig.py if you have a sombrero board.","title":"Sombrero Setup"},{"location":"guide/create_application/#robo-hat-mm1-setup","text":"Set HAVE_ROBOHAT = True in your myconfig.py if you have a Robo HAT MM1 board. Also set the following variables according to your setup. Most people will be using the below values, however, if you are using a Jetson Nano, please set MM1_SERIAL_PORT = '/dev/ttyTHS1' #ROBOHAT MM1 HAVE_ROBOHAT = False # set to true when using the Robo HAT MM1 from Robotics Masters. This will change to RC Control. MM1_STEERING_MID = 1500 # Adjust this value if your car cannot run in a straight line MM1_MAX_FORWARD = 2000 # Max throttle to go fowrward. The bigger the faster MM1_STOPPED_PWM = 1500 MM1_MAX_REVERSE = 1000 # Max throttle to go reverse. The smaller the faster MM1_SHOW_STEERING_VALUE = False # Serial port # -- Default Pi: '/dev/ttyS0' # -- Jetson Nano: '/dev/ttyTHS1' # -- Google coral: '/dev/ttymxc0' # -- Windows: 'COM3', Arduino: '/dev/ttyACM0' # -- MacOS/Linux:please use 'ls /dev/tty.*' to find the correct serial port for mm1 # eg.'/dev/tty.usbmodemXXXXXX' and replace the port accordingly MM1_SERIAL_PORT = '/dev/ttyS0' # Serial Port for reading and sending MM1 data (raspberry pi default) # adjust controller type as Robohat MM1 CONTROLLER_TYPE='MM1' # adjust drive train for web interface DRIVE_TRAIN_TYPE = 'MM1' The Robo HAT MM1 uses a RC Controller and CircuitPython script to drive the car during training. You must put the CircuitPython script onto the Robo HAT MM1 with your computer before you can continue. Download the CircuitPython Donkey Car Driver for Robo HAT MM1 to your computer from here Connect the MicroUSB connector on the Robo HAT MM1 to your computer's USB port. A CIRCUITPY device should appear on the computer as a USB Storage Device Copy the file downloaded in Step 1 to the CIRCUITPY USB Storage Device. Rename the file code.py . Unplug USB Cable from the Robo HAT MM1 and place on top of the Raspberry Pi, as you would any HAT. You may need to enable the hardware serial port on your Raspberry Pi. On your Raspberry Pi... Run the command sudo raspi-config Navigate to the 5 - Interfaceing options section. Navigate to the P6 - Serial section. When asked: Would you like a login shell to be accessible over serial? NO When asked: Would you like the serial port hardware to be enabled? YES Close raspi-config Restart If you would like additional hardware or software support with Robo HAT MM1, there are a few guides published on Hackster.io. They are listed below. Raspberry Pi + Robo HAT MM1 Jetson Nano + Robo HAT MM1 Simulator + Robo HAT MM1","title":"Robo HAT MM1 Setup"},{"location":"guide/create_application/#joystick-setup","text":"If you plan to use a joystick, take a side track over to here .","title":"Joystick setup"},{"location":"guide/create_application/#camera-setup","text":"Raspberry Pi : If you are on a raspberry pi and using the recommended pi camera, then no changes are needed to your myconfg.py . Jetson Nano : When using a Sony IMX219 based camera, and you are using the default car template, then you will want edit your myconfg.py to have: CAMERA_TYPE = \"CSIC\" . For flipping the image vertically set CSIC_CAM_GSTREAMER_FLIP_PARM = 3 - this is helpful if you have to mount the camera in a rotated position. Set IMAGE_W = 224 and also IMAGE_H = 224 . CVCAM is a camera type that has worked for USB cameras when OpenCV is setup. This requires additional setup for OpenCV for Nano or OpenCV for Raspberry Pi . WEBCAM is a camera type that uses the pygame library, also typically for USB cameras. That requires additional setup for pygame .","title":"Camera Setup"},{"location":"guide/create_application/#troubleshooting","text":"If you are having troubles with your camera, check out our Discourse FAQ for hardware troubleshooting . Check this forum for more help.","title":"Troubleshooting"},{"location":"guide/create_application/#keeping-things-up-to-date","text":"Make all config changes to myconfig.py and they will be preserved through an update. If you are a long time user, you might be used to editing config.py. You should switch to editing myconfig.py instead. Later on, when changes occur that you would like to get, you can pull latest code, then issue a: cd projects/donkeycar git pull donkey createcar --path ~/mycar --overwrite Your ~/mycar/manage.py, ~/mycar/config.py and other files will change with this operation, but myconfig.py will not be touched. Your data and models dirs will not be touched. Note: If you are updating from Donkey<3.0 to 3.0+ it is very likely you will need to start over with a new virtual environment. We've had a few users hit this snag.","title":"Keeping Things Up To Date"},{"location":"guide/create_application/#next-calibrate-your-car","text":"","title":"Next calibrate your car."},{"location":"guide/dataset_pretrained_models/","text":"Dataset and pre-trained models The purpose of providing a sample dataset and pre-trained models here is to help beginners to get started easier. You can download the dataset and use it to train together with your own dataset. Dataset Dataset in real world Sample dataset #1 by Tawn - 2.0GB , Link Dataset in simulator Not available yet Contributing dataset If you have a dataset you want to contribute, please contact us on Discord #dataset channel. Thank you. Pre-trained models Not available yet","title":"Dataset and pre-trained models"},{"location":"guide/dataset_pretrained_models/#dataset-and-pre-trained-models","text":"The purpose of providing a sample dataset and pre-trained models here is to help beginners to get started easier. You can download the dataset and use it to train together with your own dataset.","title":"Dataset and pre-trained models"},{"location":"guide/dataset_pretrained_models/#dataset","text":"","title":"Dataset"},{"location":"guide/dataset_pretrained_models/#dataset-in-real-world","text":"Sample dataset #1 by Tawn - 2.0GB , Link","title":"Dataset in real world"},{"location":"guide/dataset_pretrained_models/#dataset-in-simulator","text":"Not available yet","title":"Dataset in simulator"},{"location":"guide/dataset_pretrained_models/#contributing-dataset","text":"If you have a dataset you want to contribute, please contact us on Discord #dataset channel. Thank you.","title":"Contributing dataset"},{"location":"guide/dataset_pretrained_models/#pre-trained-models","text":"Not available yet","title":"Pre-trained models"},{"location":"guide/get_driving/","text":"Drive your car After you've calibrated your car you can start driving it. If you are not already, please ssh into your vehicle . Start your car Put your car in a safe place where the wheels are off the ground . This is the step were the car can take off. Open your car's folder and start your car. cd ~/mycar python manage.py drive This script will start the drive loop in your car which includes a part that is a web server for you to control your car. You can now control your car from a web browser at the URL: <your car's hostname.local>:8887 Driving with Web Controller On your phone you can now press start to set your phones current tilt to be zero throttle and steering. Now tilting your phone forward will increase throttle and tilting it side to side will turn the steering. Features Recording - Press record data to start recording images, steering angels and throttle values. Throttle mode - Option to set the throttle as constant. This is used in races if you have a pilot that will steer but doesn't control throttle. Pilot mode - Choose this if the pilot should control the angle and/or throttle. Max throttle - Select the maximum throttle. Keyboard shortcuts space : stop car and stop recording r : toggle recording i : increase throttle k : decrease throttle j : turn left l : turn right If you don't have a joystick then you can skip to next section - train an autopilot . Driving with Physical Joystick Controller You may find that it helps to use a physical joystick device to control your vehicle. Setup Bluetooth and pair joystick Check the Controllers section to read about setting up the bluetooth connection. Start car cd ~/mycar python manage.py drive --js Optionally, if you want joystick use to be sticky and don't want to add the --js each time, modify your myconfig.py so that USE_JOYSTICK_AS_DEFAULT = True nano myconfig.py Joystick Controls Left analog stick - Left and right to adjust steering Right analog stick - Forward to increase forward throttle Pull back twice on right analog to reverse Whenever the throttle is not zero, driving data will be recorded - as long as you are in User mode! Select button switches modes - \"User, Local Angle, Local(angle and throttle)\" Triangle - Increase max throttle X - Decrease max throttle Circle - Toggle recording (disabled by default. auto record on throttle is enabled by default) dpad up - Increase throttle scale dpad down - Decrease throttle scale dpad left - Increase steering scale dpad right - Decrease steering scale Start - Toggle constant throttle. Sets to max throttle (modified by X and Triangle). Next let's train an autopilot .","title":"Get driving."},{"location":"guide/get_driving/#drive-your-car","text":"After you've calibrated your car you can start driving it. If you are not already, please ssh into your vehicle .","title":"Drive your car"},{"location":"guide/get_driving/#start-your-car","text":"Put your car in a safe place where the wheels are off the ground . This is the step were the car can take off. Open your car's folder and start your car. cd ~/mycar python manage.py drive This script will start the drive loop in your car which includes a part that is a web server for you to control your car. You can now control your car from a web browser at the URL: <your car's hostname.local>:8887","title":"Start your car"},{"location":"guide/get_driving/#driving-with-web-controller","text":"On your phone you can now press start to set your phones current tilt to be zero throttle and steering. Now tilting your phone forward will increase throttle and tilting it side to side will turn the steering.","title":"Driving with Web Controller"},{"location":"guide/get_driving/#features","text":"Recording - Press record data to start recording images, steering angels and throttle values. Throttle mode - Option to set the throttle as constant. This is used in races if you have a pilot that will steer but doesn't control throttle. Pilot mode - Choose this if the pilot should control the angle and/or throttle. Max throttle - Select the maximum throttle.","title":"Features"},{"location":"guide/get_driving/#keyboard-shortcuts","text":"space : stop car and stop recording r : toggle recording i : increase throttle k : decrease throttle j : turn left l : turn right If you don't have a joystick then you can skip to next section - train an autopilot .","title":"Keyboard shortcuts"},{"location":"guide/get_driving/#driving-with-physical-joystick-controller","text":"You may find that it helps to use a physical joystick device to control your vehicle.","title":"Driving with Physical Joystick Controller"},{"location":"guide/get_driving/#setup-bluetooth-and-pair-joystick","text":"Check the Controllers section to read about setting up the bluetooth connection.","title":"Setup Bluetooth and pair joystick"},{"location":"guide/get_driving/#start-car","text":"cd ~/mycar python manage.py drive --js Optionally, if you want joystick use to be sticky and don't want to add the --js each time, modify your myconfig.py so that USE_JOYSTICK_AS_DEFAULT = True nano myconfig.py","title":"Start car"},{"location":"guide/get_driving/#joystick-controls","text":"Left analog stick - Left and right to adjust steering Right analog stick - Forward to increase forward throttle Pull back twice on right analog to reverse Whenever the throttle is not zero, driving data will be recorded - as long as you are in User mode! Select button switches modes - \"User, Local Angle, Local(angle and throttle)\" Triangle - Increase max throttle X - Decrease max throttle Circle - Toggle recording (disabled by default. auto record on throttle is enabled by default) dpad up - Increase throttle scale dpad down - Decrease throttle scale dpad left - Increase steering scale dpad right - Decrease steering scale Start - Toggle constant throttle. Sets to max throttle (modified by X and Triangle).","title":"Joystick Controls"},{"location":"guide/get_driving/#next-lets-train-an-autopilot","text":"","title":"Next let's train an autopilot."},{"location":"guide/install_software/","text":"Install Software Overview Software: Step 1: Install Software on Host PC Step 2: Install Software on Donkeycar Create Donkeycar Application Overview Donkeycar has components to install on a host PC. This can be a laptop, or desktop machine. The machine doesn't have to be powerful, but it will benefit from faster cpu, more ram, and an NVidia GPU. An SSD hard drive will greatly impact your training times. Donkeycar software components need to be installed on the robot platform of your choice. Raspberry Pi and Jetson Nano have setup docs. But it has been known to work on Jetson TX2, Friendly Arm SBC, or almost any Debian based SBC ( single board computer ). After install, you will create the Donkeycar application from a template. This contains code that is designed for you to customize for your particular case. Don't worry, we will get you started with some useful defaults. Next we will train the Donkeycar to drive on it's own based on your driving style! This uses a supervised learning technique often referred to as behavioral cloning. This is not the only method for getting your Donkeycar to drive itself. But it requires the least amount of hardware and least technical knowledge. Then you can explore other techniques in this Ai mobile laboratory called Donkeycar! Step 1: Install Software on Host PC When controlling your Donkey via behavioral cloning, you will need to setup a host pc to train your machine learning model from the data collected on the robot. Choose a setup that matches your computer OS. Setup Linux Host PC Setup Windows Host PC Setup Mac Host PC Step 2: Install Software On Donkeycar This guide will help you to setup the software to run Donkeycar on your Raspberry Pi or Jetson Nano. Choose a setup that matches your SBC type. (SBC = single board computer) Setup RaspberryPi Setup Jetson Nano [Optional] Use TensorRT on the Jetson Nano Read this for more information. Next: Create Your Donkeycar Application .","title":"Install the software."},{"location":"guide/install_software/#install-software","text":"Overview Software: Step 1: Install Software on Host PC Step 2: Install Software on Donkeycar Create Donkeycar Application","title":"Install Software"},{"location":"guide/install_software/#overview","text":"Donkeycar has components to install on a host PC. This can be a laptop, or desktop machine. The machine doesn't have to be powerful, but it will benefit from faster cpu, more ram, and an NVidia GPU. An SSD hard drive will greatly impact your training times. Donkeycar software components need to be installed on the robot platform of your choice. Raspberry Pi and Jetson Nano have setup docs. But it has been known to work on Jetson TX2, Friendly Arm SBC, or almost any Debian based SBC ( single board computer ). After install, you will create the Donkeycar application from a template. This contains code that is designed for you to customize for your particular case. Don't worry, we will get you started with some useful defaults. Next we will train the Donkeycar to drive on it's own based on your driving style! This uses a supervised learning technique often referred to as behavioral cloning. This is not the only method for getting your Donkeycar to drive itself. But it requires the least amount of hardware and least technical knowledge. Then you can explore other techniques in this Ai mobile laboratory called Donkeycar!","title":"Overview"},{"location":"guide/install_software/#step-1-install-software-on-host-pc","text":"When controlling your Donkey via behavioral cloning, you will need to setup a host pc to train your machine learning model from the data collected on the robot. Choose a setup that matches your computer OS. Setup Linux Host PC Setup Windows Host PC Setup Mac Host PC","title":"Step 1: Install Software on Host PC"},{"location":"guide/install_software/#step-2-install-software-on-donkeycar","text":"This guide will help you to setup the software to run Donkeycar on your Raspberry Pi or Jetson Nano. Choose a setup that matches your SBC type. (SBC = single board computer) Setup RaspberryPi Setup Jetson Nano","title":"Step 2: Install Software On Donkeycar"},{"location":"guide/install_software/#optional-use-tensorrt-on-the-jetson-nano","text":"Read this for more information.","title":"[Optional] Use TensorRT on the Jetson Nano"},{"location":"guide/install_software/#next-create-your-donkeycar-application","text":"","title":"Next: Create Your Donkeycar Application."},{"location":"guide/mobile_app/","text":"Robocar Controller Robocar Controller is a mobile app designed to provide a \u201ccommandless\u201d user experience to get started with the Donkey Car. Features Commandless experience - No SSH or text editor Built-in Hotspot Search vehicle on the network Real-time Calibration Virtual joystick Visualize the data Drive Summary Free GPU training Autopilot Advanced configuration Battery level Requirements A Donkey Car with Pi 4B (Jetson Nano is not yet supported) A Mobile phone with iOS or Android Quickstart Guide Please refer to the quick start guide here . Features Details Built-in Hotspot The car will become a hotspot when there is no known Wifi network to connect. After connecting your phone to this hotspot, you can use the app to configure the car to join the Wifi network you want. Search vehicle on the network Once your car connects to the same network as your phone, the app will scan the whole network to discover it. The app will also show you the IP address of the car in case you want to connect to it via SSH. Real-time Calibration Sometimes it is quite annoying if the car goes too fast or does not run in a straight line. The calibration UI assists you to find the right settings so that your car could be properly calibrated. With the enhanced calibration function, the change will take place in real time and you could observe the change immediately. Virtual Joystick The virtual joystick offers a quick way to test drive the car if you don't have a physical gamepad controller. It also streams the video captured from the camera in real time. You can just look at the screen and start driving. Drive Summary The app presents a drive summary with histogram, the size and the number of images you have collected. The histogram is generated automatically by calling the tubhist function in the Donkey car software. Visualize the data The app shows all the data(tubs) and the metadata you have collected on the Pi. The metadata includes number of images, size of the tub, the resolutions, the histogram and the location. The app will make use of the donkey makemovie command to generate a video so you can review how the data look like. Free GPU Training Free GPU training is available to user who use the app. You can train a model by selecting the data(tubs) you wish to train. The data will be uploaded to our server to start the training process. Once the training is completed, the app will show you the training loss and accuracy graph. At the same time, the app will download the model to your car and you can test the model right away. Note: We keep the data and models for a period of time. After that, we will delete it from our storage. More on Free GPU Training We are using AWS g4dn.xlarge instance to train the model. It feautres NVIDIA T4 GPU and up to 16GB GPU memory. Increase the batch size to 256 or more to fully utilize the powerful GPU. Limitation N.B.: To protect our equipment from being abused, we have the following rules to use the training service. Each training is limited to a maximum of 15 minutes. The training job will timeout if it last more than 15 minutes Each device could train 5 times per 24 hours. Max data size is 100MB per training Autopilot The app will list all models inside the Pi, no matter it is generated from the training function or just a model copied to the Pi. You can start the autopilot mode using a similar UI as the Drive UI. Advanced configuration The Doneky car software comes with a vast of configuration that you can experiment. We have included some of the popular options that you may want to change. Camera size Training configuration Drive train settings Battery level If you are using MM1, the app shows you the current battery level in percentage. We have also added an OS tweak that if battery level fall below 7V, the system will shutdown automatically. Upcoming features Salient visualization Auto throttle compensation based on battery level Transfer learning Report a problem If you encountered a problem, please file an issue on this github project . FAQ Why the app is called Robocar Controller instead of Donkeycar Controller? We would love to call the app Donkeycar Controller but Apple does not allow us to do so. We are working with Adam to submit a proof to Apple that we can use the Donkeycar trademark in our app. In the meanwhile, we will be using the name Robocar Controller. Commercial Usage This app is developed by Robocar Store . If you plan to use this app to make money, please follow the Donkey Car guideline and send an email to Robocar Store .","title":"Mobile app"},{"location":"guide/mobile_app/#robocar-controller","text":"Robocar Controller is a mobile app designed to provide a \u201ccommandless\u201d user experience to get started with the Donkey Car.","title":"Robocar Controller"},{"location":"guide/mobile_app/#features","text":"Commandless experience - No SSH or text editor Built-in Hotspot Search vehicle on the network Real-time Calibration Virtual joystick Visualize the data Drive Summary Free GPU training Autopilot Advanced configuration Battery level","title":"Features"},{"location":"guide/mobile_app/#requirements","text":"A Donkey Car with Pi 4B (Jetson Nano is not yet supported) A Mobile phone with iOS or Android","title":"Requirements"},{"location":"guide/mobile_app/#quickstart-guide","text":"Please refer to the quick start guide here .","title":"Quickstart Guide"},{"location":"guide/mobile_app/#features-details","text":"","title":"Features Details"},{"location":"guide/mobile_app/#built-in-hotspot","text":"The car will become a hotspot when there is no known Wifi network to connect. After connecting your phone to this hotspot, you can use the app to configure the car to join the Wifi network you want.","title":"Built-in Hotspot"},{"location":"guide/mobile_app/#search-vehicle-on-the-network","text":"Once your car connects to the same network as your phone, the app will scan the whole network to discover it. The app will also show you the IP address of the car in case you want to connect to it via SSH.","title":"Search vehicle on the network"},{"location":"guide/mobile_app/#real-time-calibration","text":"Sometimes it is quite annoying if the car goes too fast or does not run in a straight line. The calibration UI assists you to find the right settings so that your car could be properly calibrated. With the enhanced calibration function, the change will take place in real time and you could observe the change immediately.","title":"Real-time Calibration"},{"location":"guide/mobile_app/#virtual-joystick","text":"The virtual joystick offers a quick way to test drive the car if you don't have a physical gamepad controller. It also streams the video captured from the camera in real time. You can just look at the screen and start driving.","title":"Virtual Joystick"},{"location":"guide/mobile_app/#drive-summary","text":"The app presents a drive summary with histogram, the size and the number of images you have collected. The histogram is generated automatically by calling the tubhist function in the Donkey car software.","title":"Drive Summary"},{"location":"guide/mobile_app/#visualize-the-data","text":"The app shows all the data(tubs) and the metadata you have collected on the Pi. The metadata includes number of images, size of the tub, the resolutions, the histogram and the location. The app will make use of the donkey makemovie command to generate a video so you can review how the data look like.","title":"Visualize the data"},{"location":"guide/mobile_app/#free-gpu-training","text":"Free GPU training is available to user who use the app. You can train a model by selecting the data(tubs) you wish to train. The data will be uploaded to our server to start the training process. Once the training is completed, the app will show you the training loss and accuracy graph. At the same time, the app will download the model to your car and you can test the model right away. Note: We keep the data and models for a period of time. After that, we will delete it from our storage.","title":"Free GPU Training"},{"location":"guide/mobile_app/#more-on-free-gpu-training","text":"We are using AWS g4dn.xlarge instance to train the model. It feautres NVIDIA T4 GPU and up to 16GB GPU memory. Increase the batch size to 256 or more to fully utilize the powerful GPU.","title":"More on Free GPU Training"},{"location":"guide/mobile_app/#limitation","text":"N.B.: To protect our equipment from being abused, we have the following rules to use the training service. Each training is limited to a maximum of 15 minutes. The training job will timeout if it last more than 15 minutes Each device could train 5 times per 24 hours. Max data size is 100MB per training","title":"Limitation"},{"location":"guide/mobile_app/#autopilot","text":"The app will list all models inside the Pi, no matter it is generated from the training function or just a model copied to the Pi. You can start the autopilot mode using a similar UI as the Drive UI.","title":"Autopilot"},{"location":"guide/mobile_app/#advanced-configuration","text":"The Doneky car software comes with a vast of configuration that you can experiment. We have included some of the popular options that you may want to change. Camera size Training configuration Drive train settings","title":"Advanced configuration"},{"location":"guide/mobile_app/#battery-level","text":"If you are using MM1, the app shows you the current battery level in percentage. We have also added an OS tweak that if battery level fall below 7V, the system will shutdown automatically.","title":"Battery level"},{"location":"guide/mobile_app/#upcoming-features","text":"Salient visualization Auto throttle compensation based on battery level Transfer learning","title":"Upcoming features"},{"location":"guide/mobile_app/#report-a-problem","text":"If you encountered a problem, please file an issue on this github project .","title":"Report a problem"},{"location":"guide/mobile_app/#faq","text":"Why the app is called Robocar Controller instead of Donkeycar Controller? We would love to call the app Donkeycar Controller but Apple does not allow us to do so. We are working with Adam to submit a proof to Apple that we can use the Donkeycar trademark in our app. In the meanwhile, we will be using the name Robocar Controller.","title":"FAQ"},{"location":"guide/mobile_app/#commercial-usage","text":"This app is developed by Robocar Store . If you plan to use this app to make money, please follow the Donkey Car guideline and send an email to Robocar Store .","title":"Commercial Usage"},{"location":"guide/simulator/","text":"Donkey Simulator The Donkey Gym project is a OpenAI gym wrapper around the Self Driving Sandbox donkey simulator ( sdsandbox ). When building the sim from source, checkout the donkey branch of the sdsandbox project. The simulator is built on the the Unity game platform, uses their internal physics and graphics, and connects to a donkey Python process to use our trained model to control the simulated Donkey. Installation Video: Here's some videos to help you through the installation. Linux: https://youtu.be/J6Ll5Obtuxk Windows: https://youtu.be/wqQMmHVT8qw My Virtual Donkey There are many ways to use the simulator, depending on your goals. You can use the simulator to get to know and use the standard Donkeycar drive/train/test cycle by treating it as virtual hardware. You will collect data, drive, and train using the same commands as if you were using a real robot. We will walk through that use-case first. Install Download and unzip the simulator for your host pc platform from Donkey Gym Release . Place the simulator where you like. For this example it will be ~/projects/DonkeySimLinux. Your dir will have a different name depending on platform. Complete all the steps to install Donkey on your host pc . Setup DonkeyGym: cd ~/projects git clone https://github.com/tawnkramer/gym-donkeycar cd gym-donkeycar conda activate donkey pip install -e .[gym-donkeycar] You may use an existing ~/mycar donkey application, or begin a new one. Here we will start fresh: donkey createcar --path ~/mysim cd ~/mysim Edit your myconfig.py to enable donkey gym simulator wrapper, replace <user-name> and the other parts of the path: DONKEY_GYM = True DONKEY_SIM_PATH = \"/home/<user-name>/projects/DonkeySimLinux/donkey_sim.x86_64\" DONKEY_GYM_ENV_NAME = \"donkey-generated-track-v0\" Note: your path to the executable will vary depending on platform and user. Windows: DonkeySimWin/donkey_sim.exe Mac OS: DonkeySimMac/donkey_sim.app/Contents/MacOS/donkey_sim Linux: DonkeySimLinux/donkey_sim.x86_64 Drive You may use all the normal commands to manage.py at this point. Such as: python manage.py drive This should start the simulator and connect to it automatically. By default you will have a web interface to control the donkey. Navigate to http://localhost:8887/drive to see control page. On Ubuntu Linux only, you may plug in your joystick of choice. If it mounts as /dev/input/js0 then there's a good chance it will work. Modify myconfig.py to indicate your joystick model and use the --js arg to run. python manage.py drive --js As you drive, this will create a tub of records in your data dir as usual. Train You will not need to rsync your data, as it was recorded and resides locally. You can train as usual: python manage.py train --model models/mypilot.h5 Test You can use the model as usual: python manage.py drive --model models/mypilot.h5 Then navigate to web control page. Set Mode and Pilot to Local Pilot(d) . The car should start driving. Sample Driving Data Here's some sample driving data to get you started. Download this and unpack it into your data dir. This should train to a slow but stable driver. API Here's some info on the api to talk to the sim server. Make a TCP client and connect to port 9091 on whichever host the sim is running. The server sends and recieves UTF-8 encoded JSON packets. Each message must have a \"msg_type\" field. The sim will end all JSON packets with a newline character for termination. You don't have to end each packet with a newline when sending to the server. But if it gets too many messages too quickly it may have troubles. Check the player log file for JSON parse errors if you are having troubles. Get Protocol Version Client=>Sim. Ask for the version of the protocol. Will help know when changes are made to these messages. Fields: None Example: { \"msg_type\" : \"get_protocol_version\" } Protocol Version Sim=>Client. Reply for the version of the protocol. Currently at version 2. Fields: version : string integer Example: { \"msg_type\" : \"protocol_version\", \"version\" : \"2\", } Scene Selection Ready Sim=>Client. When the Menu scene is finished loading this will be sent. After this point, the sim can honor the Scene Loading message. (Menu only) Fields: None Example: { \"msg_type\" : \"scene_selection_ready\" } Get Scene Names Client=>Sim. Ask names of the scene you can load. (Menu only) Fields: None Example: { \"msg_type\" : \"get_scene_names\" } Scene Names Sim=>Client. Sim will reply with list of scene names. Fields: scene_names : array of scene names Example: { \"msg_type\" : \"scene_names\" \"scene_names\" : [ \"generated_road\", \"warehouse\", \"sparkfun_avc\". \"generated_track\" ] } Load Scene Client=>Sim. Asks the sim to load one of the scenes from the Menu screen. (Menu only) Fields: scene_name : generated_road | warehouse | sparkfun_avc | generated_track ( or whatever list the sim returns from get_scene_names) Example: { \"msg_type\" : \"load_scene\", \"scene_name\" : \"generated_track\" } Scene Loaded Sim=>Client. Once scene is loaded, in reply, you will get a: { \"msg_type\" : \"scene_loaded\" } Car Loaded Sim=>Client. Once the sim finishes loading your car, it sends this message. The car is loaded for you automatically once the scene is loaded with an active client. Or a client make a connection. Fields: None Example: { \"msg_type\" : \"car_loaded\" } Car Config Client=>Sim. Once loaded, you may configure your car visual details (scene only) Fields: body_style : donkey | bare | car01 | cybertruck | f1 body_r : string value of integer between 0-255 body_g : string value of integer between 0-255 body_b : string value of integer between 0-255 car_name : string value car name to display over car. Newline accepted for multi-line. font_size : string value of integer between 10-100 to set size of car name text Example: { \"msg_type\" : \"car_config\", \"body_style\" : \"car01\", \"body_r\" : \"128\", \"body_g\" : \"0\", \"body_b\" : \"255\", \"car_name\" : \"Your Name\", \"font_size\" : \"100\" } Camera Config Client=>Sim. Once the scene is loaded, you may configure your car camera sensor details Fields: fov : string value of float between 10-200. Sets the camera field of view in degrees. fish_eye_x : string value of float between 0-1. Causes distortion warping in x axis. fish_eye_y : string value of float between 0-1. Causes distortion warping in y axis. img_w : string value of integer between 16-512. Sets camera sensor image width. img_h : string value of integer between 16-512. Sets camera sensor image height. img_d : string value of integer 1 or 3. Sets camera sensor image depth. In case of 1, you get 3 channels but all identicle with greyscale conversion done on the sim. img_enc : Image format of data JPG | PNG | TGA offset_x : string value of float. Moves the camera left and right axis. offset_y : string value of float. Moves the camera up and down. offset_z : string value of float. Moves the camera forward and back. rot_x : string value of float. Degrees. Rotates camera around X axis. Example: { \"msg_type\" : \"cam_config\", \"fov\" : \"150\", \"fish_eye_x\" : \"1.0\", \"fish_eye_y\" : \"1.0\", \"img_w\" : \"255\", \"img_h\" : \"255\", \"img_d\" : \"1\", \"img_enc\" : \"PNG\", \"offset_x\" : \"0.0\", \"offset_y\" : \"3.0\", \"offset_z\" : \"0.0\", \"rot_x\" : \"90.0\" } Note: You can add an other camera by changing the msg_type to \"cam_config_b\" Control Car Client=>Sim. Control throttle and steering. Fields: steering : string value of float between -1 to 1. Maps to full left or right, 16 deg from center. throttle : string value of float between -1 to 1. Full forward or reverse torque to wheels. brake : string value of float between 0 to 1. Example: { \"msg_type\" : \"control\", \"steering\" : \"0.0\", \"throttle\" : \"0.3\", \"brake\" : \"0.0\" } Telemetry Sim=>Client. The sim sends this message containing camera image and details about vehicle state. These come at a regular rate set in the sim. Usually about 20 HZ. Fields: steering_angle : Last steering applied. Why not just steering like control? idk. throttle : Last throttle applied. speed : magnitude of linear velocity. image : a BinHex encoded binary image. Use PIL.Image.open(BytesIO(base64.b64decode(imgString))) imageb : (optionnal) same as above but for the second camera lidar : (optionnal) list of lidar points in the following format: {d: distanceToObject, rx: rayRotationX, ry: rayRotationY} hit : name of the last object struck. Or None if no object hit. accel_x : x acceleration of vehicle. accel_y : y acceleration of vehicle. accel_z : z acceleration of vehicle. gyro_x : x gyro acceleration. gyro_y : y gyro acceleration. gyro_z : z gyro acceleration. gyro_w : w gyro acceleration. activeNode : Progress on track (not working properly with multiple car for the moment) totalNodes : number of nodes on track pos_x : (training only) x world coordinate of vehicle. pos_y : (training only) y world coordinate of vehicle. pos_z : (training only) z world coordinate of vehicle. vel_x : (training only) x velocity of vehicle. vel_y : (training only) y velocity of vehicle. vel_z : (training only) z velocity of vehicle. cte : (training only) Cross track error. The distance from the car to the path in the center of the right most lane or center of the track (depends on the track) Example: { \"msg_type\" : \"telemetry\", \"steering_angle\" : \"0.0\", \"throttle\" : \"0.0\", \"speed\" : \"1.0\", \"image\" : \"0x123...\", \"hit\" : \"None\", \"pos_x\" : \"0.0\", \"pos_y\" : \"0.0\", \"pos_z\" : \"0.0\", \"accel_x\" : \"0.0\", \"accel_y\" : \"0.0\", \"accel_z\" : \"0.0\", \"gyro_x\" : \"0.0\", \"gyro_y\" : \"0.0\", \"gyro_z\" : \"0.0\", \"gyro_w\" : \"0.0\", \"activeNode\" : \"5\" \"totalNodes\" : \"26\" \"cte\" : \"0.5\" } Reset Car Client=>Sim. Return the car to the start point. Fields: None Example: { \"msg_type\" : \"reset_car\" } Set Car Position Client=>Sim. Move the car to the given position (training only) Fields: * pos_x : x world coordinate. * pos_y : y world coordinate. * pos_z : z world coordinate. Example: { \"msg_type\" : \"set_position\" \"pos_x\" : \"0.0\", \"pos_y\" : \"0.0\", \"pos_z\" : \"0.0\" } Exit Scene Client=>Sim. Leave the scene and return to the main menu screen. Fields: None Example: { \"msg_type\" : \"exit_scene\" } Quit App Client=>Sim. Close the sim executable. (Menu only) Fields: None Example: { \"msg_type\" : \"quit_app\" }","title":"Donkey Simulator."},{"location":"guide/simulator/#donkey-simulator","text":"The Donkey Gym project is a OpenAI gym wrapper around the Self Driving Sandbox donkey simulator ( sdsandbox ). When building the sim from source, checkout the donkey branch of the sdsandbox project. The simulator is built on the the Unity game platform, uses their internal physics and graphics, and connects to a donkey Python process to use our trained model to control the simulated Donkey.","title":"Donkey Simulator"},{"location":"guide/simulator/#installation-video","text":"Here's some videos to help you through the installation. Linux: https://youtu.be/J6Ll5Obtuxk Windows: https://youtu.be/wqQMmHVT8qw","title":"Installation Video:"},{"location":"guide/simulator/#my-virtual-donkey","text":"There are many ways to use the simulator, depending on your goals. You can use the simulator to get to know and use the standard Donkeycar drive/train/test cycle by treating it as virtual hardware. You will collect data, drive, and train using the same commands as if you were using a real robot. We will walk through that use-case first.","title":"My Virtual Donkey"},{"location":"guide/simulator/#install","text":"Download and unzip the simulator for your host pc platform from Donkey Gym Release . Place the simulator where you like. For this example it will be ~/projects/DonkeySimLinux. Your dir will have a different name depending on platform. Complete all the steps to install Donkey on your host pc . Setup DonkeyGym: cd ~/projects git clone https://github.com/tawnkramer/gym-donkeycar cd gym-donkeycar conda activate donkey pip install -e .[gym-donkeycar] You may use an existing ~/mycar donkey application, or begin a new one. Here we will start fresh: donkey createcar --path ~/mysim cd ~/mysim Edit your myconfig.py to enable donkey gym simulator wrapper, replace <user-name> and the other parts of the path: DONKEY_GYM = True DONKEY_SIM_PATH = \"/home/<user-name>/projects/DonkeySimLinux/donkey_sim.x86_64\" DONKEY_GYM_ENV_NAME = \"donkey-generated-track-v0\" Note: your path to the executable will vary depending on platform and user. Windows: DonkeySimWin/donkey_sim.exe Mac OS: DonkeySimMac/donkey_sim.app/Contents/MacOS/donkey_sim Linux: DonkeySimLinux/donkey_sim.x86_64","title":"Install"},{"location":"guide/simulator/#drive","text":"You may use all the normal commands to manage.py at this point. Such as: python manage.py drive This should start the simulator and connect to it automatically. By default you will have a web interface to control the donkey. Navigate to http://localhost:8887/drive to see control page. On Ubuntu Linux only, you may plug in your joystick of choice. If it mounts as /dev/input/js0 then there's a good chance it will work. Modify myconfig.py to indicate your joystick model and use the --js arg to run. python manage.py drive --js As you drive, this will create a tub of records in your data dir as usual.","title":"Drive"},{"location":"guide/simulator/#train","text":"You will not need to rsync your data, as it was recorded and resides locally. You can train as usual: python manage.py train --model models/mypilot.h5","title":"Train"},{"location":"guide/simulator/#test","text":"You can use the model as usual: python manage.py drive --model models/mypilot.h5 Then navigate to web control page. Set Mode and Pilot to Local Pilot(d) . The car should start driving.","title":"Test"},{"location":"guide/simulator/#sample-driving-data","text":"Here's some sample driving data to get you started. Download this and unpack it into your data dir. This should train to a slow but stable driver.","title":"Sample Driving Data"},{"location":"guide/simulator/#api","text":"Here's some info on the api to talk to the sim server. Make a TCP client and connect to port 9091 on whichever host the sim is running. The server sends and recieves UTF-8 encoded JSON packets. Each message must have a \"msg_type\" field. The sim will end all JSON packets with a newline character for termination. You don't have to end each packet with a newline when sending to the server. But if it gets too many messages too quickly it may have troubles. Check the player log file for JSON parse errors if you are having troubles.","title":"API"},{"location":"guide/simulator/#get-protocol-version","text":"Client=>Sim. Ask for the version of the protocol. Will help know when changes are made to these messages. Fields: None Example: { \"msg_type\" : \"get_protocol_version\" }","title":"Get Protocol Version"},{"location":"guide/simulator/#protocol-version","text":"Sim=>Client. Reply for the version of the protocol. Currently at version 2. Fields: version : string integer Example: { \"msg_type\" : \"protocol_version\", \"version\" : \"2\", }","title":"Protocol Version"},{"location":"guide/simulator/#scene-selection-ready","text":"Sim=>Client. When the Menu scene is finished loading this will be sent. After this point, the sim can honor the Scene Loading message. (Menu only) Fields: None Example: { \"msg_type\" : \"scene_selection_ready\" }","title":"Scene Selection Ready"},{"location":"guide/simulator/#get-scene-names","text":"Client=>Sim. Ask names of the scene you can load. (Menu only) Fields: None Example: { \"msg_type\" : \"get_scene_names\" }","title":"Get Scene Names"},{"location":"guide/simulator/#scene-names","text":"Sim=>Client. Sim will reply with list of scene names. Fields: scene_names : array of scene names Example: { \"msg_type\" : \"scene_names\" \"scene_names\" : [ \"generated_road\", \"warehouse\", \"sparkfun_avc\". \"generated_track\" ] }","title":"Scene Names"},{"location":"guide/simulator/#load-scene","text":"Client=>Sim. Asks the sim to load one of the scenes from the Menu screen. (Menu only) Fields: scene_name : generated_road | warehouse | sparkfun_avc | generated_track ( or whatever list the sim returns from get_scene_names) Example: { \"msg_type\" : \"load_scene\", \"scene_name\" : \"generated_track\" }","title":"Load Scene"},{"location":"guide/simulator/#scene-loaded","text":"Sim=>Client. Once scene is loaded, in reply, you will get a: { \"msg_type\" : \"scene_loaded\" }","title":"Scene Loaded"},{"location":"guide/simulator/#car-loaded","text":"Sim=>Client. Once the sim finishes loading your car, it sends this message. The car is loaded for you automatically once the scene is loaded with an active client. Or a client make a connection. Fields: None Example: { \"msg_type\" : \"car_loaded\" }","title":"Car Loaded"},{"location":"guide/simulator/#car-config","text":"Client=>Sim. Once loaded, you may configure your car visual details (scene only) Fields: body_style : donkey | bare | car01 | cybertruck | f1 body_r : string value of integer between 0-255 body_g : string value of integer between 0-255 body_b : string value of integer between 0-255 car_name : string value car name to display over car. Newline accepted for multi-line. font_size : string value of integer between 10-100 to set size of car name text Example: { \"msg_type\" : \"car_config\", \"body_style\" : \"car01\", \"body_r\" : \"128\", \"body_g\" : \"0\", \"body_b\" : \"255\", \"car_name\" : \"Your Name\", \"font_size\" : \"100\" }","title":"Car Config"},{"location":"guide/simulator/#camera-config","text":"Client=>Sim. Once the scene is loaded, you may configure your car camera sensor details Fields: fov : string value of float between 10-200. Sets the camera field of view in degrees. fish_eye_x : string value of float between 0-1. Causes distortion warping in x axis. fish_eye_y : string value of float between 0-1. Causes distortion warping in y axis. img_w : string value of integer between 16-512. Sets camera sensor image width. img_h : string value of integer between 16-512. Sets camera sensor image height. img_d : string value of integer 1 or 3. Sets camera sensor image depth. In case of 1, you get 3 channels but all identicle with greyscale conversion done on the sim. img_enc : Image format of data JPG | PNG | TGA offset_x : string value of float. Moves the camera left and right axis. offset_y : string value of float. Moves the camera up and down. offset_z : string value of float. Moves the camera forward and back. rot_x : string value of float. Degrees. Rotates camera around X axis. Example: { \"msg_type\" : \"cam_config\", \"fov\" : \"150\", \"fish_eye_x\" : \"1.0\", \"fish_eye_y\" : \"1.0\", \"img_w\" : \"255\", \"img_h\" : \"255\", \"img_d\" : \"1\", \"img_enc\" : \"PNG\", \"offset_x\" : \"0.0\", \"offset_y\" : \"3.0\", \"offset_z\" : \"0.0\", \"rot_x\" : \"90.0\" } Note: You can add an other camera by changing the msg_type to \"cam_config_b\"","title":"Camera Config"},{"location":"guide/simulator/#control-car","text":"Client=>Sim. Control throttle and steering. Fields: steering : string value of float between -1 to 1. Maps to full left or right, 16 deg from center. throttle : string value of float between -1 to 1. Full forward or reverse torque to wheels. brake : string value of float between 0 to 1. Example: { \"msg_type\" : \"control\", \"steering\" : \"0.0\", \"throttle\" : \"0.3\", \"brake\" : \"0.0\" }","title":"Control Car"},{"location":"guide/simulator/#telemetry","text":"Sim=>Client. The sim sends this message containing camera image and details about vehicle state. These come at a regular rate set in the sim. Usually about 20 HZ. Fields: steering_angle : Last steering applied. Why not just steering like control? idk. throttle : Last throttle applied. speed : magnitude of linear velocity. image : a BinHex encoded binary image. Use PIL.Image.open(BytesIO(base64.b64decode(imgString))) imageb : (optionnal) same as above but for the second camera lidar : (optionnal) list of lidar points in the following format: {d: distanceToObject, rx: rayRotationX, ry: rayRotationY} hit : name of the last object struck. Or None if no object hit. accel_x : x acceleration of vehicle. accel_y : y acceleration of vehicle. accel_z : z acceleration of vehicle. gyro_x : x gyro acceleration. gyro_y : y gyro acceleration. gyro_z : z gyro acceleration. gyro_w : w gyro acceleration. activeNode : Progress on track (not working properly with multiple car for the moment) totalNodes : number of nodes on track pos_x : (training only) x world coordinate of vehicle. pos_y : (training only) y world coordinate of vehicle. pos_z : (training only) z world coordinate of vehicle. vel_x : (training only) x velocity of vehicle. vel_y : (training only) y velocity of vehicle. vel_z : (training only) z velocity of vehicle. cte : (training only) Cross track error. The distance from the car to the path in the center of the right most lane or center of the track (depends on the track) Example: { \"msg_type\" : \"telemetry\", \"steering_angle\" : \"0.0\", \"throttle\" : \"0.0\", \"speed\" : \"1.0\", \"image\" : \"0x123...\", \"hit\" : \"None\", \"pos_x\" : \"0.0\", \"pos_y\" : \"0.0\", \"pos_z\" : \"0.0\", \"accel_x\" : \"0.0\", \"accel_y\" : \"0.0\", \"accel_z\" : \"0.0\", \"gyro_x\" : \"0.0\", \"gyro_y\" : \"0.0\", \"gyro_z\" : \"0.0\", \"gyro_w\" : \"0.0\", \"activeNode\" : \"5\" \"totalNodes\" : \"26\" \"cte\" : \"0.5\" }","title":"Telemetry"},{"location":"guide/simulator/#reset-car","text":"Client=>Sim. Return the car to the start point. Fields: None Example: { \"msg_type\" : \"reset_car\" }","title":"Reset Car"},{"location":"guide/simulator/#set-car-position","text":"Client=>Sim. Move the car to the given position (training only) Fields: * pos_x : x world coordinate. * pos_y : y world coordinate. * pos_z : z world coordinate. Example: { \"msg_type\" : \"set_position\" \"pos_x\" : \"0.0\", \"pos_y\" : \"0.0\", \"pos_z\" : \"0.0\" }","title":"Set Car Position"},{"location":"guide/simulator/#exit-scene","text":"Client=>Sim. Leave the scene and return to the main menu screen. Fields: None Example: { \"msg_type\" : \"exit_scene\" }","title":"Exit Scene"},{"location":"guide/simulator/#quit-app","text":"Client=>Sim. Close the sim executable. (Menu only) Fields: None Example: { \"msg_type\" : \"quit_app\" }","title":"Quit App"},{"location":"guide/train_autopilot/","text":"Train an autopilot with Keras Now that you're able to drive your car reliably you can use Keras to train a neural network to drive like you. Here are the steps. Collect Data Make sure you collect good data. Practice driving around the track a couple times. When you're confident you can drive 10 laps without mistake, restart the python mange.py process to create a new tub session. Press Start Recording if using web controller. The joystick will auto record with any non-zero throttle. If you crash or run off the track press Stop Car immediately to stop recording. If you are using a joystick tap the Triangle button to erase the last 5 seconds of records. After you've collected 10-20 laps of good data (5-20k images) you can stop your car with Ctrl-c in the ssh session for your car. The data you've collected is in the data folder in the most recent tub folder. Transfer data from your car to your computer Since the Raspberry Pi is not very powerful, we need to transfer the data to a PC computer to train. The Jetson nano is more powerful, but still quite slow to train. If desired, skip this transfer step and train on the Nano. In a new terminal session on your host PC use rsync to copy your cars folder from the Raspberry Pi. rsync -rv --progress --partial pi@<your_pi_ip_address>:~/mycar/data/ ~/mycar/data/ Train a model In the same terminal you can now run the training script on the latest tub by passing the path to that tub as an argument. You can optionally pass path masks, such as ./data/* or ./data/tub_?_17-08-28 to gather multiple tubs. For example: donkey train --tub <tub folder names comma separated> --model ./models/mypilot.h5 You can create different model types with the --type argument during training. You may also choose to change the default model type in myconfig.py DEFAULT_MODEL_TYPE . When specifying a new model type, be sure to provide that type when running the model, or using the model in other tools like plotting or profiling. For more information on the different model types, look here for Keras Parts . Copy model back to car In previous step we managed to get a model trained on the data. Now is time to move the model back to Rasberry Pi, so we can use it for testing it if it will drive itself. Use rsync again to move your trained model pilot back to your car. rsync -rv --progress --partial ~/mycar/models/ pi@<your_ip_address>:~/mycar/models/ Ensure to place car on the track so that it is ready to drive. Now you can start your car again and pass it your model to drive. python manage.py drive --model ~/mycar/models/mypilot.h5 The car should start to drive on it's own, congratulations! [Optional] Use TensorRT on the Jetson Nano Read this for more information. Training Tips Mode & Pilot : Congratulations on getting it this far. The first thing to note after running the command above, is to look at the options in the Mode & Pilot menu. It can be pretty confusing. So here's what the different options mean: a. User : As you guessed, this is where you are in control of both the steering and throttle control. b. Local Angle : Not too obvious, but this is where the trained model (mypilot from above) controls the steering. The Local refers to the trained model which is locally hosted on the raspberry-pi. c. Local Pilot : This is where the trained model (mypilot) assumes control of both the steering and the throttle. As of now, it's purportedly not very reliable. Be sure to also check out the Max Throttle and Throttle Mode options, and play around with a few settings. Can help with training quite a lot. Build a Simple Track : This isn't very well-documented, but the car should (theoretically) be able to train against any kind of track. To start off with, it might not be necessary to build a two-lane track with a striped center-lane. Try with a single lane with no center-line, or just a single strip that makes a circuit! At the least, you'll be able to do an end-to-end testing and verify that the software pipeline is all properly functional. Of course, as the next-step, you'll want to create a more standard track, and compete at a meetup nearest to you! Get help : Try to get some helping hands from a friend or two. Again, this helps immensely with building the track, because it is harder than it looks to build a two-line track on your own! Also, you can save on resources (and tapes) by using a ribbon instead of tapes. They'll still need a bit of tapes to hold them, but you can reuse them and they can be laid down with a lot less effort (Although the wind, if you're working outside, might make it difficult to lay them down initially). Training Behavior Models How to train a Behavior model Make sure TRAIN_BEHAVIORS = True in myconfig.py when training and when running on the robot. Setup an RGB led on robot to indicate which state is active. Enable in config.py. Verify when running robot that L1 PS3 button changes state led indicator. (that's the left upper shoulder button) By default there are two states. If you like, adjust the number of states in bottom of config.py. Rename or change BEHAVIOR_LIST to an arbitrary number of labels. Make sure same number of rgb colors in BEHAVIOR_LED_COLORS . Make sure to reflect any changes to both PC and Robot. Now for training: Activate any state with L1 shoulder button. Then drive as you wish the car to drive when in that state. Switch states and then transition to the new steady state behavior. For the two lane case. Drive 33% in one lane, 33% in the other, and 33% transitioning between them. It's important to trigger the state transition before changing lanes. Check the records in the tub. Open a .json. In addition to steering and throttle, you should also have some additional state information about your behavior vector and which was was activate on that frame. This is crucial to training correctly. Move data to PC and train as normal, ensuring TRAIN_BEHAVIORS = True in myconfig.py on PC, otherwise extra state information will be ignored. Move trained model back to robot. Now place the robot in the location of the initial state. Start the robot with the given model python manage.py drive --model=models/my_beh.h5 --type=behavior Now press select to switch to desired AI mode. Constant throttle available as well as trained throttle. As it drives, you can now toggle states with L1 and see whether and how much it can replicate your steady state behaviors and transitions. Be sure to include quite a lot of example of transitions from one state to another. At least 50, but more like 100.","title":"Train an autopilot."},{"location":"guide/train_autopilot/#train-an-autopilot-with-keras","text":"Now that you're able to drive your car reliably you can use Keras to train a neural network to drive like you. Here are the steps.","title":"Train an autopilot with Keras"},{"location":"guide/train_autopilot/#collect-data","text":"Make sure you collect good data. Practice driving around the track a couple times. When you're confident you can drive 10 laps without mistake, restart the python mange.py process to create a new tub session. Press Start Recording if using web controller. The joystick will auto record with any non-zero throttle. If you crash or run off the track press Stop Car immediately to stop recording. If you are using a joystick tap the Triangle button to erase the last 5 seconds of records. After you've collected 10-20 laps of good data (5-20k images) you can stop your car with Ctrl-c in the ssh session for your car. The data you've collected is in the data folder in the most recent tub folder.","title":"Collect Data"},{"location":"guide/train_autopilot/#transfer-data-from-your-car-to-your-computer","text":"Since the Raspberry Pi is not very powerful, we need to transfer the data to a PC computer to train. The Jetson nano is more powerful, but still quite slow to train. If desired, skip this transfer step and train on the Nano. In a new terminal session on your host PC use rsync to copy your cars folder from the Raspberry Pi. rsync -rv --progress --partial pi@<your_pi_ip_address>:~/mycar/data/ ~/mycar/data/","title":"Transfer data from your car to your computer"},{"location":"guide/train_autopilot/#train-a-model","text":"In the same terminal you can now run the training script on the latest tub by passing the path to that tub as an argument. You can optionally pass path masks, such as ./data/* or ./data/tub_?_17-08-28 to gather multiple tubs. For example: donkey train --tub <tub folder names comma separated> --model ./models/mypilot.h5 You can create different model types with the --type argument during training. You may also choose to change the default model type in myconfig.py DEFAULT_MODEL_TYPE . When specifying a new model type, be sure to provide that type when running the model, or using the model in other tools like plotting or profiling. For more information on the different model types, look here for Keras Parts .","title":"Train a model"},{"location":"guide/train_autopilot/#copy-model-back-to-car","text":"In previous step we managed to get a model trained on the data. Now is time to move the model back to Rasberry Pi, so we can use it for testing it if it will drive itself. Use rsync again to move your trained model pilot back to your car. rsync -rv --progress --partial ~/mycar/models/ pi@<your_ip_address>:~/mycar/models/ Ensure to place car on the track so that it is ready to drive. Now you can start your car again and pass it your model to drive. python manage.py drive --model ~/mycar/models/mypilot.h5 The car should start to drive on it's own, congratulations!","title":"Copy model back to car"},{"location":"guide/train_autopilot/#optional-use-tensorrt-on-the-jetson-nano","text":"Read this for more information.","title":"[Optional] Use TensorRT on the Jetson Nano"},{"location":"guide/train_autopilot/#training-tips","text":"Mode & Pilot : Congratulations on getting it this far. The first thing to note after running the command above, is to look at the options in the Mode & Pilot menu. It can be pretty confusing. So here's what the different options mean: a. User : As you guessed, this is where you are in control of both the steering and throttle control. b. Local Angle : Not too obvious, but this is where the trained model (mypilot from above) controls the steering. The Local refers to the trained model which is locally hosted on the raspberry-pi. c. Local Pilot : This is where the trained model (mypilot) assumes control of both the steering and the throttle. As of now, it's purportedly not very reliable. Be sure to also check out the Max Throttle and Throttle Mode options, and play around with a few settings. Can help with training quite a lot. Build a Simple Track : This isn't very well-documented, but the car should (theoretically) be able to train against any kind of track. To start off with, it might not be necessary to build a two-lane track with a striped center-lane. Try with a single lane with no center-line, or just a single strip that makes a circuit! At the least, you'll be able to do an end-to-end testing and verify that the software pipeline is all properly functional. Of course, as the next-step, you'll want to create a more standard track, and compete at a meetup nearest to you! Get help : Try to get some helping hands from a friend or two. Again, this helps immensely with building the track, because it is harder than it looks to build a two-line track on your own! Also, you can save on resources (and tapes) by using a ribbon instead of tapes. They'll still need a bit of tapes to hold them, but you can reuse them and they can be laid down with a lot less effort (Although the wind, if you're working outside, might make it difficult to lay them down initially).","title":"Training Tips"},{"location":"guide/train_autopilot/#training-behavior-models","text":"","title":"Training Behavior Models"},{"location":"guide/train_autopilot/#how-to-train-a-behavior-model","text":"Make sure TRAIN_BEHAVIORS = True in myconfig.py when training and when running on the robot. Setup an RGB led on robot to indicate which state is active. Enable in config.py. Verify when running robot that L1 PS3 button changes state led indicator. (that's the left upper shoulder button) By default there are two states. If you like, adjust the number of states in bottom of config.py. Rename or change BEHAVIOR_LIST to an arbitrary number of labels. Make sure same number of rgb colors in BEHAVIOR_LED_COLORS . Make sure to reflect any changes to both PC and Robot. Now for training: Activate any state with L1 shoulder button. Then drive as you wish the car to drive when in that state. Switch states and then transition to the new steady state behavior. For the two lane case. Drive 33% in one lane, 33% in the other, and 33% transitioning between them. It's important to trigger the state transition before changing lanes. Check the records in the tub. Open a .json. In addition to steering and throttle, you should also have some additional state information about your behavior vector and which was was activate on that frame. This is crucial to training correctly. Move data to PC and train as normal, ensuring TRAIN_BEHAVIORS = True in myconfig.py on PC, otherwise extra state information will be ignored. Move trained model back to robot. Now place the robot in the location of the initial state. Start the robot with the given model python manage.py drive --model=models/my_beh.h5 --type=behavior Now press select to switch to desired AI mode. Constant throttle available as well as trained throttle. As it drives, you can now toggle states with L1 and see whether and how much it can replicate your steady state behaviors and transitions. Be sure to include quite a lot of example of transitions from one state to another. At least 50, but more like 100.","title":"How to train a Behavior model"},{"location":"guide/virtual_race_league/","text":"Virtual Race League We've taken the next step in DIY Robocars competitions. We are now hosting special events online! We welcome competitors from all over the world. Events will be scheduled by Chris Anderson and the Donkeycar maintainers. But this is by no means donkeycar only. Please read on, and we will provide two paths depending on whether you decide to use the donkeycar framework to race. We will be broadcasting the race stream over Twitch. Check the event annoucement for the url. Race comepetitors will join a group Zoom chat event. Tawn will host the race server and share the stream over Zoom/Twitch. And we will see how things go. About the Sim server We are using the SDSandbox open source project as the racing sim. This creates a 3d environment using Unity game creation tool. It uses NVidia PhysX open source physics engine to simulate 4 wheeled vehicle dynamics. This sim also acts as a server, listening on TCP port 9091. This sends and receives JSON packets. More on the API later. We use an OpenAI GYM style wrapper to interface with the server. The project for this wrapper is gym-donkeycar . You may build the server from the source project above, but pre-built binaries for Ubuntu, Mac, and Windows. This has been tested on Ubuntu 18.04, Mac 10.13, and Windows 10. Setup for Donkeycar users If you are using the donkeycar framework to race, you can use follow the guide to setup the simulator . If visuals directions help out, checkout the Windows Sim Setup Screen-Cast on Youtube . Use this to practice before the race. When it comes time to race, modify your myconfig.py to have these two changes: DONKEY_SIM_PATH = \"remote\" SIM_HOST = \"trainmydonkey.com\" This racing server will not always be running. We will bring it up for testing events and on race day. We are aiming to have it up from 7pm-9pm Pacific every night a week before race day. If not up, ask on Discord and we will try to get things running. Note: If you trained a donkey model, but wish to run it on a Jetson Nano or some platform where you are having troubles installing all the dependencies, here's a single script you can use to run without any donkeycar or gym-donkeycar dependencies. Just pass it the model file name, the host name, and the car name. And it will run as a client to the race sim. Setup for Non-Donkeycar users If you would like to roll your own client, we have some python code to get you started. You will first want to download the sim pre-built binary for your platform. Extract that where you like. Then clone the gym-donkeycar python project and install. If you are using a virtual environment, don't forget to activate it first. git clone https://github.com/tawnkramer/gym-donkeycar pip install -e gym-donkeycar get the test client. Download via wget on Mac or Linux like: wget https://raw.githubusercontent.com/tawnkramer/sdsandbox/master/src/test_client.py or on Windows open a browser to https://github.com/tawnkramer/sdsandbox/tree/master/src then right click on test_client.py and choose \"Save link as...\" and choose a location on your PC. start up the simulator and let it get to the menu screen. run the test client like python3 test_client.py Checkout test_client.py to see what's going there. Class SimpleClient connects to the host of your choosing. Then it sends a load scene command depending on which course you want to try. It then sends some car visual configuration, and then some camera config information. Then it enters an update loop. You can try changing the num_clients variable to 2 or more clients. See how the sim can handle them. The test client will send random steering command for time_to_drive = 1.0 seconds. Then quit. During that time, the telemetry messages will come into SimpleClient::on_msg_recv. See them printed out for you. Also take a look at the 'test.png' that it writes to get a feel for what the camera looks like. There's some comments in there explaining the camera configuration in detail. If you have a custom camera setup, hopefully we can come close to matching it with these controls. When it's time to race, change the variable: host = \"trainmydonkey.com\" Be sure to enable controls to start the car on your command. We will likely be old school calling 3, 2, 1, GO! over video chat. Getting Help There's a lot to learn. And come to Discord to get some help. Check out the #virtual-racing-league channel there.","title":"Virtual Race League."},{"location":"guide/virtual_race_league/#virtual-race-league","text":"We've taken the next step in DIY Robocars competitions. We are now hosting special events online! We welcome competitors from all over the world. Events will be scheduled by Chris Anderson and the Donkeycar maintainers. But this is by no means donkeycar only. Please read on, and we will provide two paths depending on whether you decide to use the donkeycar framework to race. We will be broadcasting the race stream over Twitch. Check the event annoucement for the url. Race comepetitors will join a group Zoom chat event. Tawn will host the race server and share the stream over Zoom/Twitch. And we will see how things go.","title":"Virtual Race League"},{"location":"guide/virtual_race_league/#about-the-sim-server","text":"We are using the SDSandbox open source project as the racing sim. This creates a 3d environment using Unity game creation tool. It uses NVidia PhysX open source physics engine to simulate 4 wheeled vehicle dynamics. This sim also acts as a server, listening on TCP port 9091. This sends and receives JSON packets. More on the API later. We use an OpenAI GYM style wrapper to interface with the server. The project for this wrapper is gym-donkeycar . You may build the server from the source project above, but pre-built binaries for Ubuntu, Mac, and Windows. This has been tested on Ubuntu 18.04, Mac 10.13, and Windows 10.","title":"About the Sim server"},{"location":"guide/virtual_race_league/#setup-for-donkeycar-users","text":"If you are using the donkeycar framework to race, you can use follow the guide to setup the simulator . If visuals directions help out, checkout the Windows Sim Setup Screen-Cast on Youtube . Use this to practice before the race. When it comes time to race, modify your myconfig.py to have these two changes: DONKEY_SIM_PATH = \"remote\" SIM_HOST = \"trainmydonkey.com\" This racing server will not always be running. We will bring it up for testing events and on race day. We are aiming to have it up from 7pm-9pm Pacific every night a week before race day. If not up, ask on Discord and we will try to get things running. Note: If you trained a donkey model, but wish to run it on a Jetson Nano or some platform where you are having troubles installing all the dependencies, here's a single script you can use to run without any donkeycar or gym-donkeycar dependencies. Just pass it the model file name, the host name, and the car name. And it will run as a client to the race sim.","title":"Setup for Donkeycar users"},{"location":"guide/virtual_race_league/#setup-for-non-donkeycar-users","text":"If you would like to roll your own client, we have some python code to get you started. You will first want to download the sim pre-built binary for your platform. Extract that where you like. Then clone the gym-donkeycar python project and install. If you are using a virtual environment, don't forget to activate it first. git clone https://github.com/tawnkramer/gym-donkeycar pip install -e gym-donkeycar get the test client. Download via wget on Mac or Linux like: wget https://raw.githubusercontent.com/tawnkramer/sdsandbox/master/src/test_client.py or on Windows open a browser to https://github.com/tawnkramer/sdsandbox/tree/master/src then right click on test_client.py and choose \"Save link as...\" and choose a location on your PC. start up the simulator and let it get to the menu screen. run the test client like python3 test_client.py Checkout test_client.py to see what's going there. Class SimpleClient connects to the host of your choosing. Then it sends a load scene command depending on which course you want to try. It then sends some car visual configuration, and then some camera config information. Then it enters an update loop. You can try changing the num_clients variable to 2 or more clients. See how the sim can handle them. The test client will send random steering command for time_to_drive = 1.0 seconds. Then quit. During that time, the telemetry messages will come into SimpleClient::on_msg_recv. See them printed out for you. Also take a look at the 'test.png' that it writes to get a feel for what the camera looks like. There's some comments in there explaining the camera configuration in detail. If you have a custom camera setup, hopefully we can come close to matching it with these controls. When it's time to race, change the variable: host = \"trainmydonkey.com\" Be sure to enable controls to start the car on your command. We will likely be old school calling 3, 2, 1, GO! over video chat.","title":"Setup for Non-Donkeycar users"},{"location":"guide/virtual_race_league/#getting-help","text":"There's a lot to learn. And come to Discord to get some help. Check out the #virtual-racing-league channel there.","title":"Getting Help"},{"location":"guide/host_pc/setup_mac/","text":"Install Donkeycar on Mac Install miniconda Python 3.7 64 bit Install git 64 bit Start Terminal Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install/envs/mac.yml conda activate donkey pip install -e .[pc] Note: if you are using ZSH (you'll know if you are), you won't be able to run pip install -e .[pc] . You'll need to escape the brackets and run pip install -e .\\[pc\\] . Tensorflow GPU Currently there is no NVidia gpu support for tensorflow on mac . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Terminal, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries Next let's install software on Donkeycar","title":"Setup mac"},{"location":"guide/host_pc/setup_mac/#install-donkeycar-on-mac","text":"Install miniconda Python 3.7 64 bit Install git 64 bit Start Terminal Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install/envs/mac.yml conda activate donkey pip install -e .[pc] Note: if you are using ZSH (you'll know if you are), you won't be able to run pip install -e .[pc] . You'll need to escape the brackets and run pip install -e .\\[pc\\] . Tensorflow GPU Currently there is no NVidia gpu support for tensorflow on mac . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Terminal, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries","title":"Install Donkeycar on Mac"},{"location":"guide/host_pc/setup_mac/#next-lets-install-software-on-donkeycar","text":"","title":"Next let's install software on Donkeycar"},{"location":"guide/host_pc/setup_ubuntu/","text":"Install Donkeycar on Linux Note : tested on Ubuntu 18.04 LTS Open the Terminal application. Install miniconda Python 3.7 64 bit . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash ./Miniconda3-latest-Linux-x86_64.sh Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install/envs/ubuntu.yml conda activate donkey pip install -e .[pc] Note: if you are using ZSH (you'll know if you are), you won't be able to run pip install -e .[pc] . You'll need to escape the brackets and run pip install -e .\\[pc\\] . Optional Install Tensorflow GPU - only for NVidia Graphics cards You should have an NVidia GPU with the latest drivers. Conda will handle installing the correct cuda and cuddn libraries for the version of tensorflow you are using. conda install tensorflow-gpu==2.2.0 Optional Install Coral edge tpu compiler If you have a Google Coral edge tpu, you may wish to compile models. You will need to install the edgetpu_compiler exectutable. Follow their instructions . Optionally configure PyTorch to use GPU - only for NVidia Graphics cards If you have an NVidia card, you should update to the latest drivers and install Cuda SDK . conda install cudatoolkit=<CUDA Version> -c pytorch You should replace <CUDA Version> with your CUDA version. Any version above 10.0 should work. You can find out your CUDA version by running nvcc --version or nvidia-smi . (if those commands don't work, it means you don't already have them installed. Follow the directions given by that error to install them.) If the version given by these two commands don't match, go with the version given by nvidia-smi . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Anaconda Prompt, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries Next let's install software on Donkeycar","title":"Install Donkeycar on Linux"},{"location":"guide/host_pc/setup_ubuntu/#install-donkeycar-on-linux","text":"Note : tested on Ubuntu 18.04 LTS Open the Terminal application. Install miniconda Python 3.7 64 bit . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash ./Miniconda3-latest-Linux-x86_64.sh Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install/envs/ubuntu.yml conda activate donkey pip install -e .[pc] Note: if you are using ZSH (you'll know if you are), you won't be able to run pip install -e .[pc] . You'll need to escape the brackets and run pip install -e .\\[pc\\] . Optional Install Tensorflow GPU - only for NVidia Graphics cards You should have an NVidia GPU with the latest drivers. Conda will handle installing the correct cuda and cuddn libraries for the version of tensorflow you are using. conda install tensorflow-gpu==2.2.0 Optional Install Coral edge tpu compiler If you have a Google Coral edge tpu, you may wish to compile models. You will need to install the edgetpu_compiler exectutable. Follow their instructions . Optionally configure PyTorch to use GPU - only for NVidia Graphics cards If you have an NVidia card, you should update to the latest drivers and install Cuda SDK . conda install cudatoolkit=<CUDA Version> -c pytorch You should replace <CUDA Version> with your CUDA version. Any version above 10.0 should work. You can find out your CUDA version by running nvcc --version or nvidia-smi . (if those commands don't work, it means you don't already have them installed. Follow the directions given by that error to install them.) If the version given by these two commands don't match, go with the version given by nvidia-smi . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Anaconda Prompt, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries","title":"Install Donkeycar on Linux"},{"location":"guide/host_pc/setup_ubuntu/#next-lets-install-software-on-donkeycar","text":"","title":"Next let's install software on Donkeycar"},{"location":"guide/host_pc/setup_windows/","text":"Windows Windows provides a few different methods for setting up and installing Donkey Car. Miniconda Native Windows Subsystem for Linux (WSL) - Experimental If you are unfamiliar or concerned about the way that you install Donkey Car, please use option 1 above. Install Donkeycar on Windows (miniconda) Install miniconda Python 3.7 64 bit . Open the Anaconda prompt window via Start Menu | Anaconda 64bit | Anaconda Prompt type git . If the command is not found, then install git 64 bit Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkey from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install\\envs\\windows.yml conda activate donkey pip install --user tensorflow==2.2.0 pip install -e .[pc] Note: if you are using ZSH (you'll know if you are), you won't be able to run pip install -e .[pc] . You'll need to escape the brackets and run pip install -e .\\[pc\\] . Optionally Install Tensorflow GPU - only for NVidia Graphics cards If you have an NVidia card, you should update to the lastest drivers and install Cuda SDK . conda install tensorflow-gpu==2.2.0 Optionally configure PyTorch to use GPU - only for NVidia Graphics cards If you have an NVidia card, you should update to the lastest drivers and install Cuda SDK . conda install cudatoolkit=<CUDA Version> -c pytorch You should replace <CUDA Version> with your CUDA version. Any version above 10.0 should work. You can find out your CUDA version by running nvcc --version or nvidia-smi . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Anaconda Prompt, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries Next let's install software on Donkeycar Install Donkeycar on Windows (native) Install Python 3.6 (or later) Install Git Bash . During install make sure you check Git to run 'from command line and also from 3rd-party software'. Open Command Prompt as Administrator via the Start Menu (cmd.exe | right-click | run as administrator) Change to a folder that that you would like to use for all your projects mkdir projects cd projects Get the latest donkey from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master NOTE: The dev branch has the latest (unstable) version of donkeycar with experimental features. Install Donkeycar into Python pip3 install -e .[pc] Recommended for GPU Users: Install Tensorflow GPU - only for NVIDIA Graphics cards If you have an NVIDIA card, you should update to the lastest drivers and install Cuda SDK . pip3 install tensorflow Create your local working dir: donkey createcar --path \\Users\\<username>\\projects\\mycar --template complete Templates There are a number of different templates to choose from in Donkey Car. basic | complete You can find all the templates in the donkeycar/donkeycar/templates folder Next let's install software on Donkeycar Install Donkeycar on Windows (WSL) The Windows Subsystem for Linux (WSL) lets developers run a GNU/Linux environment -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a traditional virtual machine or dualboot setup. Install Windows Subsystem for Linux . Turn on Windows 10 \"Windows Subsystem for Linux\" Feature (Settings > Apps > Programs and Features > Turn Windows features on or off) Download a Linux Distribution from the Microsoft Store (recommend Ubuntu Latest) Open the Ubuntu App and configure. Open the Ubuntu App to get a prompt window via Start Menu | Ubuntu Install git using sudo apt install git Install python3 using sudo apt install python3 Change to a directory that you would like to use as the head of all your projects. mkdir projects cd projects Get the latest donkey from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master NOTE: The dev branch has the latest (unstable) version of donkeycar with experimental features. Install Donkeycar into Python pip3 install -e .[pc] Experimental Support - GPU Users: Install Tensorflow GPU - only for NVIDIA Graphics cards If you have an NVIDIA card, you should update to the lastest drivers and install Cuda SDK . pip3 install tensorflow Create your local working dir: donkey createcar --path /path/to/projects/mycar --template complete Templates There are a number of different templates to choose from in Donkey Car. basic | complete You can find all the templates in the donkeycar/donkeycar/templates folder Next let's install software on Donkeycar","title":"Windows"},{"location":"guide/host_pc/setup_windows/#windows","text":"Windows provides a few different methods for setting up and installing Donkey Car. Miniconda Native Windows Subsystem for Linux (WSL) - Experimental If you are unfamiliar or concerned about the way that you install Donkey Car, please use option 1 above.","title":"Windows"},{"location":"guide/host_pc/setup_windows/#install-donkeycar-on-windows-miniconda","text":"Install miniconda Python 3.7 64 bit . Open the Anaconda prompt window via Start Menu | Anaconda 64bit | Anaconda Prompt type git . If the command is not found, then install git 64 bit Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkey from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install\\envs\\windows.yml conda activate donkey pip install --user tensorflow==2.2.0 pip install -e .[pc] Note: if you are using ZSH (you'll know if you are), you won't be able to run pip install -e .[pc] . You'll need to escape the brackets and run pip install -e .\\[pc\\] . Optionally Install Tensorflow GPU - only for NVidia Graphics cards If you have an NVidia card, you should update to the lastest drivers and install Cuda SDK . conda install tensorflow-gpu==2.2.0 Optionally configure PyTorch to use GPU - only for NVidia Graphics cards If you have an NVidia card, you should update to the lastest drivers and install Cuda SDK . conda install cudatoolkit=<CUDA Version> -c pytorch You should replace <CUDA Version> with your CUDA version. Any version above 10.0 should work. You can find out your CUDA version by running nvcc --version or nvidia-smi . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Anaconda Prompt, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries","title":"Install Donkeycar on Windows (miniconda)"},{"location":"guide/host_pc/setup_windows/#next-lets-install-software-on-donkeycar","text":"","title":"Next let's install software on Donkeycar"},{"location":"guide/host_pc/setup_windows/#install-donkeycar-on-windows-native","text":"Install Python 3.6 (or later) Install Git Bash . During install make sure you check Git to run 'from command line and also from 3rd-party software'. Open Command Prompt as Administrator via the Start Menu (cmd.exe | right-click | run as administrator) Change to a folder that that you would like to use for all your projects mkdir projects cd projects Get the latest donkey from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master NOTE: The dev branch has the latest (unstable) version of donkeycar with experimental features. Install Donkeycar into Python pip3 install -e .[pc] Recommended for GPU Users: Install Tensorflow GPU - only for NVIDIA Graphics cards If you have an NVIDIA card, you should update to the lastest drivers and install Cuda SDK . pip3 install tensorflow Create your local working dir: donkey createcar --path \\Users\\<username>\\projects\\mycar --template complete Templates There are a number of different templates to choose from in Donkey Car. basic | complete You can find all the templates in the donkeycar/donkeycar/templates folder","title":"Install Donkeycar on Windows (native)"},{"location":"guide/host_pc/setup_windows/#next-lets-install-software-on-donkeycar_1","text":"","title":"Next let's install software on Donkeycar"},{"location":"guide/host_pc/setup_windows/#install-donkeycar-on-windows-wsl","text":"The Windows Subsystem for Linux (WSL) lets developers run a GNU/Linux environment -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a traditional virtual machine or dualboot setup. Install Windows Subsystem for Linux . Turn on Windows 10 \"Windows Subsystem for Linux\" Feature (Settings > Apps > Programs and Features > Turn Windows features on or off) Download a Linux Distribution from the Microsoft Store (recommend Ubuntu Latest) Open the Ubuntu App and configure. Open the Ubuntu App to get a prompt window via Start Menu | Ubuntu Install git using sudo apt install git Install python3 using sudo apt install python3 Change to a directory that you would like to use as the head of all your projects. mkdir projects cd projects Get the latest donkey from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master NOTE: The dev branch has the latest (unstable) version of donkeycar with experimental features. Install Donkeycar into Python pip3 install -e .[pc] Experimental Support - GPU Users: Install Tensorflow GPU - only for NVIDIA Graphics cards If you have an NVIDIA card, you should update to the lastest drivers and install Cuda SDK . pip3 install tensorflow Create your local working dir: donkey createcar --path /path/to/projects/mycar --template complete Templates There are a number of different templates to choose from in Donkey Car. basic | complete You can find all the templates in the donkeycar/donkeycar/templates folder","title":"Install Donkeycar on Windows (WSL)"},{"location":"guide/host_pc/setup_windows/#next-lets-install-software-on-donkeycar_2","text":"","title":"Next let's install software on Donkeycar"},{"location":"guide/robot_sbc/intel_t265/","text":"A Guide to using the Intel Realsense T265 sensor with Donkeycar Note Although the Realsense T265 can be used with a Nvidia Jetson Nano, it's a bit easier to set up with a Raspberry Pi (we recommend the RPi 4, with at least 4GB memory). Also, the Intel Realsense D4XX series can also be used with Donkeycar as a regular camera (with the use of its depth sensing data coming soon), and we'll add instructions for that when it's ready. Original T265 path follower code by Tawn Kramer Step 1: Setup Librealsense on Ubuntu Machine Using the latest version of Raspian (tested with Raspian Buster) on the RPi, follow these instructions to set up Intel's Realsense libraries (Librealsense) and dependencies. Step 1: Setup Donkeycar Follow the standard instructions here Step 3: Run the Donkeycar path follower app After you\u2019ve done that, set up the directory with this: ```donkey createcar --path ~/follow --template path_follower Running cd ~/follow python3 manage.py drive Once it\u2019s running, open a browser on your laptop and enter this in the URL bar: http:// :8890 The rest of the instructions from Tawn\u2019s repo: When you drive, this will draw a red line for the path, a green circle for the robot location. 1) Mark a nice starting spot for your robot. Be sure to put it right back there each time you start. 2) Drive the car in some kind of loop. You see the red line show the path. 3) Hit X on the PS3/4 controller to save the path. 4) Put the bot back at the start spot. 5) Then hit the \u201cselect\u201d button (on a PS3 controller) or \u201cshare\u201d (on a PS4 controller) twice to go to pilot mode. This will start driving on the path. If you want it go faster or slower, change this line in the myconfig.py file: THROTTLE_FORWARD_PWM = 530 Check the bottom of myconfig.py for some settings to tweak. PID values, map offsets and scale. things like that. You might want to start by downloading and using the myconfig.py file from my repo, which has some known-good settings and is otherwise a good place to start. Some tips: When you start, the green dot will be in the top left corner of the box. You may prefer to have it in the center. If so, change PATH_OFFSET = (0, 0) in the myconfig.py file to PATH_OFFSET = (250, 250) For a small course, you may find that the path is too small to see well. In that case, change PATH_SCALE = 5.0 to PATH_SCALE = 10.0 (or more, if necessary) If you\u2019re not seeing the red line, that means that a path file has already been written. Delete \u201cdonkey_path.pkl\u201d (rm donkey_path.pkl) and the red line should show up When you're running in auto mode, the green dot will change to blue It defaults to recording a path point every 0.3 meters. If you want it to be smoother, you can change to a smaller number in myconfig.py with this line: PATH_MIN_DIST = 0.3","title":"A Guide to using the Intel Realsense T265 sensor with Donkeycar"},{"location":"guide/robot_sbc/intel_t265/#a-guide-to-using-the-intel-realsense-t265-sensor-with-donkeycar","text":"Note Although the Realsense T265 can be used with a Nvidia Jetson Nano, it's a bit easier to set up with a Raspberry Pi (we recommend the RPi 4, with at least 4GB memory). Also, the Intel Realsense D4XX series can also be used with Donkeycar as a regular camera (with the use of its depth sensing data coming soon), and we'll add instructions for that when it's ready.","title":"A Guide to using the Intel Realsense T265 sensor with Donkeycar"},{"location":"guide/robot_sbc/intel_t265/#original-t265-path-follower-code-by-tawn-kramer","text":"","title":"Original T265 path follower code by Tawn Kramer"},{"location":"guide/robot_sbc/intel_t265/#step-1-setup-librealsense-on-ubuntu-machine","text":"Using the latest version of Raspian (tested with Raspian Buster) on the RPi, follow these instructions to set up Intel's Realsense libraries (Librealsense) and dependencies.","title":"Step 1: Setup Librealsense on Ubuntu Machine"},{"location":"guide/robot_sbc/intel_t265/#step-1-setup-donkeycar","text":"Follow the standard instructions here","title":"Step 1: Setup Donkeycar"},{"location":"guide/robot_sbc/intel_t265/#step-3-run-the-donkeycar-path-follower-app","text":"After you\u2019ve done that, set up the directory with this: ```donkey createcar --path ~/follow --template path_follower Running cd ~/follow python3 manage.py drive Once it\u2019s running, open a browser on your laptop and enter this in the URL bar: http:// :8890 The rest of the instructions from Tawn\u2019s repo: When you drive, this will draw a red line for the path, a green circle for the robot location. 1) Mark a nice starting spot for your robot. Be sure to put it right back there each time you start. 2) Drive the car in some kind of loop. You see the red line show the path. 3) Hit X on the PS3/4 controller to save the path. 4) Put the bot back at the start spot. 5) Then hit the \u201cselect\u201d button (on a PS3 controller) or \u201cshare\u201d (on a PS4 controller) twice to go to pilot mode. This will start driving on the path. If you want it go faster or slower, change this line in the myconfig.py file: THROTTLE_FORWARD_PWM = 530 Check the bottom of myconfig.py for some settings to tweak. PID values, map offsets and scale. things like that. You might want to start by downloading and using the myconfig.py file from my repo, which has some known-good settings and is otherwise a good place to start. Some tips: When you start, the green dot will be in the top left corner of the box. You may prefer to have it in the center. If so, change PATH_OFFSET = (0, 0) in the myconfig.py file to PATH_OFFSET = (250, 250) For a small course, you may find that the path is too small to see well. In that case, change PATH_SCALE = 5.0 to PATH_SCALE = 10.0 (or more, if necessary) If you\u2019re not seeing the red line, that means that a path file has already been written. Delete \u201cdonkey_path.pkl\u201d (rm donkey_path.pkl) and the red line should show up When you're running in auto mode, the green dot will change to blue It defaults to recording a path point every 0.3 meters. If you want it to be smoother, you can change to a smaller number in myconfig.py with this line: PATH_MIN_DIST = 0.3","title":"Step 3: Run the Donkeycar path follower app"},{"location":"guide/robot_sbc/setup_jetson_nano/","text":"Get Your Jetson Nano Working Step 1: Flash Operating System Step 2: Install Dependencies Step 3: Setup Virtual Env Step 4: Install Donkeycar Python Code Then Create your Donkeycar Application Step 1: Flash Operating System Visit the official Nvidia Jetson Nano Getting Started Guide . Work through the Prepare for Setup , Writing Image to the microSD Card , and Setup and First Boot instructions, then return here. Step 2: Install Dependencies ssh into your vehicle. Use the the terminal for Ubuntu or Mac. Putty for windows. Note: you can either proceed with this tutorial, or if you have Jetpack 4.4 installed, you can use a script to automate the setup. The script is located in donkeycar/install/nano/install-jp44.sh . You will need to edit line #3 of the script and replace the default password with your password. This script will install all Git repositories into a ~/projects directory. If you wish to use a different directory, you will need to change this as well (replace all instances of ~/projects with your desired folder path). First install some packages with apt-get . sudo apt-get update -y sudo apt-get upgrade -y sudo apt-get install -y libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran sudo apt-get install -y python3-dev python3-pip sudo apt-get install -y libxslt1-dev libxml2-dev libffi-dev libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev sudo apt-get install -y git sudo apt-get install -y openmpi-doc openmpi-bin libopenmpi-dev libopenblas-dev Next, you will need to install packages with pip : sudo -H pip3 install -U pip testresources setuptools sudo -H pip3 install -U futures==3.1.1 protobuf==3.12.2 pybind11==2.5.0 sudo -H pip3 install -U cython==0.29.21 sudo -H pip3 install -U numpy==1.19.0 sudo -H pip3 install -U future==0.18.2 mock==4.0.2 h5py==2.10.0 keras_preprocessing==1.1.2 keras_applications==1.0.8 gast==0.3.3 sudo -H pip3 install -U grpcio==1.30.0 absl-py==0.9.0 py-cpuinfo==7.0.0 psutil==5.7.2 portpicker==1.3.1 six requests==2.24.0 astor==0.8.1 termcolor==1.1.0 wrapt==1.12.1 google-pasta==0.2.0 sudo -H pip3 install -U scipy==1.4.1 sudo -H pip3 install -U pandas==1.0.5 sudo -H pip3 install -U gdown # This will install tensorflow as a system package sudo -H pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v44 tensorflow==2.2.0+nv20.6 Finally, you can install PyTorch: # Install PyTorch v1.7 - torchvision v0.8.1 wget https://nvidia.box.com/shared/static/wa34qwrwtk9njtyarwt5nvo6imenfy26.whl -O torch-1.7.0-cp36-cp36m-linux_aarch64.whl sudo -H pip3 install ./torch-1.7.0-cp36-cp36m-linux_aarch64.whl # Install PyTorch Vision sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev # You can replace the following line with wherever you want to store your Git repositories mkdir -p ~/projects; cd ~/projects git clone --branch v0.8.1 https://github.com/pytorch/vision torchvision cd torchvision export BUILD_VERSION=0.8.1 sudo python3 setup.py install Note: If you get errors in the above step, there may be a version mismatch with the version of Nvidia's Jetpack that you're running. If so, please install PyTorch directly from Nvidia's site here . Optionally, you can install RPi.GPIO clone for Jetson Nano from here . This is not required for default setup, but can be useful if using LED or other GPIO driven devices. Step 3: Setup Virtual Env pip3 install virtualenv sudo python3 -m virtualenv -p python3 env --system-site-packages echo \"source /env/bin/activate\" >> ~/.bashrc source ~/.bashrc Step 4: Install Donkeycar Python Code Change to a dir you would like to use as the head of your projects. mkdir -p ~/projects; cd ~/projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master pip install -e .[nano] Step 5: Install PyGame (Optional) If you plan to use a USB camera, you will also want to setup pygame: sudo apt-get install python-dev libsdl1.2-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsdl1.2-dev libsmpeg-dev python-numpy subversion libportmidi-dev ffmpeg libswscale-dev libavformat-dev libavcodec-dev libfreetype6-dev pip install pygame Later on you can add the CAMERA_TYPE=\"WEBCAM\" in myconfig.py. Next, create your Donkeycar application .","title":"Get Your Jetson Nano Working"},{"location":"guide/robot_sbc/setup_jetson_nano/#get-your-jetson-nano-working","text":"Step 1: Flash Operating System Step 2: Install Dependencies Step 3: Setup Virtual Env Step 4: Install Donkeycar Python Code Then Create your Donkeycar Application","title":"Get Your Jetson Nano Working"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-1-flash-operating-system","text":"Visit the official Nvidia Jetson Nano Getting Started Guide . Work through the Prepare for Setup , Writing Image to the microSD Card , and Setup and First Boot instructions, then return here.","title":"Step 1: Flash Operating System"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-2-install-dependencies","text":"ssh into your vehicle. Use the the terminal for Ubuntu or Mac. Putty for windows. Note: you can either proceed with this tutorial, or if you have Jetpack 4.4 installed, you can use a script to automate the setup. The script is located in donkeycar/install/nano/install-jp44.sh . You will need to edit line #3 of the script and replace the default password with your password. This script will install all Git repositories into a ~/projects directory. If you wish to use a different directory, you will need to change this as well (replace all instances of ~/projects with your desired folder path). First install some packages with apt-get . sudo apt-get update -y sudo apt-get upgrade -y sudo apt-get install -y libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran sudo apt-get install -y python3-dev python3-pip sudo apt-get install -y libxslt1-dev libxml2-dev libffi-dev libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev sudo apt-get install -y git sudo apt-get install -y openmpi-doc openmpi-bin libopenmpi-dev libopenblas-dev Next, you will need to install packages with pip : sudo -H pip3 install -U pip testresources setuptools sudo -H pip3 install -U futures==3.1.1 protobuf==3.12.2 pybind11==2.5.0 sudo -H pip3 install -U cython==0.29.21 sudo -H pip3 install -U numpy==1.19.0 sudo -H pip3 install -U future==0.18.2 mock==4.0.2 h5py==2.10.0 keras_preprocessing==1.1.2 keras_applications==1.0.8 gast==0.3.3 sudo -H pip3 install -U grpcio==1.30.0 absl-py==0.9.0 py-cpuinfo==7.0.0 psutil==5.7.2 portpicker==1.3.1 six requests==2.24.0 astor==0.8.1 termcolor==1.1.0 wrapt==1.12.1 google-pasta==0.2.0 sudo -H pip3 install -U scipy==1.4.1 sudo -H pip3 install -U pandas==1.0.5 sudo -H pip3 install -U gdown # This will install tensorflow as a system package sudo -H pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v44 tensorflow==2.2.0+nv20.6 Finally, you can install PyTorch: # Install PyTorch v1.7 - torchvision v0.8.1 wget https://nvidia.box.com/shared/static/wa34qwrwtk9njtyarwt5nvo6imenfy26.whl -O torch-1.7.0-cp36-cp36m-linux_aarch64.whl sudo -H pip3 install ./torch-1.7.0-cp36-cp36m-linux_aarch64.whl # Install PyTorch Vision sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev # You can replace the following line with wherever you want to store your Git repositories mkdir -p ~/projects; cd ~/projects git clone --branch v0.8.1 https://github.com/pytorch/vision torchvision cd torchvision export BUILD_VERSION=0.8.1 sudo python3 setup.py install Note: If you get errors in the above step, there may be a version mismatch with the version of Nvidia's Jetpack that you're running. If so, please install PyTorch directly from Nvidia's site here . Optionally, you can install RPi.GPIO clone for Jetson Nano from here . This is not required for default setup, but can be useful if using LED or other GPIO driven devices.","title":"Step 2: Install Dependencies"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-3-setup-virtual-env","text":"pip3 install virtualenv sudo python3 -m virtualenv -p python3 env --system-site-packages echo \"source /env/bin/activate\" >> ~/.bashrc source ~/.bashrc","title":"Step 3: Setup Virtual Env"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-4-install-donkeycar-python-code","text":"Change to a dir you would like to use as the head of your projects. mkdir -p ~/projects; cd ~/projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master pip install -e .[nano]","title":"Step 4: Install Donkeycar Python Code"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-5-install-pygame-optional","text":"If you plan to use a USB camera, you will also want to setup pygame: sudo apt-get install python-dev libsdl1.2-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsdl1.2-dev libsmpeg-dev python-numpy subversion libportmidi-dev ffmpeg libswscale-dev libavformat-dev libavcodec-dev libfreetype6-dev pip install pygame Later on you can add the CAMERA_TYPE=\"WEBCAM\" in myconfig.py.","title":"Step 5: Install PyGame (Optional)"},{"location":"guide/robot_sbc/setup_jetson_nano/#next-create-your-donkeycar-application","text":"","title":"Next, create your Donkeycar application."},{"location":"guide/robot_sbc/setup_raspberry_pi/","text":"Get Your Raspberry Pi Working Step 1: Flash Operating System Step 2: Setup the WiFi for First Boot Step 3: Setup Pi's Hostname Step 4: Enable SSH on Boot Step 5: Connecting to the Pi Step 6: Update and Upgrade Step 7: Raspi-config Step 8: Install Dependencies Step 9: Install Optional OpenCV Dependencies Step 10: Setup Virtual Env Step 11: Install Donkeycar Python Code Step 12: Install Optional OpenCV Then Create your Donkeycar Application Step 1: Flash Operating System Note: If you plan to use the mobile app, consider using the pre-built image. Refer to the mobile app user guide for details. You need to flash a micro SD image with an operating system. Download Latest Raspian(Buster) . Follow OS specific guides here . Leave micro SD card in your machine and edit/create some files as below: Step 2: Setup the WiFi for first boot We can create a special file which will be used to login to wifi on first boot. More reading here , but we will walk you through it. On Windows, with your memory card image burned and memory disc still inserted, you should see two drives, which are actually two partitions on the mem disc. One is labeled boot . On Mac and Linux, you should also have access to the boot partition of the mem disc. This is formatted with the common FAT type and is where we will edit some files to help it find and log-on to your wifi on its first boot. Note: If boot is not visible right away, try unplugging and re-inserting the memory card reader. Start a text editor: gedit on Linux. Notepad++ on Windows. TextEdit on a Mac. Possible country codes to use can be found here Paste and edit this contents to match your wifi, adjust as needed: country=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"<your network name>\" psk=\"<your password>\" } Note - country defines allowed wifi channels, ensure to set it properly to your location and hardware. Replace <your network name> with the ID of your network. Leave the quotes. I've seen problems when the network name contained an apostrophe, like \"Joe's iPhone\". Replace <your password> with your password, leaving it surrounded by quotes. If it bothers you to leave your password unencrypted, you may change the contents later once you've gotten the pi to boot and log-in. Save this file to the root of boot partition with the filename wpa_supplicant.conf . On first boot, this file will be moved to /etc/wpa_supplicant/wpa_supplicant.conf where it may be edited later. If you are using Notepad on Windows, make sure it doesn't have a .txt at the end. Step 3: Setup Pi's Hostname Note: This step only possible on a Linux host pc. Otherwise you can set it up later in raspi-config after logging in to your pi. We can also setup the hostname so that your Pi easier to find once on the network. If yours is the only Pi on the network, then you can find it with ping raspberrypi.local once it's booted. If there are many other Pi's on the network, then this will have problems. If you are on a Linux machine, or are able to edit the UUID partition, then you can edit the /etc/hostname and /etc/hosts files now to make finding your pi on the network easier after boot. Edit those to replace raspberrypi with a name of your choosing. Use all lower case, no special characters, no hyphens, yes underscores _ . Good idea is to use something like pi-<MAC_ADDRESS> such as pi-deadbeef especially if you have more pi devices in the same network. sudo vi /media/userID/UUID/etc/hostname sudo vi /media/userID/UUID/etc/hosts Step 4: Enable SSH on Boot Put a file named ssh in the root of your boot partition. Now your SD card is ready. Eject it from your computer - wait until system shows the writing is done and it is safe to remove card. Ensure Pi is turned off, put the card in the Pi and power on the Pi. Step 5: Connecting to the Pi If you followed the above instructions to add wifi access, your Pi should now be connected to your wifi network. Now you need to find its IP address so you can connect to it via SSH. The easiest way (on Ubuntu) is to use the findcar donkey command. You can try ping raspberrypi.local . If you've modified the hostname, then you should try: ping <your hostname>.local This will fail on a windows machine. Windows users will need the full IP address (unless using cygwin). If you are having troubles locating your Pi on the network, you will want to plug in an HDMI monitor and USB keyboard into the Pi. Boot it. Login with: Username: pi Password: raspberry Then try the command: ifconfig wlan0 or just all Ip addresses assigned to the pi (wifi or cable): ip -br a If this has a valid IPv4 address, 4 groups of numbers separated by dots, then you can try that with your SSH command. If you don't see anything like that, then your wifi config might have a mistake. You can try to fix with sudo nano /etc/wpa_supplicant/wpa_supplicant.conf If you don't have a HDMI monitor and keyboard, you can plug-in the Pi with a CAT5 cable to a router with DHCP. If that router is on the same network as your PC, you can try: ping raspberrypi.local Hopefully, one of those methods worked and you are now ready to SSH into your Pi. On Mac and Linux, you can open Terminal. On Windows you can install Putty , one of the alternatives , or on Windows 10 you may have ssh via the command prompt. If you have a command prompt, you can try: ssh pi@raspberrypi.local or ssh pi@<your pi ip address> or via Putty. Username: pi Password: raspberry Hostname: <your pi IP address> Step 6: Update and Upgrade sudo apt-get update sudo apt-get upgrade Step 7: Raspi-config sudo raspi-config change default password for pi change hostname enable Interfacing Options - I2C enable Interfacing Options - Camera select Advanced Options - Expand Filesystem so you can use your whole sd-card storage Choose <Finish> and hit enter. Note: Reboot after changing these settings. Should happen if you select yes . Step 8: Install Dependencies sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev git ntp Step 9: Optional - Install OpenCV Dependencies If you are going for a minimal install, you can get by without these. But it can be handy to have OpenCV. sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test Step 10: Setup Virtual Env This needs to be done only once: python3 -m virtualenv -p python3 env --system-site-packages echo \"source env/bin/activate\" >> ~/.bashrc source ~/.bashrc Modifying your .bashrc in this way will automatically enable this environment each time you login. To return to the system python you can type deactivate . Step 11: Install Donkeycar Python Code Create and change to a directory you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master pip install -e .[pi] pip install numpy --upgrade wget \"https://raw.githubusercontent.com/PINTO0309/Tensorflow-bin/master/tensorflow-2.3.1-cp37-none-linux_armv7l_download.sh\" chmod u+x tensorflow-2.3.1-cp37-none-linux_armv7l_download.sh ./tensorflow-2.3.1-cp37-none-linux_armv7l_download.sh pip install tensorflow-2.3.1-cp37-none-linux_armv7l.whl You can validate your tensorflow install with python -c \"import tensorflow\" Step 12: Optional - Install OpenCV If you've opted to install the OpenCV dependencies earlier, you can install Python OpenCV bindings now with command: sudo apt install python3-opencv If that failed, you can try pip: pip install opencv-python Then test to see if import succeeds. python -c \"import cv2\" And if no errors, you have OpenCV installed! Next, create your Donkeycar application .","title":"Get Your Raspberry Pi Working"},{"location":"guide/robot_sbc/setup_raspberry_pi/#get-your-raspberry-pi-working","text":"Step 1: Flash Operating System Step 2: Setup the WiFi for First Boot Step 3: Setup Pi's Hostname Step 4: Enable SSH on Boot Step 5: Connecting to the Pi Step 6: Update and Upgrade Step 7: Raspi-config Step 8: Install Dependencies Step 9: Install Optional OpenCV Dependencies Step 10: Setup Virtual Env Step 11: Install Donkeycar Python Code Step 12: Install Optional OpenCV Then Create your Donkeycar Application","title":"Get Your Raspberry Pi Working"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-1-flash-operating-system","text":"Note: If you plan to use the mobile app, consider using the pre-built image. Refer to the mobile app user guide for details. You need to flash a micro SD image with an operating system. Download Latest Raspian(Buster) . Follow OS specific guides here . Leave micro SD card in your machine and edit/create some files as below:","title":"Step 1: Flash Operating System"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-2-setup-the-wifi-for-first-boot","text":"We can create a special file which will be used to login to wifi on first boot. More reading here , but we will walk you through it. On Windows, with your memory card image burned and memory disc still inserted, you should see two drives, which are actually two partitions on the mem disc. One is labeled boot . On Mac and Linux, you should also have access to the boot partition of the mem disc. This is formatted with the common FAT type and is where we will edit some files to help it find and log-on to your wifi on its first boot. Note: If boot is not visible right away, try unplugging and re-inserting the memory card reader. Start a text editor: gedit on Linux. Notepad++ on Windows. TextEdit on a Mac. Possible country codes to use can be found here Paste and edit this contents to match your wifi, adjust as needed: country=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"<your network name>\" psk=\"<your password>\" } Note - country defines allowed wifi channels, ensure to set it properly to your location and hardware. Replace <your network name> with the ID of your network. Leave the quotes. I've seen problems when the network name contained an apostrophe, like \"Joe's iPhone\". Replace <your password> with your password, leaving it surrounded by quotes. If it bothers you to leave your password unencrypted, you may change the contents later once you've gotten the pi to boot and log-in. Save this file to the root of boot partition with the filename wpa_supplicant.conf . On first boot, this file will be moved to /etc/wpa_supplicant/wpa_supplicant.conf where it may be edited later. If you are using Notepad on Windows, make sure it doesn't have a .txt at the end.","title":"Step 2: Setup the WiFi for first boot"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-3-setup-pis-hostname","text":"Note: This step only possible on a Linux host pc. Otherwise you can set it up later in raspi-config after logging in to your pi. We can also setup the hostname so that your Pi easier to find once on the network. If yours is the only Pi on the network, then you can find it with ping raspberrypi.local once it's booted. If there are many other Pi's on the network, then this will have problems. If you are on a Linux machine, or are able to edit the UUID partition, then you can edit the /etc/hostname and /etc/hosts files now to make finding your pi on the network easier after boot. Edit those to replace raspberrypi with a name of your choosing. Use all lower case, no special characters, no hyphens, yes underscores _ . Good idea is to use something like pi-<MAC_ADDRESS> such as pi-deadbeef especially if you have more pi devices in the same network. sudo vi /media/userID/UUID/etc/hostname sudo vi /media/userID/UUID/etc/hosts","title":"Step 3: Setup Pi's Hostname"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-4-enable-ssh-on-boot","text":"Put a file named ssh in the root of your boot partition. Now your SD card is ready. Eject it from your computer - wait until system shows the writing is done and it is safe to remove card. Ensure Pi is turned off, put the card in the Pi and power on the Pi.","title":"Step 4: Enable SSH on Boot"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-5-connecting-to-the-pi","text":"If you followed the above instructions to add wifi access, your Pi should now be connected to your wifi network. Now you need to find its IP address so you can connect to it via SSH. The easiest way (on Ubuntu) is to use the findcar donkey command. You can try ping raspberrypi.local . If you've modified the hostname, then you should try: ping <your hostname>.local This will fail on a windows machine. Windows users will need the full IP address (unless using cygwin). If you are having troubles locating your Pi on the network, you will want to plug in an HDMI monitor and USB keyboard into the Pi. Boot it. Login with: Username: pi Password: raspberry Then try the command: ifconfig wlan0 or just all Ip addresses assigned to the pi (wifi or cable): ip -br a If this has a valid IPv4 address, 4 groups of numbers separated by dots, then you can try that with your SSH command. If you don't see anything like that, then your wifi config might have a mistake. You can try to fix with sudo nano /etc/wpa_supplicant/wpa_supplicant.conf If you don't have a HDMI monitor and keyboard, you can plug-in the Pi with a CAT5 cable to a router with DHCP. If that router is on the same network as your PC, you can try: ping raspberrypi.local Hopefully, one of those methods worked and you are now ready to SSH into your Pi. On Mac and Linux, you can open Terminal. On Windows you can install Putty , one of the alternatives , or on Windows 10 you may have ssh via the command prompt. If you have a command prompt, you can try: ssh pi@raspberrypi.local or ssh pi@<your pi ip address> or via Putty. Username: pi Password: raspberry Hostname: <your pi IP address>","title":"Step 5: Connecting to the Pi"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-6-update-and-upgrade","text":"sudo apt-get update sudo apt-get upgrade","title":"Step 6: Update and Upgrade"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-7-raspi-config","text":"sudo raspi-config change default password for pi change hostname enable Interfacing Options - I2C enable Interfacing Options - Camera select Advanced Options - Expand Filesystem so you can use your whole sd-card storage Choose <Finish> and hit enter. Note: Reboot after changing these settings. Should happen if you select yes .","title":"Step 7: Raspi-config"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-8-install-dependencies","text":"sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev git ntp","title":"Step 8: Install Dependencies"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-9-optional-install-opencv-dependencies","text":"If you are going for a minimal install, you can get by without these. But it can be handy to have OpenCV. sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test","title":"Step 9: Optional - Install OpenCV Dependencies"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-10-setup-virtual-env","text":"This needs to be done only once: python3 -m virtualenv -p python3 env --system-site-packages echo \"source env/bin/activate\" >> ~/.bashrc source ~/.bashrc Modifying your .bashrc in this way will automatically enable this environment each time you login. To return to the system python you can type deactivate .","title":"Step 10: Setup Virtual Env"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-11-install-donkeycar-python-code","text":"Create and change to a directory you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master pip install -e .[pi] pip install numpy --upgrade wget \"https://raw.githubusercontent.com/PINTO0309/Tensorflow-bin/master/tensorflow-2.3.1-cp37-none-linux_armv7l_download.sh\" chmod u+x tensorflow-2.3.1-cp37-none-linux_armv7l_download.sh ./tensorflow-2.3.1-cp37-none-linux_armv7l_download.sh pip install tensorflow-2.3.1-cp37-none-linux_armv7l.whl You can validate your tensorflow install with python -c \"import tensorflow\"","title":"Step 11: Install Donkeycar Python Code"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-12-optional-install-opencv","text":"If you've opted to install the OpenCV dependencies earlier, you can install Python OpenCV bindings now with command: sudo apt install python3-opencv If that failed, you can try pip: pip install opencv-python Then test to see if import succeeds. python -c \"import cv2\" And if no errors, you have OpenCV installed!","title":"Step 12: Optional - Install OpenCV"},{"location":"guide/robot_sbc/setup_raspberry_pi/#next-create-your-donkeycar-application","text":"","title":"Next, create your Donkeycar application."},{"location":"guide/robot_sbc/tensorrt_jetson_nano/","text":"A Guide to using TensorRT on the Nvidia Jetson Nano Note This guide assumes that you are using Ubuntu 18.04 . If you are using Windows refer to these instructions on how to setup your computer to use TensorRT. Step 1: Setup TensorRT on Ubuntu Machine Follow the instructions here . Make sure you use the tar file instructions unless you have previously installed CUDA using .deb files. Step 2: Setup TensorRT on your Jetson Nano Setup some environment variables so nvcc is on $PATH . Add the following lines to your ~/.bashrc file. # Add this to your .bashrc file export CUDA_HOME=/usr/local/cuda # Adds the CUDA compiler to the PATH export PATH=$CUDA_HOME/bin:$PATH # Adds the libraries export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH Test the changes to your .bashrc . source ~/.bashrc nvcc --version You should see something like: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2018 NVIDIA Corporation Built on ... Cuda compilation tools, release 10.0, Vxxxxx Switch to your virtualenv and install PyCUDA. # This takes a a while.` pip install pycuda After this you will also need to setup PYTHONPATH such that your dist-packages are included as part of your virtualenv . Add this to your .bashrc . This needs to be done because the python bindings to tensorrt are available in dist-packages and this folder is usually not visible to your virtualenv. To make them visible we add it to PYTHONPATH . export PYTHONPATH=/usr/lib/python3.6/dist-packages:$PYTHONPATH Test this change by switching to your virtualenv and importing tensorrt . > import tensorrt as trt > # This import should succeed Step 3: Train, Freeze and Export your model to TensorRT format ( uff ) After you train the linear model you end up with a file with a .h5 extension. # You end up with a Linear.h5 in the models folder python manage.py train --model=./models/Linear.h5 --tub=./data/tub_1_19-06-29,... # (optional) copy './models/Linear.h5' from your desktop computer to your Jetson Nano in your working dir (~mycar/models/) # Freeze model using freeze_model.py in donkeycar/scripts ; the frozen model is stored as protocol buffers. # This command also exports some metadata about the model which is saved in ./models/Linear.metadata python ~/projects/donkeycar/scripts/freeze_model.py --model=~/mycar/models/Linear.h5 --output=~/mycar/models/Linear.pb # Convert the frozen model to UFF. The command below creates a file ./models/Linear.uff cd /usr/lib/python3.6/dist-packages/uff/bin/ python convert_to_uff.py ~/mycar/models/Linear.pb Now copy the converted uff model and the metadata to your Jetson Nano. Step 4 In myconfig.py pick the model type as tensorrt_linear . DEFAULT_MODEL_TYPE = `tensorrt_linear` Finally you can do # After you scp your `uff` model to the Nano python manage.py drive --model=./models/Linear.uff","title":"A Guide to using TensorRT on the Nvidia Jetson Nano"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#a-guide-to-using-tensorrt-on-the-nvidia-jetson-nano","text":"Note This guide assumes that you are using Ubuntu 18.04 . If you are using Windows refer to these instructions on how to setup your computer to use TensorRT.","title":"A Guide to using TensorRT on the Nvidia Jetson Nano"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#step-1-setup-tensorrt-on-ubuntu-machine","text":"Follow the instructions here . Make sure you use the tar file instructions unless you have previously installed CUDA using .deb files.","title":"Step 1: Setup TensorRT on Ubuntu Machine"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#step-2-setup-tensorrt-on-your-jetson-nano","text":"Setup some environment variables so nvcc is on $PATH . Add the following lines to your ~/.bashrc file. # Add this to your .bashrc file export CUDA_HOME=/usr/local/cuda # Adds the CUDA compiler to the PATH export PATH=$CUDA_HOME/bin:$PATH # Adds the libraries export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH Test the changes to your .bashrc . source ~/.bashrc nvcc --version You should see something like: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2018 NVIDIA Corporation Built on ... Cuda compilation tools, release 10.0, Vxxxxx Switch to your virtualenv and install PyCUDA. # This takes a a while.` pip install pycuda After this you will also need to setup PYTHONPATH such that your dist-packages are included as part of your virtualenv . Add this to your .bashrc . This needs to be done because the python bindings to tensorrt are available in dist-packages and this folder is usually not visible to your virtualenv. To make them visible we add it to PYTHONPATH . export PYTHONPATH=/usr/lib/python3.6/dist-packages:$PYTHONPATH Test this change by switching to your virtualenv and importing tensorrt . > import tensorrt as trt > # This import should succeed","title":"Step 2: Setup TensorRT on your Jetson Nano"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#step-3-train-freeze-and-export-your-model-to-tensorrt-format-uff","text":"After you train the linear model you end up with a file with a .h5 extension. # You end up with a Linear.h5 in the models folder python manage.py train --model=./models/Linear.h5 --tub=./data/tub_1_19-06-29,... # (optional) copy './models/Linear.h5' from your desktop computer to your Jetson Nano in your working dir (~mycar/models/) # Freeze model using freeze_model.py in donkeycar/scripts ; the frozen model is stored as protocol buffers. # This command also exports some metadata about the model which is saved in ./models/Linear.metadata python ~/projects/donkeycar/scripts/freeze_model.py --model=~/mycar/models/Linear.h5 --output=~/mycar/models/Linear.pb # Convert the frozen model to UFF. The command below creates a file ./models/Linear.uff cd /usr/lib/python3.6/dist-packages/uff/bin/ python convert_to_uff.py ~/mycar/models/Linear.pb Now copy the converted uff model and the metadata to your Jetson Nano.","title":"Step 3: Train, Freeze and Export your model to TensorRT format (uff)"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#step-4","text":"In myconfig.py pick the model type as tensorrt_linear . DEFAULT_MODEL_TYPE = `tensorrt_linear` Finally you can do # After you scp your `uff` model to the Nano python manage.py drive --model=./models/Linear.uff","title":"Step 4"},{"location":"parts/about/","text":"What is a Part A part is a Python class that wraps a functional component of a vehicle. These include: Sensors - Cameras, Lidar, Odometers, GPS ... Actuators - Motor Controllers Pilots - Lane Detectors, Behavioral Cloning models, ... Controllers - Web based or Bluetooth. Stores - Tub, or a way to save data. This is an excellent video by Tawn Kramer that walks through how to make a part: https://www.youtube.com/watch?v=YZ4ESrtfShs Here is an example how to use the PiCamera part to publish an image in the 'cam/img' channel on every drive loop. V = dk.Vehicle() #initialize the camera part cam = PiCamera() #add the part to the vehicle. V.add(cam, outputs=['cam/img']) V.start() Anatomy of a Part All parts share a common structure so that they can all be run by the vehicle's drive loop. A part must have either an \"run\" or a \"run_threaded\" function that does the work (see below). It may also require inputs=['in single quotes', 'seperated by commas'] and will always generate at least one outputs=['in single quotes', 'seperated by commas'] If the part grabs some hardware resource, such as a camera or a serial port, it should also have a shutdown function that releases those resources properly when donkey is stopped. Here is an example of a part that will accept a number, multiply it by a random number and return the result. import random class RandPercent: def run(self, x): return x * random.random() Now to add this to a vehicle: V = dk.Vehicle() # initialize the channel value V.mem['const'] = 4 # add the part to read and write to the same channel. V.add(RandPercent, inputs=['const'], outputs=['const']) V.start(max_loops=5) Threaded Parts For a vehicle to perform well the drive loop must execute 10-30 times per second (determined by the DRIVE_LOOP_HZ setting in your config file; the default is 20hz) overall, so slow parts should be threaded to avoid holding up the drive loop. A threaded part needs to define the function that runs in the separate thread and the function to call that will return the most recent values quickly. When you add threaded = True when adding a part, the main Donkey program will call the part's \"run_threaded\" function instead of the \"run\" function. So for the below example you would add the part with this: V.add(RandPercent, inputs=['const'], outputs=['const'], threaded=True) Once you have a \"run_threaded\" function, Donkey will automatically look for an \"update\" function and run that in its own thread, as you can see below. Here's an example how to make the RandPercent part threaded if the run function takes a second to complete. import random import time class RandPercent: self.in = 0.0 self.out = 0.0 def run(self, x): return x * random.random() time.sleep(1) def update(self): # the function run in its own thread while True: self.out = self.run(self.in) def run_threaded(self, x): self.in = x return self.out part.run : function used to run the part part.run_threaded : drive loop function run if part is threaded. part.update : threaded function part.shutdown","title":"About"},{"location":"parts/about/#what-is-a-part","text":"A part is a Python class that wraps a functional component of a vehicle. These include: Sensors - Cameras, Lidar, Odometers, GPS ... Actuators - Motor Controllers Pilots - Lane Detectors, Behavioral Cloning models, ... Controllers - Web based or Bluetooth. Stores - Tub, or a way to save data. This is an excellent video by Tawn Kramer that walks through how to make a part: https://www.youtube.com/watch?v=YZ4ESrtfShs Here is an example how to use the PiCamera part to publish an image in the 'cam/img' channel on every drive loop. V = dk.Vehicle() #initialize the camera part cam = PiCamera() #add the part to the vehicle. V.add(cam, outputs=['cam/img']) V.start()","title":"What is a Part"},{"location":"parts/about/#anatomy-of-a-part","text":"All parts share a common structure so that they can all be run by the vehicle's drive loop. A part must have either an \"run\" or a \"run_threaded\" function that does the work (see below). It may also require inputs=['in single quotes', 'seperated by commas'] and will always generate at least one outputs=['in single quotes', 'seperated by commas'] If the part grabs some hardware resource, such as a camera or a serial port, it should also have a shutdown function that releases those resources properly when donkey is stopped. Here is an example of a part that will accept a number, multiply it by a random number and return the result. import random class RandPercent: def run(self, x): return x * random.random() Now to add this to a vehicle: V = dk.Vehicle() # initialize the channel value V.mem['const'] = 4 # add the part to read and write to the same channel. V.add(RandPercent, inputs=['const'], outputs=['const']) V.start(max_loops=5)","title":"Anatomy of a Part"},{"location":"parts/about/#threaded-parts","text":"For a vehicle to perform well the drive loop must execute 10-30 times per second (determined by the DRIVE_LOOP_HZ setting in your config file; the default is 20hz) overall, so slow parts should be threaded to avoid holding up the drive loop. A threaded part needs to define the function that runs in the separate thread and the function to call that will return the most recent values quickly. When you add threaded = True when adding a part, the main Donkey program will call the part's \"run_threaded\" function instead of the \"run\" function. So for the below example you would add the part with this: V.add(RandPercent, inputs=['const'], outputs=['const'], threaded=True) Once you have a \"run_threaded\" function, Donkey will automatically look for an \"update\" function and run that in its own thread, as you can see below. Here's an example how to make the RandPercent part threaded if the run function takes a second to complete. import random import time class RandPercent: self.in = 0.0 self.out = 0.0 def run(self, x): return x * random.random() time.sleep(1) def update(self): # the function run in its own thread while True: self.out = self.run(self.in) def run_threaded(self, x): self.in = x return self.out part.run : function used to run the part part.run_threaded : drive loop function run if part is threaded. part.update : threaded function part.shutdown","title":"Threaded Parts"},{"location":"parts/actuators/","text":"Acutators Oh noes, nothing in here except Arduino actuator ! This section needs expansion! Arduino Arduino can be used in the following fashion to generate PWM signals to control the steering and throttle. For now the Arduino mode is only tested on the Latte Panda Delta (LP-D) board. However it should be straigtforward to use it with Raspberry Pi / Jetson Nano (instead of PCA 9685). Refer to the below block diagram to understand where things fits in. Arduino board should be running the standard firmata sketch (This sketch comes by default when you download the arduino tool). Load the standard firmata sketch (from Examples > Firmata > StandardFirmata ) onto the Arduino. Further pymata_aio_ python package needs to be installed on the car computer via pip3 install pymata_aio . As shown in the block-diagram above LattePanda combines both the x86 CPU and the Connected Arduino into a single board. The following diagram shows how to connect the Arduino pins to steering servo and ESC. Note that the power for the servo is provided by the ESC battery elemininator circuit (BEC) which most ESC's provide. This is done to avoid supplying the entire servo power from Arduino's 5v. In large RC cars the servo can drag up to 2 amps, which lead to a destruction of the Arduino. Calibration Note that the calibration procedure/values are slightly different for the Arduino (than PCA9685). Note that 90 is the usual midpoint (i.e. 1.5 ms pulse width at 50 Hz), so it is recommended to start with 90 and adjust +/- 5 until you figure the desired range for steering / throttle. (env1) jithu@jithu-lp:~/master/pred_mt/lp/001/donkey$ donkey calibrate --arduino --channel 6 using donkey v2.6.0t ... pymata_aio Version 2.33 Copyright (c) 2015-2018 Alan Yorinks All rights reserved. Using COM Port:/dev/ttyACM0 Initializing Arduino - Please wait... Arduino Firmware ID: 2.5 StandardFirmata.ino Auto-discovery complete. Found 30 Digital Pins and 12 Analog Pins Enter a PWM setting to test(0-180)95 Enter a PWM setting to test(0-180)90 Enter a PWM setting to test(0-180)85 ... Note the --arduino switch passed to the calibrate command. Further note that the arduino pin being calibrated is passed via the --channel parameter. Using the arduino actuator part The following snippet illustrates how to exercise the Arduino actuator in the drive() loop: #Drive train setup arduino_controller = ArduinoFirmata( servo_pin=cfg.STEERING_ARDUINO_PIN, esc_pin=cfg.THROTTLE_ARDUINO_PIN) steering = ArdPWMSteering(controller=arduino_controller, left_pulse=cfg.STEERING_ARDUINO_LEFT_PWM, right_pulse=cfg.STEERING_ARDUINO_RIGHT_PWM) throttle = ArdPWMThrottle(controller=arduino_controller, max_pulse=cfg.THROTTLE_ARDUINO_FORWARD_PWM, zero_pulse=cfg.THROTTLE_ARDUINO_STOPPED_PWM, min_pulse=cfg.THROTTLE_ARDUINO_REVERSE_PWM) V.add(steering, inputs=['user/angle']) V.add(throttle, inputs=['user/throttle']) Refer to templates/arduino_drive.py for more details.","title":"Actuators"},{"location":"parts/actuators/#acutators","text":"Oh noes, nothing in here except Arduino actuator ! This section needs expansion!","title":"Acutators"},{"location":"parts/actuators/#arduino","text":"Arduino can be used in the following fashion to generate PWM signals to control the steering and throttle. For now the Arduino mode is only tested on the Latte Panda Delta (LP-D) board. However it should be straigtforward to use it with Raspberry Pi / Jetson Nano (instead of PCA 9685). Refer to the below block diagram to understand where things fits in. Arduino board should be running the standard firmata sketch (This sketch comes by default when you download the arduino tool). Load the standard firmata sketch (from Examples > Firmata > StandardFirmata ) onto the Arduino. Further pymata_aio_ python package needs to be installed on the car computer via pip3 install pymata_aio . As shown in the block-diagram above LattePanda combines both the x86 CPU and the Connected Arduino into a single board. The following diagram shows how to connect the Arduino pins to steering servo and ESC. Note that the power for the servo is provided by the ESC battery elemininator circuit (BEC) which most ESC's provide. This is done to avoid supplying the entire servo power from Arduino's 5v. In large RC cars the servo can drag up to 2 amps, which lead to a destruction of the Arduino.","title":"Arduino"},{"location":"parts/actuators/#calibration","text":"Note that the calibration procedure/values are slightly different for the Arduino (than PCA9685). Note that 90 is the usual midpoint (i.e. 1.5 ms pulse width at 50 Hz), so it is recommended to start with 90 and adjust +/- 5 until you figure the desired range for steering / throttle. (env1) jithu@jithu-lp:~/master/pred_mt/lp/001/donkey$ donkey calibrate --arduino --channel 6 using donkey v2.6.0t ... pymata_aio Version 2.33 Copyright (c) 2015-2018 Alan Yorinks All rights reserved. Using COM Port:/dev/ttyACM0 Initializing Arduino - Please wait... Arduino Firmware ID: 2.5 StandardFirmata.ino Auto-discovery complete. Found 30 Digital Pins and 12 Analog Pins Enter a PWM setting to test(0-180)95 Enter a PWM setting to test(0-180)90 Enter a PWM setting to test(0-180)85 ... Note the --arduino switch passed to the calibrate command. Further note that the arduino pin being calibrated is passed via the --channel parameter.","title":"Calibration"},{"location":"parts/actuators/#using-the-arduino-actuator-part","text":"The following snippet illustrates how to exercise the Arduino actuator in the drive() loop: #Drive train setup arduino_controller = ArduinoFirmata( servo_pin=cfg.STEERING_ARDUINO_PIN, esc_pin=cfg.THROTTLE_ARDUINO_PIN) steering = ArdPWMSteering(controller=arduino_controller, left_pulse=cfg.STEERING_ARDUINO_LEFT_PWM, right_pulse=cfg.STEERING_ARDUINO_RIGHT_PWM) throttle = ArdPWMThrottle(controller=arduino_controller, max_pulse=cfg.THROTTLE_ARDUINO_FORWARD_PWM, zero_pulse=cfg.THROTTLE_ARDUINO_STOPPED_PWM, min_pulse=cfg.THROTTLE_ARDUINO_REVERSE_PWM) V.add(steering, inputs=['user/angle']) V.add(throttle, inputs=['user/throttle']) Refer to templates/arduino_drive.py for more details.","title":"Using the arduino actuator part"},{"location":"parts/controllers/","text":"Controller Parts Web Controller The default controller to drive the car with your phone or browser. This has a web live preview of camera. Control options include: A virtual joystick The tilt, when using a mobile device with supported accelerometer A physical joystick using the web adapter. Support varies per browser, OS, and joystick combination. Keyboard input via the 'ikjl' keys. Note: Recently iOS has disabled default Safari access to motion control. Joystick Controller Many people find it easier to control the car using a game controller. There are several parts that provide this option. The default web controller may be replaced with a one line change to use a physical joystick part for input. This uses the OS device /dev/input/js0 by default. In theory, any joystick device that the OS mounts like this can be used. In practice, the behavior will change depending on the model of joystick ( Sony, or knockoff ), or XBox controller and the Bluetooth driver used to support it. The default code has been written and tested with a Sony brand PS3 Sixaxis controller . Other controllers may work, but will require alternative Bluetooth installs, and tweaks to the software for correct axis and buttons. These joysticks are known to work: Logitech Gamepad F710 Sony PS3 Sixaxis OEM (Not compatible with Jetson Nano) Sony PS4 Dualshock OEM WiiU Pro XBox Controller SteelSeries Nimbus (works only on TX2 jetpack 4.2+, may work on the Nano) These can be enabled by finding the CONTROLLER_TYPE in your myconfig.py and setting it to the correct string identifier ( after disabling the comment ). These can be used plugged in with a USB cable. It's been much more convenient to setup Bluetooth for a wireless control. There are controller specific setup details below. Note: If you have a controller that is not listed below, or you are having troubles getting your controller to work or you want to map your controller differently, see Creating a New or Custom Game Controller . Change myconfig.py or run with --js python manage.py drive --js Will enable driving with the joystick. This disables the live preview of the camera and the web page features. If you modify myconfig.py to make USE_JOYSTICK_AS_DEFAULT = True , then you do not need to run with the --js . PS3 Controller Bluetooth Setup Follow this guide . You can ignore steps past the 'Accessing the SixAxis from Python' section. I will include steps here in case the link becomes stale. sudo apt-get install bluetooth libbluetooth3 libusb-dev sudo systemctl enable bluetooth.service sudo usermod -G bluetooth -a pi Reboot after changing the user group. Plug in the PS3 with USB cable. Hit center PS logo button. Get and build the command line pairing tool. Run it: wget http://www.pabr.org/sixlinux/sixpair.c gcc -o sixpair sixpair.c -lusb sudo ./sixpair Use bluetoothctl to pair bluetoothctl agent on devices trust <MAC ADDRESS> default-agent quit Unplug USB cable. Hit center PS logo button. To test that the Bluetooth PS3 remote is working, verify that /dev/input/js0 exists: ls /dev/input/js0 Troubleshooting In case the BT connection on the Raspberry Pi does not work, you see might something like this in bluetoothctl : [NEW] Controller 00:11:22:33:44:55 super-donkey [default] [NEW] Device AA:BB:CC:DD:EE:FF PLAYSTATION(R)3 Controller [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes ... [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [bluetooth]# Try updating the Linux kernel and firmware by running: sudo rpi-update And then reboot: sudo reboot Charging PS3 Sixaxis Joystick For some reason, they don't like to charge in a powered USB port that doesn't have an active Bluetooth control and OS driver. This means a phone type USB charger will not work. Try a powered Linux or mac laptop USB port. You should see the lights blink after plugging in and hitting center PS logo. After charging, you will need to plug-in the controller again to the Pi, hit the PS logo, then unplug to pair again. New Battery for PS3 Sixaxis Joystick Sometimes these controllers can be quite old. Here's a link to a new battery . Be careful when taking off the cover. Remove 5 screws. There's a tab on the top half between the hand grips. You'll want to split/open it from the front and try pulling the bottom forward as you do, or you'll break the tab off as I did. PS3 Mouse problems on Linux Sometimes when you plug-in the PS3 joystick it starts taking over your mouse. If you want to prevent that, you can run this: xinput set-prop \"Sony PLAYSTATION(R)3 Controller\" \"Device Enabled\" 0 PS4 Controller The following instructions are based on RetroPie and ds4drv . Install ds4drv Running on your pi over ssh, you can directly install it: sudo /home/pi/env/bin/pip install ds4drv Grant permission to ds4drv sudo wget https://raw.githubusercontent.com/chrippa/ds4drv/master/udev/50-ds4drv.rules -O /etc/udev/rules.d/50-ds4drv.rules sudo udevadm control --reload-rules sudo udevadm trigger Run ds4drv ds4drv --hidraw --led 00ff00 If you see Failed to create input device: \"/dev/uinput\" cannot be opened for writing , reboot and retry. Probably granting permission step doesn't take effect until rebooting. Some controllers don't work with --hidraw . If that's the case try the command without it. --led 00ff00 changes the light bar color, it's optional. Start controller in pairing mode Press and hold Share button, then press and hold PS button until the light bar starts blinking. If it goes green after a few seconds, pairing is successful. Run ds4drv in background on startup once booted sudo nano /etc/rc.local paste: /home/pi/env/bin/ds4drv --led 00ff00 Save and exit. Again, with or without --hidraw , depending on the particular controller you are using. To disconnect, kill the process ds4drv and hold PS for 10 seconds to power off the controller. XBox One Controller bluetooth pairing This code presumes the built-in linux driver for 'Xbox Wireless Controller'; this is pre-installed on Raspbian, so there is no need to install any other drivers. This will generally show up on /dev/input/js0. There is another userland driver called xboxdrv; this code has not been tested with that driver. The XBox One controller requires that the bluetooth disable_ertm parameter be set to true; to do this: edit the file /etc/modprobe.d/xbox_bt.conf (that may create the file; it is commonly not there by default) add the line: options bluetooth disable_ertm=1 reboot so that this takes affect. after reboot you can verify that disable_ertm is set to true entering this command in a terminal: bash cat /sys/module/bluetooth/parameters/disable_ertm the result should print 'Y'. If not, make sure the above steps have been done correctly. Once that is done, you can pair your controller to your Raspberry Pi using the bluetooth tool. Enter the following command into a bash shell prompt: sudo bluetoothctl That will start blue tooth pairing in interactive mode. The remaining commands will be entered in that interactive session. Enter the following commands: agent on default-agent scan on That last command will start the Raspberry Pi scanning for new bluetooth devices. At this point, turn on your XBox One controller using the big round 'X' button on top, then start the pairing mode by pressing the 'sync' button on the front of the controller. Within a few minutes, you should see the controller show up in the output something like this; [NEW] Device B8:27:EB:A4:59:08 XBox One Wireless Controller Write down the MAC address, you will need it for the following steps. Enter this command to pair with your controller: connect YOUR_MAC_ADDRESS where YOUR_MAC_ADDRESS is the MAC address you copied previously. If it does not connect on the first try, try again. It can take a few tries. If your controller connects, but then immediately disconnects, your disable_ertm setting might be wrong (see above). Once your controller is connected, the big round 'X' button on the top of your controller should be solid white. Enter the following commands to finish: trust YOUR_MAC_ADDRESS quit Now that your controller is trusted, it should automatically connect with your Raspberry Pi when they are both turned on. If your controller fails to connect, run the bluetoothctl steps again to reconnect. Creating a New or Custom Game Controller To discover or modify the button and axis mappings for your controller, you can use the Joystick Wizard . The Joystick Wizard will write a custom controller named 'my_joystick.py' to your mycar folder. To use the custom controller, set CONTROLLER_TYPE=\"custom\" in your myconfig.py.","title":"Controllers"},{"location":"parts/controllers/#controller-parts","text":"","title":"Controller Parts"},{"location":"parts/controllers/#web-controller","text":"The default controller to drive the car with your phone or browser. This has a web live preview of camera. Control options include: A virtual joystick The tilt, when using a mobile device with supported accelerometer A physical joystick using the web adapter. Support varies per browser, OS, and joystick combination. Keyboard input via the 'ikjl' keys. Note: Recently iOS has disabled default Safari access to motion control.","title":"Web Controller"},{"location":"parts/controllers/#joystick-controller","text":"Many people find it easier to control the car using a game controller. There are several parts that provide this option. The default web controller may be replaced with a one line change to use a physical joystick part for input. This uses the OS device /dev/input/js0 by default. In theory, any joystick device that the OS mounts like this can be used. In practice, the behavior will change depending on the model of joystick ( Sony, or knockoff ), or XBox controller and the Bluetooth driver used to support it. The default code has been written and tested with a Sony brand PS3 Sixaxis controller . Other controllers may work, but will require alternative Bluetooth installs, and tweaks to the software for correct axis and buttons.","title":"Joystick Controller"},{"location":"parts/controllers/#these-joysticks-are-known-to-work","text":"Logitech Gamepad F710 Sony PS3 Sixaxis OEM (Not compatible with Jetson Nano) Sony PS4 Dualshock OEM WiiU Pro XBox Controller SteelSeries Nimbus (works only on TX2 jetpack 4.2+, may work on the Nano) These can be enabled by finding the CONTROLLER_TYPE in your myconfig.py and setting it to the correct string identifier ( after disabling the comment ). These can be used plugged in with a USB cable. It's been much more convenient to setup Bluetooth for a wireless control. There are controller specific setup details below. Note: If you have a controller that is not listed below, or you are having troubles getting your controller to work or you want to map your controller differently, see Creating a New or Custom Game Controller .","title":"These joysticks are known to work:"},{"location":"parts/controllers/#change-myconfigpy-or-run-with-js","text":"python manage.py drive --js Will enable driving with the joystick. This disables the live preview of the camera and the web page features. If you modify myconfig.py to make USE_JOYSTICK_AS_DEFAULT = True , then you do not need to run with the --js .","title":"Change myconfig.py or run with --js"},{"location":"parts/controllers/#ps3-controller","text":"","title":"PS3 Controller"},{"location":"parts/controllers/#bluetooth-setup","text":"Follow this guide . You can ignore steps past the 'Accessing the SixAxis from Python' section. I will include steps here in case the link becomes stale. sudo apt-get install bluetooth libbluetooth3 libusb-dev sudo systemctl enable bluetooth.service sudo usermod -G bluetooth -a pi Reboot after changing the user group. Plug in the PS3 with USB cable. Hit center PS logo button. Get and build the command line pairing tool. Run it: wget http://www.pabr.org/sixlinux/sixpair.c gcc -o sixpair sixpair.c -lusb sudo ./sixpair Use bluetoothctl to pair bluetoothctl agent on devices trust <MAC ADDRESS> default-agent quit Unplug USB cable. Hit center PS logo button. To test that the Bluetooth PS3 remote is working, verify that /dev/input/js0 exists: ls /dev/input/js0","title":"Bluetooth Setup"},{"location":"parts/controllers/#troubleshooting","text":"In case the BT connection on the Raspberry Pi does not work, you see might something like this in bluetoothctl : [NEW] Controller 00:11:22:33:44:55 super-donkey [default] [NEW] Device AA:BB:CC:DD:EE:FF PLAYSTATION(R)3 Controller [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes ... [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [bluetooth]# Try updating the Linux kernel and firmware by running: sudo rpi-update And then reboot: sudo reboot","title":"Troubleshooting"},{"location":"parts/controllers/#charging-ps3-sixaxis-joystick","text":"For some reason, they don't like to charge in a powered USB port that doesn't have an active Bluetooth control and OS driver. This means a phone type USB charger will not work. Try a powered Linux or mac laptop USB port. You should see the lights blink after plugging in and hitting center PS logo. After charging, you will need to plug-in the controller again to the Pi, hit the PS logo, then unplug to pair again.","title":"Charging PS3 Sixaxis Joystick"},{"location":"parts/controllers/#new-battery-for-ps3-sixaxis-joystick","text":"Sometimes these controllers can be quite old. Here's a link to a new battery . Be careful when taking off the cover. Remove 5 screws. There's a tab on the top half between the hand grips. You'll want to split/open it from the front and try pulling the bottom forward as you do, or you'll break the tab off as I did.","title":"New Battery for PS3 Sixaxis Joystick"},{"location":"parts/controllers/#ps3-mouse-problems-on-linux","text":"Sometimes when you plug-in the PS3 joystick it starts taking over your mouse. If you want to prevent that, you can run this: xinput set-prop \"Sony PLAYSTATION(R)3 Controller\" \"Device Enabled\" 0","title":"PS3 Mouse problems on Linux"},{"location":"parts/controllers/#ps4-controller","text":"The following instructions are based on RetroPie and ds4drv .","title":"PS4 Controller"},{"location":"parts/controllers/#install-ds4drv","text":"Running on your pi over ssh, you can directly install it: sudo /home/pi/env/bin/pip install ds4drv","title":"Install ds4drv"},{"location":"parts/controllers/#grant-permission-to-ds4drv","text":"sudo wget https://raw.githubusercontent.com/chrippa/ds4drv/master/udev/50-ds4drv.rules -O /etc/udev/rules.d/50-ds4drv.rules sudo udevadm control --reload-rules sudo udevadm trigger","title":"Grant permission to ds4drv"},{"location":"parts/controllers/#run-ds4drv","text":"ds4drv --hidraw --led 00ff00 If you see Failed to create input device: \"/dev/uinput\" cannot be opened for writing , reboot and retry. Probably granting permission step doesn't take effect until rebooting. Some controllers don't work with --hidraw . If that's the case try the command without it. --led 00ff00 changes the light bar color, it's optional.","title":"Run ds4drv"},{"location":"parts/controllers/#start-controller-in-pairing-mode","text":"Press and hold Share button, then press and hold PS button until the light bar starts blinking. If it goes green after a few seconds, pairing is successful.","title":"Start controller in pairing mode"},{"location":"parts/controllers/#run-ds4drv-in-background-on-startup-once-booted","text":"sudo nano /etc/rc.local paste: /home/pi/env/bin/ds4drv --led 00ff00 Save and exit. Again, with or without --hidraw , depending on the particular controller you are using. To disconnect, kill the process ds4drv and hold PS for 10 seconds to power off the controller.","title":"Run ds4drv in background on startup once booted"},{"location":"parts/controllers/#xbox-one-controller","text":"","title":"XBox One Controller"},{"location":"parts/controllers/#bluetooth-pairing","text":"This code presumes the built-in linux driver for 'Xbox Wireless Controller'; this is pre-installed on Raspbian, so there is no need to install any other drivers. This will generally show up on /dev/input/js0. There is another userland driver called xboxdrv; this code has not been tested with that driver. The XBox One controller requires that the bluetooth disable_ertm parameter be set to true; to do this: edit the file /etc/modprobe.d/xbox_bt.conf (that may create the file; it is commonly not there by default) add the line: options bluetooth disable_ertm=1 reboot so that this takes affect. after reboot you can verify that disable_ertm is set to true entering this command in a terminal: bash cat /sys/module/bluetooth/parameters/disable_ertm the result should print 'Y'. If not, make sure the above steps have been done correctly. Once that is done, you can pair your controller to your Raspberry Pi using the bluetooth tool. Enter the following command into a bash shell prompt: sudo bluetoothctl That will start blue tooth pairing in interactive mode. The remaining commands will be entered in that interactive session. Enter the following commands: agent on default-agent scan on That last command will start the Raspberry Pi scanning for new bluetooth devices. At this point, turn on your XBox One controller using the big round 'X' button on top, then start the pairing mode by pressing the 'sync' button on the front of the controller. Within a few minutes, you should see the controller show up in the output something like this; [NEW] Device B8:27:EB:A4:59:08 XBox One Wireless Controller Write down the MAC address, you will need it for the following steps. Enter this command to pair with your controller: connect YOUR_MAC_ADDRESS where YOUR_MAC_ADDRESS is the MAC address you copied previously. If it does not connect on the first try, try again. It can take a few tries. If your controller connects, but then immediately disconnects, your disable_ertm setting might be wrong (see above). Once your controller is connected, the big round 'X' button on the top of your controller should be solid white. Enter the following commands to finish: trust YOUR_MAC_ADDRESS quit Now that your controller is trusted, it should automatically connect with your Raspberry Pi when they are both turned on. If your controller fails to connect, run the bluetoothctl steps again to reconnect.","title":"bluetooth pairing"},{"location":"parts/controllers/#creating-a-new-or-custom-game-controller","text":"To discover or modify the button and axis mappings for your controller, you can use the Joystick Wizard . The Joystick Wizard will write a custom controller named 'my_joystick.py' to your mycar folder. To use the custom controller, set CONTROLLER_TYPE=\"custom\" in your myconfig.py.","title":"Creating a New or Custom Game Controller"},{"location":"parts/imu/","text":"IMU IMUs or inertial measurement units are parts that sense the inertial forces on a robot. They vary depending on sensor, but may commonly include linear and rotational accelleration. They may sometimes include magnetometer to give global compasss facing dir. Frequently temperature is available from these as it affects their sensitivity. MPU6050/MPU9250 This is a cheap, small, and moderately precise imu. Commonly available at Amazon . MPU9250 offers additional integrated magnetometer. Typically uses the I2C interface and can be chained off the default PWM PCA9685 board. This configuration will also provide power. MPU6050: Outputs acceleration X, Y, Z, Gyroscope X, Y, Z, and temperature. MPU6250: Outputs acceleration X, Y, Z, Gyroscope X, Y, Z, Magnetometer X, Y, Z and temperature. Chip built-in 16bit AD converter, 16bit data output Gyroscopes range: +/- 250 500 1000 2000 degree/sec Acceleration range: \u00b12 \u00b14 \u00b18 \u00b116g Software Setup Install smbus either from package: sudo apt install python3-smbus or from source: sudo apt-get install i2c-tools libi2c-dev python-dev python3-dev git clone https://github.com/pimoroni/py-smbus.git cd py-smbus/library python setup.py build sudo python setup.py install For MPU6050: Install pip lib for mpu6050 : pip install mpu6050-raspberrypi For MPU9250: Install pip lib for mpu9250-jmdev : pip install mpu9250-jmdev Configuration Enable the following configurations to your myconfig.py : #IMU HAVE_IMU = True IMU_SENSOR = 'mpu9250' # (mpu6050|mpu9250) IMU_DLP_CONFIG = 3 IMU_SENSOR can be either mpu6050 or mpu9250 based on the sensor you are using. IMU_DLP_CONFIG allows to change the digital lowpass filter settings for your IMU. Lower frequency settings (see below) can filter high frequency noise at the expense of increased latency in IMU sensor data. Valid settings are from 0 to 6: 0 250Hz 1 184Hz 2 92Hz 3 41Hz 4 20Hz 5 10Hz 6 5Hz Notes on MPU9250 At startup the MPU9250 driver performs calibration to zero accel and gyro bias. Usually the process takes less than 10 seconds, and in that time avoid moving or touching the car. Please place the car on the ground before starting Donkey.","title":"IMU"},{"location":"parts/imu/#imu","text":"IMUs or inertial measurement units are parts that sense the inertial forces on a robot. They vary depending on sensor, but may commonly include linear and rotational accelleration. They may sometimes include magnetometer to give global compasss facing dir. Frequently temperature is available from these as it affects their sensitivity.","title":"IMU"},{"location":"parts/imu/#mpu6050mpu9250","text":"This is a cheap, small, and moderately precise imu. Commonly available at Amazon . MPU9250 offers additional integrated magnetometer. Typically uses the I2C interface and can be chained off the default PWM PCA9685 board. This configuration will also provide power. MPU6050: Outputs acceleration X, Y, Z, Gyroscope X, Y, Z, and temperature. MPU6250: Outputs acceleration X, Y, Z, Gyroscope X, Y, Z, Magnetometer X, Y, Z and temperature. Chip built-in 16bit AD converter, 16bit data output Gyroscopes range: +/- 250 500 1000 2000 degree/sec Acceleration range: \u00b12 \u00b14 \u00b18 \u00b116g","title":"MPU6050/MPU9250"},{"location":"parts/imu/#software-setup","text":"Install smbus either from package: sudo apt install python3-smbus or from source: sudo apt-get install i2c-tools libi2c-dev python-dev python3-dev git clone https://github.com/pimoroni/py-smbus.git cd py-smbus/library python setup.py build sudo python setup.py install For MPU6050: Install pip lib for mpu6050 : pip install mpu6050-raspberrypi For MPU9250: Install pip lib for mpu9250-jmdev : pip install mpu9250-jmdev","title":"Software Setup"},{"location":"parts/imu/#configuration","text":"Enable the following configurations to your myconfig.py : #IMU HAVE_IMU = True IMU_SENSOR = 'mpu9250' # (mpu6050|mpu9250) IMU_DLP_CONFIG = 3 IMU_SENSOR can be either mpu6050 or mpu9250 based on the sensor you are using. IMU_DLP_CONFIG allows to change the digital lowpass filter settings for your IMU. Lower frequency settings (see below) can filter high frequency noise at the expense of increased latency in IMU sensor data. Valid settings are from 0 to 6: 0 250Hz 1 184Hz 2 92Hz 3 41Hz 4 20Hz 5 10Hz 6 5Hz","title":"Configuration"},{"location":"parts/imu/#notes-on-mpu9250","text":"At startup the MPU9250 driver performs calibration to zero accel and gyro bias. Usually the process takes less than 10 seconds, and in that time avoid moving or touching the car. Please place the car on the ground before starting Donkey.","title":"Notes on MPU9250"},{"location":"parts/keras/","text":"Keras Parts These parts encapsulate models defined using the Keras high level api. They are intended to be used with the Tensorflow backend. The parts are designed to use the trained artificial neural network to reproduce the steering and throttle given the image the camera sees. They are created by using the train command . Keras Categorical This model type is created with the --type=categorical . The KerasCategorical pilot breaks the steering and throttle decisions into discreet angles and then uses categorical cross entropy to train the network to activate a single neuron for each steering and throttle choice. This can be interesting because we get the confidence value as a distribution over all choices. This uses the dk.utils.linear_bin and dk.utils.linear_unbin to transform continuous real numbers into a range of discreet values for training and runtime. The input and output are therefore bounded and must be chosen wisely to match the data. The default ranges work for the default setup. But cars which go faster may want to enable a higher throttle range. And cars with larger steering throw may want more bins. This model was the original model, with some modifications, when Donkey was first created. Pros It has some benefits of showing the confidense as a distribution via the makemovie command. It has been very robust. In some cases this model has learned thottle control better than other models. Performs well in a limited compute environment like the Pi3. Cons Suffers from some arbitrary limitations of the chosen limits for number of categories, and thottle upper limit. Model Summary Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers, 16, and 20 w categorical output Keras Linear This model type is created with the --type=linear . The KerasLinear pilot uses one neuron to output a continous value via the Keras Dense layer with linear activation. One each for steering and throttle. The output is not bounded. Pros Steers smoothly. It has been very robust. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Cons May sometimes fail to learn throttle well. Model Summary Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers with one scalar output each with linear activation for steering and throttle. Keras IMU This model type is created with the --type=imu . The KerasIMU pilot is very similar to the KerasLinear model, except that it takes intertial measurment data in addition to images when learning to drive. This gives our stateless model some additional information about the motion of the vehicle. This can be a good starting point example of ingesting more data into your models. Pros Steers very smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Gives additional state to the model, which might help it come to a stop at a stop sign. Cons Driving quality will suffer if noisy imu is used. Model Summary Input: Image, vector of linear and angular acceleration Network: 5 Convolution layers followed by two dense layers before output, Vector data is followed by 3 dense layers then concatenating before 2 dense control layers and after conv2d layers. Output: Two dense layers with one scalar output each with linear activation for steering and throttle. Keras Latent This model type is created with the --type=latent . The KerasLatent pilot tries to force the model to learn a latent vector in addition to driving. This latent vector is a bottleneck in a CNN that then tries to reproduce the given input image and produce driving commands. These dual tasks could produce a model that learns to distill the driving scene and perhaps better abstract to a new track. Pros Steers smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Image output a measure of what the model has deemed important in the scene. Cons Needs more testing to prove theory. Model Summary Input: Image Network: 5 Convolution layers bottleneck to a 10x1x1 vector, followed by 6Conv2dTranspose layers before outputing to a image and 3 dense layers and driving controls. Output: Two dense layers with one scalar output each with linear activation for steering and throttle. Outputs an image. Keras RNN This model type is created with the --type=rnn . The KerasRNN pilot uses a sequence of images to control driving rather than just a single frame. The number of images used is controlled by the SEQUENCE_LENGTH value in myconfig.py. Pros Steers very smoothly. Can train to a lower loss Cons Performs worse in a limited compute environment like the Pi3. Takes longer to train. Model Summary Input: Image Network: 4 time distributed Convolution layers, followed by 2 LSTM layers, 3 dense layers, and driving controls. Output: One dense layer with two scalar outputs for steering and throttle. Keras 3D This model type is created with the --type=3d . The Keras3D_CNN pilot uses a sequence of images to control driving rather than just a single frame. The number of images used is controlled by the SEQUENCE_LENGTH value in myconfig.py. Instead of 2d convolutions like most other models, this uses a 3D convolution across layers. Pros Steers very smoothly. Can train to a lower loss. Cons Performs worse in a limited compute environment like the Pi3. Takes longer to train. Model Summary Input: Image Network: 4 3D Convolution layers each followed by max pooling, followed by 2 dense layers, and driving controls. Output: One dense layer with two scalar outputs for steering and throttle. Keras Behavior This model type is created with the --type=behavior . The KerasBehavioral pilot takes an image and a vector as input. The vector is one hot activated vector of commands. This vector might be of length two and have two states, one for left lane driving and one for right lane driving. Then during training one element of the vector is activated while the desired behavior is demonstrated. This vector is defined in myconfig.py BEHAVIOR_LIST . BEHAVIOR_LED_COLORS must match the same length and can be useful when showing the current state. TRAIN_BEHAVIORS must be set to True. Pros Can create a model which can perform multiple tasks Cons Takes more effort to train. Model Summary Input: Image, Behavior vector Network: 5 Convolution layers, followed by 2 dense layers, and driving controls. Output: Categorical steering, throttle output similar to Categorical keras model. Keras Localizer This model type is not created without some code modification. The KerasLocalizer pilot is very similar to the Keras Linear model, except that it learns to output it's location as a category. This category is arbitrary, but has only been tested as a 0-9 range segment of the track. This requires that the driving data is marked up with a category label for location. This could supply some higher level logic with track location, for driving stategy, lap counting, or other. Pros Steers smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Location to supply some higher level logic. Cons May sometimes fail to learn throttle well. Model Summary Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers with one scalar output each with linear activation for steering and throttle. One categorical output for location.","title":"Keras"},{"location":"parts/keras/#keras-parts","text":"These parts encapsulate models defined using the Keras high level api. They are intended to be used with the Tensorflow backend. The parts are designed to use the trained artificial neural network to reproduce the steering and throttle given the image the camera sees. They are created by using the train command .","title":"Keras Parts"},{"location":"parts/keras/#keras-categorical","text":"This model type is created with the --type=categorical . The KerasCategorical pilot breaks the steering and throttle decisions into discreet angles and then uses categorical cross entropy to train the network to activate a single neuron for each steering and throttle choice. This can be interesting because we get the confidence value as a distribution over all choices. This uses the dk.utils.linear_bin and dk.utils.linear_unbin to transform continuous real numbers into a range of discreet values for training and runtime. The input and output are therefore bounded and must be chosen wisely to match the data. The default ranges work for the default setup. But cars which go faster may want to enable a higher throttle range. And cars with larger steering throw may want more bins. This model was the original model, with some modifications, when Donkey was first created.","title":"Keras Categorical"},{"location":"parts/keras/#pros","text":"It has some benefits of showing the confidense as a distribution via the makemovie command. It has been very robust. In some cases this model has learned thottle control better than other models. Performs well in a limited compute environment like the Pi3.","title":"Pros"},{"location":"parts/keras/#cons","text":"Suffers from some arbitrary limitations of the chosen limits for number of categories, and thottle upper limit.","title":"Cons"},{"location":"parts/keras/#model-summary","text":"Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers, 16, and 20 w categorical output","title":"Model Summary"},{"location":"parts/keras/#keras-linear","text":"This model type is created with the --type=linear . The KerasLinear pilot uses one neuron to output a continous value via the Keras Dense layer with linear activation. One each for steering and throttle. The output is not bounded.","title":"Keras Linear"},{"location":"parts/keras/#pros_1","text":"Steers smoothly. It has been very robust. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle.","title":"Pros"},{"location":"parts/keras/#cons_1","text":"May sometimes fail to learn throttle well.","title":"Cons"},{"location":"parts/keras/#model-summary_1","text":"Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers with one scalar output each with linear activation for steering and throttle.","title":"Model Summary"},{"location":"parts/keras/#keras-imu","text":"This model type is created with the --type=imu . The KerasIMU pilot is very similar to the KerasLinear model, except that it takes intertial measurment data in addition to images when learning to drive. This gives our stateless model some additional information about the motion of the vehicle. This can be a good starting point example of ingesting more data into your models.","title":"Keras IMU"},{"location":"parts/keras/#pros_2","text":"Steers very smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Gives additional state to the model, which might help it come to a stop at a stop sign.","title":"Pros"},{"location":"parts/keras/#cons_2","text":"Driving quality will suffer if noisy imu is used.","title":"Cons"},{"location":"parts/keras/#model-summary_2","text":"Input: Image, vector of linear and angular acceleration Network: 5 Convolution layers followed by two dense layers before output, Vector data is followed by 3 dense layers then concatenating before 2 dense control layers and after conv2d layers. Output: Two dense layers with one scalar output each with linear activation for steering and throttle.","title":"Model Summary"},{"location":"parts/keras/#keras-latent","text":"This model type is created with the --type=latent . The KerasLatent pilot tries to force the model to learn a latent vector in addition to driving. This latent vector is a bottleneck in a CNN that then tries to reproduce the given input image and produce driving commands. These dual tasks could produce a model that learns to distill the driving scene and perhaps better abstract to a new track.","title":"Keras Latent"},{"location":"parts/keras/#pros_3","text":"Steers smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Image output a measure of what the model has deemed important in the scene.","title":"Pros"},{"location":"parts/keras/#cons_3","text":"Needs more testing to prove theory.","title":"Cons"},{"location":"parts/keras/#model-summary_3","text":"Input: Image Network: 5 Convolution layers bottleneck to a 10x1x1 vector, followed by 6Conv2dTranspose layers before outputing to a image and 3 dense layers and driving controls. Output: Two dense layers with one scalar output each with linear activation for steering and throttle. Outputs an image.","title":"Model Summary"},{"location":"parts/keras/#keras-rnn","text":"This model type is created with the --type=rnn . The KerasRNN pilot uses a sequence of images to control driving rather than just a single frame. The number of images used is controlled by the SEQUENCE_LENGTH value in myconfig.py.","title":"Keras RNN"},{"location":"parts/keras/#pros_4","text":"Steers very smoothly. Can train to a lower loss","title":"Pros"},{"location":"parts/keras/#cons_4","text":"Performs worse in a limited compute environment like the Pi3. Takes longer to train.","title":"Cons"},{"location":"parts/keras/#model-summary_4","text":"Input: Image Network: 4 time distributed Convolution layers, followed by 2 LSTM layers, 3 dense layers, and driving controls. Output: One dense layer with two scalar outputs for steering and throttle.","title":"Model Summary"},{"location":"parts/keras/#keras-3d","text":"This model type is created with the --type=3d . The Keras3D_CNN pilot uses a sequence of images to control driving rather than just a single frame. The number of images used is controlled by the SEQUENCE_LENGTH value in myconfig.py. Instead of 2d convolutions like most other models, this uses a 3D convolution across layers.","title":"Keras 3D"},{"location":"parts/keras/#pros_5","text":"Steers very smoothly. Can train to a lower loss.","title":"Pros"},{"location":"parts/keras/#cons_5","text":"Performs worse in a limited compute environment like the Pi3. Takes longer to train.","title":"Cons"},{"location":"parts/keras/#model-summary_5","text":"Input: Image Network: 4 3D Convolution layers each followed by max pooling, followed by 2 dense layers, and driving controls. Output: One dense layer with two scalar outputs for steering and throttle.","title":"Model Summary"},{"location":"parts/keras/#keras-behavior","text":"This model type is created with the --type=behavior . The KerasBehavioral pilot takes an image and a vector as input. The vector is one hot activated vector of commands. This vector might be of length two and have two states, one for left lane driving and one for right lane driving. Then during training one element of the vector is activated while the desired behavior is demonstrated. This vector is defined in myconfig.py BEHAVIOR_LIST . BEHAVIOR_LED_COLORS must match the same length and can be useful when showing the current state. TRAIN_BEHAVIORS must be set to True.","title":"Keras Behavior"},{"location":"parts/keras/#pros_6","text":"Can create a model which can perform multiple tasks","title":"Pros"},{"location":"parts/keras/#cons_6","text":"Takes more effort to train.","title":"Cons"},{"location":"parts/keras/#model-summary_6","text":"Input: Image, Behavior vector Network: 5 Convolution layers, followed by 2 dense layers, and driving controls. Output: Categorical steering, throttle output similar to Categorical keras model.","title":"Model Summary"},{"location":"parts/keras/#keras-localizer","text":"This model type is not created without some code modification. The KerasLocalizer pilot is very similar to the Keras Linear model, except that it learns to output it's location as a category. This category is arbitrary, but has only been tested as a 0-9 range segment of the track. This requires that the driving data is marked up with a category label for location. This could supply some higher level logic with track location, for driving stategy, lap counting, or other.","title":"Keras Localizer"},{"location":"parts/keras/#pros_7","text":"Steers smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Location to supply some higher level logic.","title":"Pros"},{"location":"parts/keras/#cons_7","text":"May sometimes fail to learn throttle well.","title":"Cons"},{"location":"parts/keras/#model-summary_7","text":"Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers with one scalar output each with linear activation for steering and throttle. One categorical output for location.","title":"Model Summary"},{"location":"parts/oled/","text":"OLED Displays OLED displays can be used to show information about the current state of the car. This is especially useful in the when collecting data for training, and when racing. The OLED display currently displays the following information: * The IP address of the car ( eth and wlan ) * The number of records collected, for training. * The driving mode. Supported displays Examples of displays that are currently supported are: Adafruit PiOLED - 128X32 MonoChrome OLED Hardware Setup Simply connect the display to the I2C pins on the Raspberry Pi or the Jetson Nano. Use bus 1 so the display can be inserted directly on the pins. Here is an example of what that looks like. Software Setup Enable the display in myconfig.py . # SSD1306_128_32 USE_SSD1306_128_32 = True # Enable the SSD_1306 OLED Display SSD1306_128_32_I2C_BUSNUM = 1 # I2C bus number Troubleshooting If you are unable to start the car, ensure that the Adafruit_SSD1306 package is installed in your virtual environment. This should automatically be installed, if you are using a recent version of donkeycar . pip install Adafruit_SSD1306","title":"OLED"},{"location":"parts/oled/#oled-displays","text":"OLED displays can be used to show information about the current state of the car. This is especially useful in the when collecting data for training, and when racing. The OLED display currently displays the following information: * The IP address of the car ( eth and wlan ) * The number of records collected, for training. * The driving mode.","title":"OLED Displays"},{"location":"parts/oled/#supported-displays","text":"Examples of displays that are currently supported are: Adafruit PiOLED - 128X32 MonoChrome OLED","title":"Supported displays"},{"location":"parts/oled/#hardware-setup","text":"Simply connect the display to the I2C pins on the Raspberry Pi or the Jetson Nano. Use bus 1 so the display can be inserted directly on the pins. Here is an example of what that looks like.","title":"Hardware Setup"},{"location":"parts/oled/#software-setup","text":"Enable the display in myconfig.py . # SSD1306_128_32 USE_SSD1306_128_32 = True # Enable the SSD_1306 OLED Display SSD1306_128_32_I2C_BUSNUM = 1 # I2C bus number","title":"Software Setup"},{"location":"parts/oled/#troubleshooting","text":"If you are unable to start the car, ensure that the Adafruit_SSD1306 package is installed in your virtual environment. This should automatically be installed, if you are using a recent version of donkeycar . pip install Adafruit_SSD1306","title":"Troubleshooting"},{"location":"parts/stop_sign_detection/","text":"Stop Sign Detection This part utilize a Google Coral accelerator and a pre-trained object detection model by Coral project to perform stop sign detection. If the donkey car see a stop sign, it will override the pilot/throttle to 0. In addition, a bounding box will be annotated to the cam/image_array . Your browser does not support the video tag. Requirement To use this part, you must have: Google Coral USB Accelerator How to use Put the following lines in myconfig.py STOP_SIGN_DETECTOR = True STOP_SIGN_MIN_SCORE = 0.2 STOP_SIGN_SHOW_BOUNDING_BOX = True Install Edge TPU dependencies Follow this instruction to install and setup Google Coral on Pi 4 In addition, install the dependency on your dev pc or pi4 like this pip3 install https://dl.google.com/coral/edgetpu_api/edgetpu-2.14.0-py3-none-any.whl Detecting other objects Since the pre-trained model are trained on coco, there are 80 objects that the model is able to detect. You can simply change the STOP_SIGN_CLASS_ID in stop_sign_detector.py to try. Accuracy Since SSD is not good at detecting small objects , the accuracy of detecting the stop sign from far away may not be good. There are some ways that we can make enhancement but this is out of the scope of this part.","title":"Stop Sign Detection"},{"location":"parts/stop_sign_detection/#stop-sign-detection","text":"This part utilize a Google Coral accelerator and a pre-trained object detection model by Coral project to perform stop sign detection. If the donkey car see a stop sign, it will override the pilot/throttle to 0. In addition, a bounding box will be annotated to the cam/image_array . Your browser does not support the video tag.","title":"Stop Sign Detection"},{"location":"parts/stop_sign_detection/#requirement","text":"To use this part, you must have: Google Coral USB Accelerator","title":"Requirement"},{"location":"parts/stop_sign_detection/#how-to-use","text":"Put the following lines in myconfig.py STOP_SIGN_DETECTOR = True STOP_SIGN_MIN_SCORE = 0.2 STOP_SIGN_SHOW_BOUNDING_BOX = True","title":"How to use"},{"location":"parts/stop_sign_detection/#install-edge-tpu-dependencies","text":"Follow this instruction to install and setup Google Coral on Pi 4 In addition, install the dependency on your dev pc or pi4 like this pip3 install https://dl.google.com/coral/edgetpu_api/edgetpu-2.14.0-py3-none-any.whl","title":"Install Edge TPU dependencies"},{"location":"parts/stop_sign_detection/#detecting-other-objects","text":"Since the pre-trained model are trained on coco, there are 80 objects that the model is able to detect. You can simply change the STOP_SIGN_CLASS_ID in stop_sign_detector.py to try.","title":"Detecting other objects"},{"location":"parts/stop_sign_detection/#accuracy","text":"Since SSD is not good at detecting small objects , the accuracy of detecting the stop sign from far away may not be good. There are some ways that we can make enhancement but this is out of the scope of this part.","title":"Accuracy"},{"location":"parts/stores/","text":"Stores Stores are parts that record and replay vehicle data produced by other parts. Tub This is the standard donkey data store and it is modeled after the ROSBAG . TODO: The structure of the Tub part is not ideal and should be changed. types should not need to be specified and could be inspected and saved on the first loop. Example creation import donkey as dk T = dk.parts.Tub(path, inputs, types) Accepted Types float - saved as record int - saved as record","title":"Stores"},{"location":"parts/stores/#stores","text":"Stores are parts that record and replay vehicle data produced by other parts.","title":"Stores"},{"location":"parts/stores/#tub","text":"This is the standard donkey data store and it is modeled after the ROSBAG . TODO: The structure of the Tub part is not ideal and should be changed. types should not need to be specified and could be inspected and saved on the first loop. Example creation import donkey as dk T = dk.parts.Tub(path, inputs, types)","title":"Tub"},{"location":"parts/stores/#accepted-types","text":"float - saved as record int - saved as record","title":"Accepted Types"},{"location":"parts/voice_control/","text":"Alexa Support Overview This part works together with a public Alexa skill that we have released. When you say a command, the Alexa skill will forward this command to a server hosted by us to temporarily store it. Your donkey car, installed with this part and with proper configuration, poll our server for any new command from Alexa. Demo Click the image below to open the video on youtube Command Supported Report device code autopilot slowdown speedup stop/manual Get Started Use your Alexa app , navigate to Skills and Games Search for \"Donkey Car Control\" Enable the Skill Say \"Open car control and report device code\". Use a pencil to write down the device code. Follow the instructions below to install the part in donkey car software running on Pi Installation To install this part, add the following lines to manage.py , right after the controller setup. In manage.py: if cfg.USE_ALEXA_CONTROL: from donkeycar.parts.voice_control.alexa import AlexaController V.add(AlexaController(ctr, cfg), threaded=True) In myconfig.py, add the following parameters: USE_ALEXA_CONTROL = True ALEXA_DEVICE_CODE = \"123456\" Commands Autopilot Phrases: autopilot, start autopilot If you use this command, it is expected that the donkey car is started with a model. This command will set the variable mode of the controller to local . Slowdown / Speedup Phrases: slow down, speed up, go faster, go slower This command alters the cfg.AI_THROTTLE_MULT variable passed from the constructor. Each time this command is received, the AI_THROTTLE_MULT is increased/decreased by 0.05. Note: Since this command alters AI_THROTTLE_MULT , it won't speed up when you are running in user or local_angle mode. Stop/Manual Phrases: human control, user mode, stop autopilot, manual This command will set the variable mode of the controller to user Report device code Phrases: report device code, what is your device code, device code Device code is a 6-digit numeric string derived by a hash function from your Alexa device ID. In order to distinguish commands from multiple Alexa devices, commands sent to our server would require an identifier, which is the device code. When donkey car poll for new command, the part will use this device code to poll for new commands. Backend Check here for our web service source code, it is open source too. https://github.com/robocarstore/donkeycar-alexa-backend Copyright Copyright (c) 2020 Robocar Ltd","title":"Voice Control"},{"location":"parts/voice_control/#alexa-support","text":"","title":"Alexa Support"},{"location":"parts/voice_control/#overview","text":"This part works together with a public Alexa skill that we have released. When you say a command, the Alexa skill will forward this command to a server hosted by us to temporarily store it. Your donkey car, installed with this part and with proper configuration, poll our server for any new command from Alexa.","title":"Overview"},{"location":"parts/voice_control/#demo","text":"Click the image below to open the video on youtube","title":"Demo"},{"location":"parts/voice_control/#command-supported","text":"Report device code autopilot slowdown speedup stop/manual","title":"Command Supported"},{"location":"parts/voice_control/#get-started","text":"Use your Alexa app , navigate to Skills and Games Search for \"Donkey Car Control\" Enable the Skill Say \"Open car control and report device code\". Use a pencil to write down the device code. Follow the instructions below to install the part in donkey car software running on Pi","title":"Get Started"},{"location":"parts/voice_control/#installation","text":"To install this part, add the following lines to manage.py , right after the controller setup. In manage.py: if cfg.USE_ALEXA_CONTROL: from donkeycar.parts.voice_control.alexa import AlexaController V.add(AlexaController(ctr, cfg), threaded=True) In myconfig.py, add the following parameters: USE_ALEXA_CONTROL = True ALEXA_DEVICE_CODE = \"123456\"","title":"Installation"},{"location":"parts/voice_control/#commands","text":"","title":"Commands"},{"location":"parts/voice_control/#autopilot","text":"Phrases: autopilot, start autopilot If you use this command, it is expected that the donkey car is started with a model. This command will set the variable mode of the controller to local .","title":"Autopilot"},{"location":"parts/voice_control/#slowdown-speedup","text":"Phrases: slow down, speed up, go faster, go slower This command alters the cfg.AI_THROTTLE_MULT variable passed from the constructor. Each time this command is received, the AI_THROTTLE_MULT is increased/decreased by 0.05. Note: Since this command alters AI_THROTTLE_MULT , it won't speed up when you are running in user or local_angle mode.","title":"Slowdown / Speedup"},{"location":"parts/voice_control/#stopmanual","text":"Phrases: human control, user mode, stop autopilot, manual This command will set the variable mode of the controller to user","title":"Stop/Manual"},{"location":"parts/voice_control/#report-device-code","text":"Phrases: report device code, what is your device code, device code Device code is a 6-digit numeric string derived by a hash function from your Alexa device ID. In order to distinguish commands from multiple Alexa devices, commands sent to our server would require an identifier, which is the device code. When donkey car poll for new command, the part will use this device code to poll for new commands.","title":"Report device code"},{"location":"parts/voice_control/#backend","text":"Check here for our web service source code, it is open source too. https://github.com/robocarstore/donkeycar-alexa-backend","title":"Backend"},{"location":"parts/voice_control/#copyright","text":"Copyright (c) 2020 Robocar Ltd","title":"Copyright"},{"location":"utility/donkey/","text":"Donkey Command-line Utilities The donkey command is created when you install the donkeycar Python package. This is a Python script that adds some important functionality. The operations here are vehicle independent, and should work on any hardware configuration. Create Car This command creates a new dir which will contain the files needed to run and train your robot. Usage: donkey createcar --path <dir> [--overwrite] [--template <donkey2>] This command may be run from any dir Run on the host computer or the robot It uses the --path as the destination dir to create. If .py files exist there, it will not overwrite them, unless the optional --overwrite is used. The optional --template will specify the template file to start from. For a list of templates, see the donkeycar/templates dir. This source template will be copied over the manage.py for the user. Find Car This command attempts to locate your car on the local network using nmap. Usage: donkey findcar Run on the host computer Prints the host computer IP address and the car IP address if found Requires the nmap utility: sudo apt install nmap Calibrate Car This command allows you to manually enter values to interactively set the PWM values and experiment with how your robot responds. See also more information. Usage: donkey calibrate --channel <0-15 channel id> Run on the host computer Opens the PWM channel specified by --channel Type integer values to specify PWM values and hit enter Hit Ctrl + C to exit Clean data in Tub Opens a web server to delete bad data from a tub. Usage: donkey tubclean <folder containing tubs> Run on pi or host computer. Opens the web server to delete bad data. Hit Ctrl + C to exit Train the model Note: This section only applies to version >= 4.1 This command trains the model. donkey train --tub=<tub_path> [--config=<config.py>] [--model=<model path>] [--model_type=(linear|categorical|inferred)] The createcar command still creates a train.py file for backward compatibility, but it's not required for training. Make Movie from Tub This command allows you to create a movie file from the images in a Tub. Usage: donkey makemovie --tub=<tub_path> [--out=<tub_movie.mp4>] [--config=<config.py>] [--model=<model path>] [--model_type=(linear|categorical|rnn|imu|behavior|3d)] [--start=0] [--end=-1] [--scale=2] [--salient] Run on the host computer or the robot Uses the image records from --tub dir path given Creates a movie given by --out . Codec is inferred from file extension. Default: tub_movie.mp4 Optional argument to specify a different config.py other than default: config.py Optional model argument will load the keras model and display prediction as lines on the movie model_type may optionally give a hint about what model type we are loading. Categorical is default. optional --salient will overlay a visualization of which pixels excited the NN the most optional --start and/or --end can specify a range of frame numbers to use. scale will cause ouput image to be scaled by this amount Check Tub This command allows you to see how many records are contained in any/all tubs. It will also open each record and ensure that the data is readable and intact. If not, it will allow you to remove corrupt records. Note: This should be moved from manage.py to donkey command Usage: donkey tubcheck <tub_path> [--fix] Run on the host computer or the robot It will print summary of record count and channels recorded for each tub It will print the records that throw an exception while reading The optional --fix will delete records that have problems Augment Tub This command allows you to perform the data augmentation on a tub or set of tubs directly. The augmentation is also available in training via the --aug flag. Preprocessing the tub can speed up the training as the augmentation can take some time. Also you can train with the unmodified tub and the augmented tub joined together. Usage: donkey tubaugment <tub_path> [--inplace] Run on the host computer or the robot The optional --inplace will replace the original tub images when provided. Otherwise tub_XY_YY-MM-DD will be copied to a new tub tub_XX_aug_YY-MM-DD and the original data remains unchanged Histogram This command will show a pop-up window showing the histogram of record values in a given tub. Note: This should be moved from manage.py to donkey command Usage: donkey tubhist <tub_path> --rec=<\"user/angle\"> Run on the host computer When the --tub is omitted, it will check all tubs in the default data dir Plot Predictions This command allows you plot steering and throttle against predictions coming from a trained model. Note: This should be moved from manage.py to donkey command Usage: donkey tubplot <tub_path> [--model=<model_path>] This command may be run from ~/mycar dir Run on the host computer Will show a pop-up window showing the plot of steering values in a given tub compared to NN predictions from the trained model When the --tub is omitted, it will check all tubs in the default data dir Continuous Rsync This command uses rsync to copy files from your pi to your host. It does so in a loop, continuously copying files. By default, it will also delete any files on the host that are deleted on the pi. This allows your PS3 Triangle edits to affect the files on both machines. Usage: donkey consync [--dir = <data_path>] [--delete=<y|n>] Run on the host computer First copy your public key to the pi so you don't need a password for each rsync: cat ~/.ssh/id_rsa.pub | ssh pi@<your pi ip> 'cat >> .ssh/authorized_keys' If you don't have a id_rsa.pub then google how to make one Edit your config.py and make sure the fields PI_USERNAME , PI_HOSTNAME , PI_DONKEY_ROOT are setup. Only on windows, you need to set PI_PASSWD . This command may be run from ~/mycar dir Continuous Train This command fires off the keras training in a mode where it will continuously look for new data at the end of every epoch. Usage: donkey contrain [--tub=<data_path>] [--model=<path to model>] [--transfer=<path to model>] [--type=<linear|categorical|rnn|imu|behavior|3d>] [--aug] This command may be run from ~/mycar dir Run on the host computer First copy your public key to the pi so you don't need a password for each rsync: cat ~/.ssh/id_rsa.pub | ssh pi@<your pi ip> 'cat >> .ssh/authorized_keys' If you don't have a id_rsa.pub then google how to make one Edit your config.py and make sure the fields PI_USERNAME , PI_HOSTNAME , PI_DONKEY_ROOT are setup. Only on windows, you need to set PI_PASSWD . Optionally it can send the model file to your pi when it achieves a best loss. In config.py set SEND_BEST_MODEL_TO_PI = True . Your pi drive loop will autoload the weights file when it changes. This works best if car started with .json weights like: python manage.py drive --model models/drive.json Joystick Wizard This command line wizard will walk you through the steps to create a custom/customized controller. Usage: donkey createjs Run the command from your ~/mycar dir First make sure the OS can access your device. The utility jstest can be useful here. Installed via: sudo apt install joystick You must pass this utility the path to your controller's device. Typically this is /dev/input/js0 However, it if is not, you must find the correct device path and provide it to the utility. You will need this for the createjs command as well. Run the command donkey createjs and it will create a file named my_joystick.py in your ~/mycar folder, next to your manage.py Modify myconfig.py to set CONTROLLER_TYPE=\"custom\" to use your my_joystick.py controller Visualize CNN filter activations Shows feature maps of the provided image for each filter in each of the convolutional layers in the model provided. Debugging tool to visualize how well feature extraction is performing. Usage: donkey cnnactivations [--tub=<data_path>] [--model=<path to model>] This will open a figure for each Conv2d layer in the model. Example: donkey cnnactivations --model models/model.h5 --image data/tub/1_cam-image_array_.jpg","title":"donkey"},{"location":"utility/donkey/#donkey-command-line-utilities","text":"The donkey command is created when you install the donkeycar Python package. This is a Python script that adds some important functionality. The operations here are vehicle independent, and should work on any hardware configuration.","title":"Donkey Command-line Utilities"},{"location":"utility/donkey/#create-car","text":"This command creates a new dir which will contain the files needed to run and train your robot. Usage: donkey createcar --path <dir> [--overwrite] [--template <donkey2>] This command may be run from any dir Run on the host computer or the robot It uses the --path as the destination dir to create. If .py files exist there, it will not overwrite them, unless the optional --overwrite is used. The optional --template will specify the template file to start from. For a list of templates, see the donkeycar/templates dir. This source template will be copied over the manage.py for the user.","title":"Create Car"},{"location":"utility/donkey/#find-car","text":"This command attempts to locate your car on the local network using nmap. Usage: donkey findcar Run on the host computer Prints the host computer IP address and the car IP address if found Requires the nmap utility: sudo apt install nmap","title":"Find Car"},{"location":"utility/donkey/#calibrate-car","text":"This command allows you to manually enter values to interactively set the PWM values and experiment with how your robot responds. See also more information. Usage: donkey calibrate --channel <0-15 channel id> Run on the host computer Opens the PWM channel specified by --channel Type integer values to specify PWM values and hit enter Hit Ctrl + C to exit","title":"Calibrate Car"},{"location":"utility/donkey/#clean-data-in-tub","text":"Opens a web server to delete bad data from a tub. Usage: donkey tubclean <folder containing tubs> Run on pi or host computer. Opens the web server to delete bad data. Hit Ctrl + C to exit","title":"Clean data in Tub"},{"location":"utility/donkey/#train-the-model","text":"Note: This section only applies to version >= 4.1 This command trains the model. donkey train --tub=<tub_path> [--config=<config.py>] [--model=<model path>] [--model_type=(linear|categorical|inferred)] The createcar command still creates a train.py file for backward compatibility, but it's not required for training.","title":"Train the model"},{"location":"utility/donkey/#make-movie-from-tub","text":"This command allows you to create a movie file from the images in a Tub. Usage: donkey makemovie --tub=<tub_path> [--out=<tub_movie.mp4>] [--config=<config.py>] [--model=<model path>] [--model_type=(linear|categorical|rnn|imu|behavior|3d)] [--start=0] [--end=-1] [--scale=2] [--salient] Run on the host computer or the robot Uses the image records from --tub dir path given Creates a movie given by --out . Codec is inferred from file extension. Default: tub_movie.mp4 Optional argument to specify a different config.py other than default: config.py Optional model argument will load the keras model and display prediction as lines on the movie model_type may optionally give a hint about what model type we are loading. Categorical is default. optional --salient will overlay a visualization of which pixels excited the NN the most optional --start and/or --end can specify a range of frame numbers to use. scale will cause ouput image to be scaled by this amount","title":"Make Movie from Tub"},{"location":"utility/donkey/#check-tub","text":"This command allows you to see how many records are contained in any/all tubs. It will also open each record and ensure that the data is readable and intact. If not, it will allow you to remove corrupt records. Note: This should be moved from manage.py to donkey command Usage: donkey tubcheck <tub_path> [--fix] Run on the host computer or the robot It will print summary of record count and channels recorded for each tub It will print the records that throw an exception while reading The optional --fix will delete records that have problems","title":"Check Tub"},{"location":"utility/donkey/#augment-tub","text":"This command allows you to perform the data augmentation on a tub or set of tubs directly. The augmentation is also available in training via the --aug flag. Preprocessing the tub can speed up the training as the augmentation can take some time. Also you can train with the unmodified tub and the augmented tub joined together. Usage: donkey tubaugment <tub_path> [--inplace] Run on the host computer or the robot The optional --inplace will replace the original tub images when provided. Otherwise tub_XY_YY-MM-DD will be copied to a new tub tub_XX_aug_YY-MM-DD and the original data remains unchanged","title":"Augment Tub"},{"location":"utility/donkey/#histogram","text":"This command will show a pop-up window showing the histogram of record values in a given tub. Note: This should be moved from manage.py to donkey command Usage: donkey tubhist <tub_path> --rec=<\"user/angle\"> Run on the host computer When the --tub is omitted, it will check all tubs in the default data dir","title":"Histogram"},{"location":"utility/donkey/#plot-predictions","text":"This command allows you plot steering and throttle against predictions coming from a trained model. Note: This should be moved from manage.py to donkey command Usage: donkey tubplot <tub_path> [--model=<model_path>] This command may be run from ~/mycar dir Run on the host computer Will show a pop-up window showing the plot of steering values in a given tub compared to NN predictions from the trained model When the --tub is omitted, it will check all tubs in the default data dir","title":"Plot Predictions"},{"location":"utility/donkey/#continuous-rsync","text":"This command uses rsync to copy files from your pi to your host. It does so in a loop, continuously copying files. By default, it will also delete any files on the host that are deleted on the pi. This allows your PS3 Triangle edits to affect the files on both machines. Usage: donkey consync [--dir = <data_path>] [--delete=<y|n>] Run on the host computer First copy your public key to the pi so you don't need a password for each rsync: cat ~/.ssh/id_rsa.pub | ssh pi@<your pi ip> 'cat >> .ssh/authorized_keys' If you don't have a id_rsa.pub then google how to make one Edit your config.py and make sure the fields PI_USERNAME , PI_HOSTNAME , PI_DONKEY_ROOT are setup. Only on windows, you need to set PI_PASSWD . This command may be run from ~/mycar dir","title":"Continuous Rsync"},{"location":"utility/donkey/#continuous-train","text":"This command fires off the keras training in a mode where it will continuously look for new data at the end of every epoch. Usage: donkey contrain [--tub=<data_path>] [--model=<path to model>] [--transfer=<path to model>] [--type=<linear|categorical|rnn|imu|behavior|3d>] [--aug] This command may be run from ~/mycar dir Run on the host computer First copy your public key to the pi so you don't need a password for each rsync: cat ~/.ssh/id_rsa.pub | ssh pi@<your pi ip> 'cat >> .ssh/authorized_keys' If you don't have a id_rsa.pub then google how to make one Edit your config.py and make sure the fields PI_USERNAME , PI_HOSTNAME , PI_DONKEY_ROOT are setup. Only on windows, you need to set PI_PASSWD . Optionally it can send the model file to your pi when it achieves a best loss. In config.py set SEND_BEST_MODEL_TO_PI = True . Your pi drive loop will autoload the weights file when it changes. This works best if car started with .json weights like: python manage.py drive --model models/drive.json","title":"Continuous Train"},{"location":"utility/donkey/#joystick-wizard","text":"This command line wizard will walk you through the steps to create a custom/customized controller. Usage: donkey createjs Run the command from your ~/mycar dir First make sure the OS can access your device. The utility jstest can be useful here. Installed via: sudo apt install joystick You must pass this utility the path to your controller's device. Typically this is /dev/input/js0 However, it if is not, you must find the correct device path and provide it to the utility. You will need this for the createjs command as well. Run the command donkey createjs and it will create a file named my_joystick.py in your ~/mycar folder, next to your manage.py Modify myconfig.py to set CONTROLLER_TYPE=\"custom\" to use your my_joystick.py controller","title":"Joystick Wizard"},{"location":"utility/donkey/#visualize-cnn-filter-activations","text":"Shows feature maps of the provided image for each filter in each of the convolutional layers in the model provided. Debugging tool to visualize how well feature extraction is performing. Usage: donkey cnnactivations [--tub=<data_path>] [--model=<path to model>] This will open a figure for each Conv2d layer in the model. Example: donkey cnnactivations --model models/model.h5 --image data/tub/1_cam-image_array_.jpg","title":"Visualize CNN filter activations"}]}